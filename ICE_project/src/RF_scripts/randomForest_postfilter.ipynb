{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal, ogr, gdal_array\n",
    "import dask\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "import os\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices, DEADataHandling\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from query_from_shp import query_from_shp\n",
    "from rsgislib.segmentation import segutils\n",
    "from rasterstats import zonal_stats\n",
    "from imageSeg import imageSeg\n",
    "import fiona\n",
    "import rasterio.features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is your data and results folder?\n",
    "results = \"results/\"\n",
    "data = 'data/'\n",
    "\n",
    "sensors = ['ls5', 'ls7','ls8']\n",
    "\n",
    "#are we using a polygon to mask the AOI?\n",
    "shp_fpath = 'data/spatial/murrumbidgee_boundingbox.shp'\n",
    "\n",
    "#Input your area of interest's name, coords, and \n",
    "#the year you're interested in?\n",
    "AOI = 'Murrum_randomForest'\n",
    "year = 'Winter2016'\n",
    "\n",
    "time_period = ('2016-04-01', '2016-09-30')\n",
    "\n",
    "#What thresholds should I use?\n",
    "threshold = 0.8\n",
    "wofs_theshold = 0.15\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder to keep things neat\n",
    "directory = results + AOI + \"_\" + year\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "results = results + AOI + \"_\" + year + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up query\n",
    "query = query_from_shp(shp_fpath, time_period[0], time_period[1], dask_chunks = 500)\n",
    "#landsat\n",
    "# dc = datacube.Datacube(app='dc_name')\n",
    "# landsat = DEADataHandling.load_clearlandsat(dc,query=query, sensors=sensors, product='nbart',\n",
    "#                        masked_prop=0.75)\n",
    "\n",
    "landsat = load_data(dc_name = 'irrigated_areas', sensors=sensors, export_name = 'fh',query=query)\n",
    "#wofs\n",
    "# dc = datacube.Datacube(app='wofs')\n",
    "# del query['time'] \n",
    "# wofs_alltime = dc.load(product = 'wofs_summary', **query)\n",
    "\n",
    "#masking the returned array to the polygon area\n",
    "with fiona.open(shp_fpath) as shapes:\n",
    "        crs = geometry.CRS(shapes.crs_wkt)\n",
    "        first_geometry = next(iter(shapes))['geometry']\n",
    "        geom = geometry.Geometry(first_geometry, crs=crs)\n",
    "\n",
    "mask = rasterio.features.geometry_mask([geom.to_crs(landsat.geobox.crs) for geoms in [geom]],\n",
    "                                           out_shape=landsat.geobox.shape,\n",
    "                                           transform=landsat.geobox.affine,\n",
    "                                           all_touched=False,\n",
    "                                           invert=True)\n",
    "# Mask the xarrays\n",
    "landsat = landsat.where(mask)\n",
    "#wofs_alltime = wofs_alltime.where(mask)\n",
    "#datacube.storage.storage.write_dataset_to_netcdf(landsat, results + AOI \"_\" + year + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#band indices calculation\n",
    "# def ndvi_ufunc(ds):\n",
    "#     def ndvi_func(nir, red):\n",
    "#         return ((nir - red)/(nir + red))\n",
    "    \n",
    "#     return xr.apply_ufunc(\n",
    "#         ndvi_func, ds.nir, ds.red,\n",
    "#         dask='parallelized',\n",
    "#         output_dtypes=[float])\n",
    "\n",
    "def brightness_ufunc(ds):\n",
    "    def brightness_func(g,r,nir,swir):\n",
    "        return (g**2 + r**2 + nir**2 + swir**2)**(1/2.0)\n",
    "    \n",
    "    return xr.apply_ufunc(\n",
    "        brightness_func, ds.green,ds.red, ds.nir, ds.swir1,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "def greenness_ufunc(ds):\n",
    "    def greenness_func(nir,g):\n",
    "        return nir/g\n",
    "    \n",
    "    return xr.apply_ufunc(\n",
    "        greenness_func, ds.nir,ds.green,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "# ds.nir - ds.swir1)/(ds.nir + ds.swir1\n",
    "\n",
    "# NDVI_landsat = ndvi_ufunc(landsat)\n",
    "NDMI_landsat = BandIndices.calculate_indices(landsat, index='NDMI-nir')\n",
    "brightness_landsat = brightness_ufunc(landsat)\n",
    "greenness_landsat = greenness_ufunc(landsat)\n",
    "\n",
    "#calculate NDVI & NDMI stats\n",
    "\n",
    "print('calculating NDVI & NDMI stats')\n",
    "# NDVI_max = NDVI_landsat.max('time').rename('NDVI_max')\n",
    "# NDVI_mean = NDVI_landsat.mean('time').rename('NDVI_mean')\n",
    "# NDVI_std = NDVI_landsat.std('time').rename('NDVI_std')\n",
    "# NDVI_min = NDVI_landsat.min('time').rename('NDVI_min')\n",
    "# NDVI_range = NDVI_max - NDVI_min\n",
    "# NDVI_range = NDVI_range.rename('NDVI_range')\n",
    "\n",
    "green_max = greenness_landsat.max('time').rename('NDVI_max')\n",
    "green_mean = greenness_landsat.mean('time').rename('NDVI_mean')\n",
    "green_std = greenness_landsat.std('time').rename('NDVI_std')\n",
    "green_min = greenness_landsat.min('time').rename('NDVI_min')\n",
    "green_range = green_max - green_min\n",
    "green_range = green_range.rename('NDVI_range')\n",
    "\n",
    "NDMI_max = NDMI_landsat.max('time').rename('NDMI_max')\n",
    "NDMI_mean = NDMI_landsat.mean('time').rename('NDMI_mean')\n",
    "NDMI_std = NDMI_landsat.std('time').rename('NDMI_std')\n",
    "NDMI_min = NDMI_landsat.min('time').rename('NDMI_min')\n",
    "\n",
    "#brightness stats\n",
    "brightness_max = brightness_landsat.max('time').rename('brightness_max')\n",
    "brightness_mean = brightness_landsat.mean('time').rename('brightness_mean')\n",
    "brightness_std = brightness_landsat.std('time').rename('brightness_std')\n",
    "brightness_min = brightness_landsat.min('time').rename('brightness_min')\n",
    "\n",
    "greenness_landsat_resample = greenness_landsat.resample(time='M').mean('time')\n",
    "\n",
    "def nanarg(xarr, dim, stat):\n",
    "    \"\"\"\n",
    "    Deals with all-NaN slices by first identifying the offending cells\n",
    "    to create a mask, then filling NaNs with an integer to calculate\n",
    "    nanargmax(min), then masking out all-NaN cells using the mask.\n",
    "    The fill number is set such that it will never be returned as the min\n",
    "    or max of the argmax(min).\n",
    "    \"\"\"\n",
    "    mask = xarr.min(dim=dim, skipna=True).isnull()\n",
    "    if stat=='max':\n",
    "        fill = np.nanmin(xarr.values) -1\n",
    "        y = xarr.fillna(fill)\n",
    "        y = y.argmax(dim=dim, skipna=True).astype(float)\n",
    "        y = y.where(~mask)\n",
    "        return y\n",
    "    if stat == 'min':\n",
    "        fill = np.nanmax(xarr.values) + 1\n",
    "        y = xarr.fillna(fill)\n",
    "        y = y.argmin(dim=dim, skipna=True).astype(float)\n",
    "        y = y.where(~mask)\n",
    "        return y\n",
    "print(\"calculating rate etc\")\n",
    "timeofmax = nanarg(greenness_landsat_resample,dim ='time', stat='max')\n",
    "timeofmin = nanarg(greenness_landsat_resample,dim ='time', stat='min')\n",
    "\n",
    "rate = (green_max-green_min)/(timeofmax - timeofmin)\n",
    "rate = rate.where(~np.isinf(rate), other=3) #remove unreasonable values #remove unreasonable values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI_max, NDVI_mean, NDVI_std, NDVI_min, NDVI_range,\n",
    "xray_list = [green_max, green_mean, green_std, green_min, green_range,\n",
    "             NDMI_max, NDMI_mean, NDMI_std, NDMI_min,timeofmax, timeofmin, rate,\n",
    "             brightness_max, brightness_mean, brightness_std, brightness_std]\n",
    "names = ['green_max', 'green_mean', 'green_std', 'green_min', 'green_range',\n",
    "         'NDMI_max', 'NDMI_mean', 'NDMI_std', 'NDMI_min','timeofmax', 'timeofmin','rate',\n",
    "            'brightness_max', 'brightness_mean', 'brightness_std', 'brightness_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation for use in masking AFTER the RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export Gtiff for use in Image segmentation\n",
    "transform, projection = transform_tuple(green_max, (green_max.x, green_max.y), epsg=3577)\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"_green_max.tif\",\n",
    "              green_max.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=np.nan)\n",
    "\n",
    "# #export all GTiffs for catchment so I don't have to keep loading them.\n",
    "# for l, n in zip(xray_list, names):\n",
    "#     SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"_\" + n + \".tif\",\n",
    "#               l.values, geo_transform = transform, \n",
    "#               projection = projection, nodata_val=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputNDVIStats = results + AOI + \"_\" + year + \"_green_max.tif\"\n",
    "KEAFile = results + AOI + '_' + year + '.kea'\n",
    "SegmentedKEAFile = results + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "SegmentedTiffFile = results + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "SegmentedPolygons = results + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, minPxls = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a training dataset\n",
    "\n",
    "I was generating the training dataset in the code below that is now commented out,\n",
    "\n",
    "have since moved to generating a random dataset using R.  10,000 points extracted\n",
    "\n",
    "per class across from the OEH landuse layer. Rasterizing this file directly. UPDATE: The training dataset\n",
    "\n",
    "is now being hand drawn using sentinel imagery because the OEH landuse later isn't fit-for-purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rasterize the training dataset\n",
    "# NDVI_max = xr.open_rasterio(results + AOI + \"_\" + year + \"_NDVI_max.tif\")\n",
    "# NDVI_max = NDVI_max.squeeze()\n",
    "# #get the transform and projection of our gtiff\n",
    "# transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "# #find the width and height of the xarray dataset we want to mask\n",
    "# width,height = NDVI_max.shape\n",
    "# # rasterize vector\n",
    "# training_set = SpatialTools.rasterize_vector(results + \"murrumbidgee_training_samples.shp\",\n",
    "#                height, width, transform, projection, field='id',raster_path= results + AOI + \"_\" + year +'training_raster.tif')\n",
    "# #xr.DataArray(training_set, coords = [NDVI_max.y, NDVI_max.x], dims = ['y', 'x'], name='training areas').plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peel_landuse = gpd.read_file('results/Murrum_randomForest_Winter2013/hand_trainingSet.shp')\n",
    "peel_landuse = peel_landuse.to_crs(epsg=3577)\n",
    "\n",
    "peel_trainset = peel_landuse[(peel_landuse.Id == 430)| # irrigated cropping\n",
    "                        (peel_landuse.Id == 330) |      #cropping\n",
    "                        (peel_landuse.Id == 133) |      #native cover (bushland)\n",
    "                        (peel_landuse.Id == 541)]       #urban\n",
    "\n",
    "peel_trainset = peel_trainset[['Id', 'geometry']]\n",
    "\n",
    "peel_trainset = peel_trainset.replace([330,133,541], 10) #testing just two classes\n",
    "\n",
    "peel_trainset.to_file(results + \"waggaHAND_trainset.shp\")\n",
    "peel_trainset.plot(column = 'Id', legend=True, figsize=(7,7))\n",
    "\n",
    "# #rasterize the training dataset\n",
    "NDVI_max = xr.open_rasterio(results + AOI + \"_\" + year + \"_green_max.tif\")\n",
    "NDVI_max = NDVI_max.squeeze()\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = NDVI_max.shape\n",
    "# rasterize vector\n",
    "training_set = SpatialTools.rasterize_vector(results + \"trainingset_ready.shp\",\n",
    "               height, width, transform, projection, field='Id',raster_path= results + AOI + \"_\" + year +'training_raster.tif')\n",
    "# xr.DataArray(training_set, coords = [NDVI_max.y, NDVI_max.x], dims = ['y', 'x'], name='training areas').plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = xr.open_rasterio(results + AOI + \"_\" + year +'training_raster.tif')\n",
    "k = k.squeeze()\n",
    "classes = np.unique(k)\n",
    "for c in classes:\n",
    "    print('Class {c} contains {n} pixels'.format(c=c,n=(training_set == c).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our training data\n",
    "roi_ds = gdal.Open(results + AOI + \"_\" + year +'training_raster.tif', gdal.GA_ReadOnly)\n",
    "roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.uint16)\n",
    "\n",
    "# convert to numpy arrays (SLOW BECAUSE DASK ARRAYS ARE COMPUTED)\n",
    "x,y = NDVI_max.shape\n",
    "z = len(xray_list)\n",
    "img = np.zeros((x,y,z))\n",
    "count=0\n",
    "for b,c in zip(xray_list, range(img.shape[2])):\n",
    "    count += 1\n",
    "    progress = round((count/z) * 100, 3)\n",
    "    print(\"\\r\", \"adding slice: \" + str(count) + \", \" + str(progress) + \"%\" + \" complete. \", end = '')\n",
    "    img[:, :, c] = b.values \n",
    "    \n",
    "img_noNaNs = np.nan_to_num(img) #remove nans as they f/w classifier\n",
    "np.save(results + 'img_noNaNs.npy', img_noNaNs) #save it as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell if importing .npy file\n",
    "# img_noNaNs = np.load(results + 'img_noNaNs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display them\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_noNaNs[:, :, 1])\n",
    "plt.title('NDVI')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi, cmap=plt.cm.jet)\n",
    "plt.title('AOI Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many non-zero entries we have -- i.e. how many training data samples?\n",
    "n_samples = (roi > 0).sum()\n",
    "print('We have {n} samples'.format(n=n_samples))\n",
    "\n",
    "# What are our classification labels?\n",
    "labels = np.unique(roi[roi > 0])\n",
    "print('The training data include {n} classes: {classes}'.format(n=labels.size, \n",
    "                                                                classes=labels))\n",
    "x = img_noNaNs[roi > 0,:]\n",
    "y = roi[roi > 0]\n",
    "\n",
    "print('Our x matrix is sized: {sz}'.format(sz=x.shape))\n",
    "print('Our y array is sized: {sz}'.format(sz=y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize our model with 300 trees, and use 4 cpus\n",
    "rf = RandomForestClassifier(n_estimators=300, oob_score=True, verbose=True,\n",
    "                            n_jobs=4, max_features=\"auto\") #auto = sqrt(n_features)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "from joblib import dump, load\n",
    "dump(rf, results + 'murrumbidgee_rfModel_binary.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the importance of the individual bands\n",
    "for b, imp in zip(names, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation dataframe to check out how each class performs\n",
    "df = pd.DataFrame()\n",
    "df['truth'] = y\n",
    "df['predict'] = rf.predict(x)\n",
    "\n",
    "# Cross-tabulate predictions\n",
    "print(pd.crosstab(df['truth'], df['predict'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to load back in the trained RF model:\n",
    "# rf = load(results + 'murrumbidgee_rfModel.joblib')\n",
    "\n",
    "# Take our full image, and reshape into long 2d array (nrow * ncol, nband) for classification\n",
    "new_shape = (img_noNaNs.shape[0] * img_noNaNs.shape[1], img_noNaNs.shape[2])\n",
    "\n",
    "img_as_array = img_noNaNs[:, :, :z].reshape(new_shape)\n",
    "print('Reshaped from {o} to {n}'.format(o=img_noNaNs.shape,\n",
    "                                        n=img_as_array.shape))\n",
    "\n",
    "# Now predict for each pixel\n",
    "print('generating prediction')\n",
    "class_prediction = rf.predict(img_as_array)\n",
    "\n",
    "# Reshape our classification map\n",
    "class_prediction = class_prediction.reshape(img_noNaNs[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a plot of the predictions\n",
    "import matplotlib.patches as mpatches\n",
    "values = np.unique(class_prediction.ravel())\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(class_prediction, interpolation='none')\n",
    "# colors = [im.cmap(im.norm(value)) for value in values]\n",
    "# patches = [ mpatches.Patch(color=colors[i], label=\"Level {l}\".format(l=values[i]) ) for i in range(len(values)) ]\n",
    "# plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "plt.subplot(122)\n",
    "irr = np.where(class_prediction==430, 1, 0)\n",
    "plt.imshow(irr)\n",
    "plt.title('Irrigation Pixels Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export out the results\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"classpredict_handtrain_binary.tif\",\n",
    "              class_prediction, geo_transform = transform, \n",
    "              projection = projection, nodata_val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export eroded classified image for comparison\n",
    "from scipy.ndimage import morphology\n",
    "x=np.where(class_prediction==10, 0, 1)\n",
    "y = morphology.binary_erosion(x)\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"classpredict_handtrain_binaryeroded.tif\",\n",
    "              y, geo_transform = transform, \n",
    "              projection = projection, nodata_val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use image segmentation polygons to filter results of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predict = xr.open_rasterio(results + AOI + \"_\" + year + \"classpredict_handtrain_binary.tif\")\n",
    "class_predict = class_predict.squeeze()\n",
    "\n",
    "gdf = gpd.read_file(results + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "#calculate majority values inside segments\n",
    "gdf['majority'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=results + AOI + \"_\" + year + \"classpredict_handtrain.tif\", stats='majority'))['majority']\n",
    "#calculate area of polygons\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "#filter by area and majority values\n",
    "smallArea = gdf['area'] <= 5500000\n",
    "irrigated = gdf['majority'] == 430.0 #filtering for irrigated areas only\n",
    "gdf = gdf[smallArea&irrigated]\n",
    "#export shapefile\n",
    "gdf.to_file(results + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(class_predict, (class_predict.x, class_predict.y), epsg=3577)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = class_predict.shape\n",
    "# rasterize vector\n",
    "gdf_raster = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                           height, width, transform, projection, raster_path=results + AOI + \"_\" + year + \"_Irrigated.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
