{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal, ogr, gdal_array\n",
    "import dask\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "import os\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices, DEADataHandling\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from query_from_shp import query_from_shp\n",
    "from rsgislib.segmentation import segutils\n",
    "from rasterstats import zonal_stats\n",
    "from imageSeg import imageSeg\n",
    "import fiona\n",
    "import rasterio.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is your data and results folder?\n",
    "results = \"results/\"\n",
    "data = 'data/training_data/'\n",
    "\n",
    "sensors = ['ls5','ls7','ls8']\n",
    "\n",
    "#are we using a polygon to mask the AOI?\n",
    "shp_fpath = 'data/spatial/PeelR_AOI_Claire.shp'\n",
    "\n",
    "#Input your area of interest's name, coords, and \n",
    "#the year you're interested in?\n",
    "AOI = 'Peel_randomForest'\n",
    "year = 'Summer2012-13'\n",
    "\n",
    "time_period = ('2015-10-15', '2016-03-31')\n",
    "\n",
    "#What thresholds should I use?\n",
    "threshold = 0.8\n",
    "wofs_theshold = 0.15\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder to keep things neat\n",
    "directory = results + AOI + \"_\" + year\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "results = results + AOI + \"_\" + year + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls5_loading...\n",
      "ls5_loaded\n",
      "ls7_loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls7_loaded\n",
      "ls8_loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls8_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    }
   ],
   "source": [
    "#set up query\n",
    "query = query_from_shp(shp_fpath, time_period[0], time_period[1], dask_chunks = 1000)\n",
    "#landsat\n",
    "# dc = datacube.Datacube(app='dc_name')\n",
    "# landsat = DEADataHandling.load_clearlandsat(dc,query=query, sensors=sensors, product='nbart',\n",
    "#                        masked_prop=0.75)\n",
    "\n",
    "landsat = load_data(dc_name = 'irrigated_areas', sensors=sensors,\n",
    "          export_name = data + AOI + \"_\" + year + '.nc', query=query)\n",
    "#wofs\n",
    "dc = datacube.Datacube(app='wofs')\n",
    "del query['time'] \n",
    "wofs_alltime = dc.load(product = 'wofs_summary', **query)\n",
    "\n",
    "#masking the returned array to the polygon area\n",
    "with fiona.open(shp_fpath) as shapes:\n",
    "        crs = geometry.CRS(shapes.crs_wkt)\n",
    "        first_geometry = next(iter(shapes))['geometry']\n",
    "        geom = geometry.Geometry(first_geometry, crs=crs)\n",
    "\n",
    "mask = rasterio.features.geometry_mask([geom.to_crs(landsat.geobox.crs) for geoms in [geom]],\n",
    "                                           out_shape=landsat.geobox.shape,\n",
    "                                           transform=landsat.geobox.affine,\n",
    "                                           all_touched=False,\n",
    "                                           invert=True)\n",
    "# Mask the xarrays\n",
    "landsat = landsat.where(mask)\n",
    "#wofs_alltime = wofs_alltime.where(mask)\n",
    "#datacube.storage.storage.write_dataset_to_netcdf(landsat, results + AOI \"_\" + year + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula we are using is (nir - swir1)/(nir + swir1)\n"
     ]
    }
   ],
   "source": [
    "#band indices calculation\n",
    "def ndvi_func(nir, red):\n",
    "    return ((nir - red)/(nir + red))\n",
    "\n",
    "def ndvi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        ndvi_func, ds.nir, ds.red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "def brightness_func(g,r,nir,swir):\n",
    "    return (g**2 + r**2 + nir**2 + swir**2)**(1/2.0)\n",
    "\n",
    "def brightness_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        brightness_func, ds.green,ds.red, ds.nir, ds.swir1,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "NDVI_landsat = ndvi_ufunc(landsat)\n",
    "NDMI_landsat = BandIndices.calculate_indices(landsat, index='NDMI-nir')\n",
    "brightness_landsat = brightness_ufunc(landsat)\n",
    "\n",
    "#calculate per pixel summary stats\n",
    "NDVI_max = NDVI_landsat.max('time').rename('NDVI_max')\n",
    "NDVI_mean = NDVI_landsat.mean('time').rename('NDVI_mean')\n",
    "NDVI_std = NDVI_landsat.std('time').rename('NDVI_std')\n",
    "NDVI_min = NDVI_landsat.min('time').rename('NDVI_min')\n",
    "\n",
    "NDMI_max = NDMI_landsat.max('time').rename('NDMI_max')\n",
    "NDMI_mean = NDMI_landsat.mean('time').rename('NDMI_mean')\n",
    "NDMI_std = NDMI_landsat.std('time').rename('NDMI_std')\n",
    "NDMI_min = NDMI_landsat.min('time').rename('NDMI_min')\n",
    "\n",
    "# brightness_max = brightness_landsat.max('time').rename('brightness_max')\n",
    "# brightness_mean = brightness_landsat.mean('time').rename('brightness_mean')\n",
    "# brightness_std = brightness_landsat.std('time').rename('brightness_std')\n",
    "# brightness_min = brightness_landsat.min('time').rename('brightness_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_list = [NDVI_max, NDVI_mean, NDVI_std, NDVI_min, NDMI_max, NDMI_mean,NDMI_std, NDMI_min,\n",
    "            brightness_max, brightness_mean, brightness_std, brightness_min]\n",
    "names = ['NDVI_max', 'NDVI_mean', 'NDVI_std', 'NDVI_min', 'NDMI_max', 'NDMI_mean', 'NDMI_std', 'NDMI_min',\n",
    "       'brightness_max', 'brightness_mean', 'brightness_std', 'brightness_min']\n",
    "\n",
    "# ds_list = []\n",
    "# for i, j in zip(xray_list, names):\n",
    "#     x = i.to_dataset(name=j).chunk({'x':1000, 'y':1000})\n",
    "#     ds_list.append(x)\n",
    "\n",
    "# aoi_params = xr.merge(ds_list)\n",
    "# aoi_params.attrs=dict(crs=landsat.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation and mean filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export Gtiff for use in Image segmentation\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"ndvimax.tif\",\n",
    "              NDVI_max.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputNDVIStats = results + AOI + \"_\" + year + \"ndvimax.tif\"\n",
    "KEAFile = results + AOI + '_' + year + '.kea'\n",
    "SegmentedKEAFile = results + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "SegmentedTiffFile = results + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "SegmentedPolygons = results + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, minPxls = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peel_landuse = gpd.read_file('data/spatial/Peel_landuse_small.shp')\n",
    "peel_landuse = peel_landuse.to_crs(epsg=3577)\n",
    "\n",
    "peel_trainset = peel_landuse[(peel_landuse.TertiaryAL == 430) | #irrigated cropping\n",
    "                        (peel_landuse.TertiaryAL == 330) |      #cropping\n",
    "                        (peel_landuse.TertiaryAL == 220) |      #forestry\n",
    "                        (peel_landuse.TertiaryAL == 133) |      #native cover (bushland)\n",
    "                        (peel_landuse.TertiaryAL == 541)]       #urban\n",
    "\n",
    "peel_trainset = peel_trainset[['TertiaryAL', 'd_Tertiary', 'geometry']]\n",
    "peel_trainset.columns = ['id', 'class', 'geometry']\n",
    "peel_trainset.to_file(results + AOI + \"_\" + year + \"_peel_trainset.shp\")\n",
    "peel_trainset.plot(column = 'class', legend=True, figsize=(7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI_max = xr.open_rasterio(results + AOI + \"_\" + year + \"ndvimax.tif\")\n",
    "NDVI_max = NDVI_max.squeeze()\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = NDVI_max.shape\n",
    "# rasterize vector\n",
    "training_set = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_peel_trainset.shp\",\n",
    "               height, width, transform, projection, field='id',raster_path= results + AOI + \"_\" + year +'training_raster.tif')\n",
    "#xr.DataArray(training_set, coords = [NDVI_max.y, NDVI_max.x], dims = ['y', 'x'], name='training areas').plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(training_set)\n",
    "for c in classes:\n",
    "    print('Class {c} contains {n} pixels'.format(c=c,n=(training_set == c).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our training data\n",
    "roi_ds = gdal.Open(results + AOI + \"_\" + year +'training_raster.tif', gdal.GA_ReadOnly)\n",
    "roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.uint16)\n",
    "#convert to numpy arrays (SLOW BECAUSE DASK ARRAYS ARE COMPUTED)\n",
    "x,y = NDVI_max.shape\n",
    "z = len(xray_list)\n",
    "img = np.zeros((x,y,z))\n",
    "for b,c in zip(xray_list, range(img.shape[2])):\n",
    "    img[:, :, c] = b.values \n",
    "    \n",
    "img_noNaNs = np.nan_to_num(img) #remove nans as they f/w classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display them\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_noNaNs[:, :, 1])\n",
    "plt.title('NDVI')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi, cmap=plt.cm.Spectral)\n",
    "plt.title('AOI Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many non-zero entries we have -- i.e. how many training data samples?\n",
    "n_samples = (roi > 0).sum()\n",
    "print('We have {n} samples'.format(n=n_samples))\n",
    "\n",
    "# What are our classification labels?\n",
    "labels = np.unique(roi[roi > 0])\n",
    "print('The training data include {n} classes: {classes}'.format(n=labels.size, \n",
    "                                                                classes=labels))\n",
    "x = img_noNaNs[roi > 0,:]\n",
    "y = roi[roi > 0]\n",
    "\n",
    "print('Our X matrix is sized: {sz}'.format(sz=x.shape))\n",
    "print('Our y array is sized: {sz}'.format(sz=y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize our model with 500 trees\n",
    "rf = RandomForestClassifier(n_estimators=500, oob_score=True, verbose=True)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disaply the importance of the individual bands\n",
    "for b, imp in zip(names, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation dataframe to check out how each class performs\n",
    "df = pd.DataFrame()\n",
    "df['truth'] = y\n",
    "df['predict'] = rf.predict(x)\n",
    "\n",
    "# Cross-tabulate predictions\n",
    "print(pd.crosstab(df['truth'], df['predict'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our full image, and reshape into long 2d array (nrow * ncol, nband) for classification\n",
    "new_shape = (img_noNaNs.shape[0] * img_noNaNs.shape[1], img_noNaNs.shape[2])\n",
    "\n",
    "img_as_array = img_noNaNs[:, :, :z].reshape(new_shape)\n",
    "print('Reshaped from {o} to {n}'.format(o=img_noNaNs.shape,\n",
    "                                        n=img_as_array.shape))\n",
    "\n",
    "# Now predict for each pixel\n",
    "class_prediction = rf.predict(img_as_array)\n",
    "\n",
    "# Reshape our classification map\n",
    "class_prediction = class_prediction.reshape(img_noNaNs[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i\n",
    "# class_prediction\n",
    "np.unique(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a plot of the predictions\n",
    "import matplotlib.patches as mpatches\n",
    "values = np.unique(class_prediction.ravel())\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(class_prediction, interpolation='none')\n",
    "# colors = [im.cmap(im.norm(value)) for value in values]\n",
    "# patches = [ mpatches.Patch(color=colors[i], label=\"Level {l}\".format(l=values[i]) ) for i in range(len(values)) ]\n",
    "# plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "plt.subplot(122)\n",
    "irr = np.where(class_prediction==430, 1, 0)\n",
    "plt.imshow(irr)\n",
    "plt.title('Irrigation Pixels Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export out the results\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"classpredict.tif\",\n",
    "              class_prediction, geo_transform = transform, \n",
    "              projection = projection, nodata_val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use image segmentation polygons to filter results of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_predict = xr.open_rasterio(results + AOI + \"_\" + year + \"classpredict.tif\")\n",
    "class_predict = class_predict.squeeze()\n",
    "\n",
    "gdf = gpd.read_file(results + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "#calculate majority of values inside segments\n",
    "gdf['majority'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=results + AOI + \"_\" + year + \"classpredict.tif\", stats='majority'))['majority']\n",
    "#calculate area of polygons\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "#filter by area and majority values\n",
    "smallArea = gdf['area'] <= 5500000\n",
    "irrigated = gdf['majority'] == 430.0 #filtering for irrigated areas only\n",
    "gdf = gdf[smallArea&irrigated]\n",
    "#export shapefile\n",
    "gdf.to_file(results + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(class_predict, (class_predict.x, class_predict.y), epsg=3577)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = class_predict.shape\n",
    "# rasterize vector\n",
    "gdf_raster = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                           height, width, transform, projection, raster_path=results + AOI + \"_\" + year + \"_Irrigated.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_raster = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_Irrigated.shp\", \n",
    "                                           height, width, transform, projection, raster_path=results + AOI + \"_\" + year + \"_Irrigated_new.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(gdf_raster, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(gdf_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
