{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal, ogr, gdal_array\n",
    "import dask\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "import os\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices, DEADataHandling\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from query_from_shp import query_from_shp\n",
    "from rsgislib.segmentation import segutils\n",
    "from rasterstats import zonal_stats\n",
    "from imageSeg import imageSeg\n",
    "import fiona\n",
    "import rasterio.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is your data and results folder?\n",
    "results = \"results/\"\n",
    "data = 'data/'\n",
    "\n",
    "sensors = ['ls5','ls7','ls8']\n",
    "\n",
    "#are we using a polygon to mask the AOI?\n",
    "shp_fpath = 'data/spatial/murrumbidgee_boundingbox.shp'\n",
    "\n",
    "#Input your area of interest's name, coords, and \n",
    "#the year you're interested in?\n",
    "AOI = 'Murrum_randomForest'\n",
    "year = 'Winter2013'\n",
    "\n",
    "time_period = ('2013-01-01', '2013-12-31')\n",
    "\n",
    "#What thresholds should I use?\n",
    "threshold = 0.8\n",
    "wofs_theshold = 0.15\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder to keep things neat\n",
    "directory = results + AOI + \"_\" + year\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "results = results + AOI + \"_\" + year + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls5_loading...\n",
      "ls5_loaded\n",
      "ls7_loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls7_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls8_loading...\n",
      "ls8_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n",
      "/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/xarray/core/indexing.py:1199: PerformanceWarning: Slicing with an out-of-order index is generating 38 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "#set up query\n",
    "query = query_from_shp(shp_fpath, time_period[0], time_period[1], dask_chunks = 1000)\n",
    "#landsat\n",
    "# dc = datacube.Datacube(app='dc_name')\n",
    "# landsat = DEADataHandling.load_clearlandsat(dc,query=query, sensors=sensors, product='nbart',\n",
    "#                        masked_prop=0.75)\n",
    "\n",
    "landsat = load_data(dc_name = 'irrigated_areas', sensors=sensors,\n",
    "          export_name = data + AOI + \"_\" + year + '.nc', query=query)\n",
    "#wofs\n",
    "# dc = datacube.Datacube(app='wofs')\n",
    "# del query['time'] \n",
    "# wofs_alltime = dc.load(product = 'wofs_summary', **query)\n",
    "\n",
    "#masking the returned array to the polygon area\n",
    "with fiona.open(shp_fpath) as shapes:\n",
    "        crs = geometry.CRS(shapes.crs_wkt)\n",
    "        first_geometry = next(iter(shapes))['geometry']\n",
    "        geom = geometry.Geometry(first_geometry, crs=crs)\n",
    "\n",
    "mask = rasterio.features.geometry_mask([geom.to_crs(landsat.geobox.crs) for geoms in [geom]],\n",
    "                                           out_shape=landsat.geobox.shape,\n",
    "                                           transform=landsat.geobox.affine,\n",
    "                                           all_touched=False,\n",
    "                                           invert=True)\n",
    "# Mask the xarrays\n",
    "landsat = landsat.where(mask)\n",
    "#wofs_alltime = wofs_alltime.where(mask)\n",
    "#datacube.storage.storage.write_dataset_to_netcdf(landsat, results + AOI \"_\" + year + '.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 99, x: 24218, y: 15524)\n",
       "Coordinates:\n",
       "  * y        (y) float64 -3.714e+06 -3.714e+06 ... -4.102e+06 -4.102e+06\n",
       "  * x        (x) float64 1.011e+06 1.011e+06 1.011e+06 ... 1.616e+06 1.616e+06\n",
       "  * time     (time) datetime64[ns] 2013-05-01T00:16:24 ... 2013-09-30T00:10:24.500000\n",
       "Data variables:\n",
       "    blue     (time, y, x) float64 dask.array<shape=(99, 15524, 24218), chunksize=(1, 1000, 1000)>\n",
       "    green    (time, y, x) float64 dask.array<shape=(99, 15524, 24218), chunksize=(1, 1000, 1000)>\n",
       "    red      (time, y, x) float64 dask.array<shape=(99, 15524, 24218), chunksize=(1, 1000, 1000)>\n",
       "    nir      (time, y, x) float64 dask.array<shape=(99, 15524, 24218), chunksize=(1, 1000, 1000)>\n",
       "    swir1    (time, y, x) float64 dask.array<shape=(99, 15524, 24218), chunksize=(1, 1000, 1000)>\n",
       "    swir2    (time, y, x) float64 dask.array<shape=(99, 15524, 24218), chunksize=(1, 1000, 1000)>\n",
       "Attributes:\n",
       "    crs:      EPSG:3577"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating NDVI stats\n",
      "calculating birghtness stats\n",
      "resampling timeseries\n",
      "calculating argmax stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/dask/array/numpy_compat.py:28: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.divide(x1, x2, out)\n"
     ]
    }
   ],
   "source": [
    "#band indices calculation\n",
    "\n",
    "def ndvi_ufunc(ds):\n",
    "    def ndvi_func(nir, red):\n",
    "        return ((nir - red)/(nir + red))\n",
    "    \n",
    "    return xr.apply_ufunc(\n",
    "        ndvi_func, ds.nir, ds.red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "def brightness_ufunc(ds):\n",
    "    def brightness_func(g,r,nir,swir):\n",
    "        return (g**2 + r**2 + nir**2 + swir**2)**(1/2.0)\n",
    "    \n",
    "    return xr.apply_ufunc(\n",
    "        brightness_func, ds.green,ds.red, ds.nir, ds.swir1,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "NDVI_landsat = ndvi_ufunc(landsat)\n",
    "brightness_landsat = brightness_ufunc(landsat)\n",
    "\n",
    "#calculate per pixel summary stats\n",
    "print('calculating NDVI stats')\n",
    "NDVI_max = NDVI_landsat.max('time').rename('NDVI_max')\n",
    "NDVI_mean = NDVI_landsat.mean('time').rename('NDVI_mean')\n",
    "NDVI_std = NDVI_landsat.std('time').rename('NDVI_std')\n",
    "NDVI_min = NDVI_landsat.min('time').rename('NDVI_min')\n",
    "NDVI_range = NDVI_max - NDVI_min\n",
    "NDVI_range = NDVI_range.rename('NDVI_range')\n",
    "\n",
    "print('calculating birghtness stats')\n",
    "brightness_max = brightness_landsat.max('time').rename('brightness_max')\n",
    "brightness_mean = brightness_landsat.mean('time').rename('brightness_mean')\n",
    "brightness_std = brightness_landsat.std('time').rename('brightness_std')\n",
    "brightness_min = brightness_landsat.min('time').rename('brightness_min')\n",
    "\n",
    "print('resampling timeseries')\n",
    "NDVI_landsat_resample = NDVI_landsat.resample(time='M').mean('time')\n",
    "y = NDVI_landsat.coords['y']\n",
    "x = NDVI_landsat.coords['x']\n",
    "\n",
    "print('calculating argmax stats') #SLOW BECAUSE DASK ARRAYS COMPUTED\n",
    "timeofmax = NDVI_landsat_resample.values.argmax(axis=0)\n",
    "timeofmax = xr.DataArray(timeofmax, coords = [y, x], dims = ['y', 'x'], name='time_of_max')\n",
    "timeofmin = NDVI_landsat_resample.values.argmin(axis=0)\n",
    "timeofmin = xr.DataArray(timeofmin, coords = [y, x], dims = ['y', 'x'], name='time_of_min')\n",
    "rate = (NDVI_max-NDVI_min)/(timeofmax - timeofmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xray_list = [NDVI_max, NDVI_mean, NDVI_std, NDVI_min, NDVI_range,timeofmax, timeofmin, rate,\n",
    "            brightness_max, brightness_mean, brightness_std, brightness_std]\n",
    "names = ['NDVI_max', 'NDVI_mean', 'NDVI_std', 'NDVI_min', 'NDVI_range','timeofmax', 'timeofmin','rate',\n",
    "            'brightness_max', 'brightness_mean', 'brightness_std', 'brightness_std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation for use in masking AFTER the RF classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export Gtiff for use in Image segmentation\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "# SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"ndvimax.tif\",\n",
    "#               NDVI_max.values, geo_transform = transform, \n",
    "#               projection = projection, nodata_val=np.nan)\n",
    "\n",
    "#export all GTiffs for catchment so I don't have to keep loading them.\n",
    "for l, n in zip(xray_list, names):\n",
    "    SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"_\" + n + \".tif\",\n",
    "              l.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputNDVIStats = results + AOI + \"_\" + year + \"_NDVI_max.tif\"\n",
    "KEAFile = results + AOI + '_' + year + '.kea'\n",
    "SegmentedKEAFile = results + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "SegmentedTiffFile = results + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "SegmentedPolygons = results + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, minPxls = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a training dataset\n",
    "\n",
    "I was generating the training dataset in the code that is now commented out,\n",
    "\n",
    "have since moved to generating a random dataset using R.  10,000 points extracted\n",
    "\n",
    "per class across the Murrumbidgee. Rasterizing this file directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rasterize the training dataset\n",
    "NDVI_max = xr.open_rasterio(results + AOI + \"_\" + year + \"_NDVI_max.tif\")\n",
    "NDVI_max = NDVI_max.squeeze()\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = NDVI_max.shape\n",
    "# rasterize vector\n",
    "training_set = SpatialTools.rasterize_vector(results + \"murrumbidgee_training_samples.shp\",\n",
    "               height, width, transform, projection, field='id',raster_path= results + AOI + \"_\" + year +'training_raster.tif')\n",
    "#xr.DataArray(training_set, coords = [NDVI_max.y, NDVI_max.x], dims = ['y', 'x'], name='training areas').plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peel_landuse = gpd.read_file('data/spatial/Peel_landuse_small.shp')\n",
    "# peel_landuse = peel_landuse.to_crs(epsg=3577)\n",
    "\n",
    "# peel_trainset = peel_landuse[(peel_landuse.TertiaryAL == 430) | #irrigated cropping\n",
    "#                         (peel_landuse.TertiaryAL == 330) |      #cropping\n",
    "#                         (peel_landuse.TertiaryAL == 220) |      #forestry\n",
    "#                         (peel_landuse.TertiaryAL == 133) |      #native cover (bushland)\n",
    "#                         (peel_landuse.TertiaryAL == 541)]       #urban\n",
    "\n",
    "# peel_trainset = peel_trainset[['TertiaryAL', 'd_Tertiary', 'geometry']]\n",
    "# peel_trainset.columns = ['id', 'class', 'geometry']\n",
    "# peel_trainset.to_file(results + AOI + \"_\" + year + \"_peel_trainset.shp\")\n",
    "# peel_trainset.plot(column = 'class', legend=True, figsize=(7,7))\n",
    "\n",
    "# #rasterize the training dataset\n",
    "# NDVI_max = xr.open_rasterio(results + AOI + \"_\" + year + \"ndvimax.tif\")\n",
    "# NDVI_max = NDVI_max.squeeze()\n",
    "# #get the transform and projection of our gtiff\n",
    "# transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "# #find the width and height of the xarray dataset we want to mask\n",
    "# width,height = NDVI_max.shape\n",
    "# # rasterize vector\n",
    "# training_set = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_peel_trainset.shp\",\n",
    "#                height, width, transform, projection, field='id',raster_path= results + AOI + \"_\" + year +'training_raster.tif')\n",
    "# #xr.DataArray(training_set, coords = [NDVI_max.y, NDVI_max.x], dims = ['y', 'x'], name='training areas').plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = xr.open_rasterio(results + AOI + \"_\" + year +'training_raster.tif')\n",
    "k = k.squeeze()\n",
    "classes = np.unique(k)\n",
    "for c in classes:\n",
    "    print('Class {c} contains {n} pixels'.format(c=c,n=(training_set == c).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in our training data\n",
    "roi_ds = gdal.Open(results + AOI + \"_\" + year +'training_raster.tif', gdal.GA_ReadOnly)\n",
    "roi = roi_ds.GetRasterBand(1).ReadAsArray().astype(np.uint16)\n",
    "#convert to numpy arrays (SLOW BECAUSE DASK ARRAYS ARE COMPUTED)\n",
    "x,y = NDVI_max.shape\n",
    "z = len(xray_list)\n",
    "img = np.zeros((x,y,z))\n",
    "for b,c in zip(xray_list, range(img.shape[2])):\n",
    "    print('adding slice to array...')\n",
    "    img[:, :, c] = b.values \n",
    "    \n",
    "img_noNaNs = np.nan_to_num(img) #remove nans as they f/w classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display them\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_noNaNs[:, :, 1])\n",
    "plt.title('NDVI')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(roi, cmap=plt.cm.Spectral)\n",
    "plt.title('AOI Training Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how many non-zero entries we have -- i.e. how many training data samples?\n",
    "n_samples = (roi > 0).sum()\n",
    "print('We have {n} samples'.format(n=n_samples))\n",
    "\n",
    "# What are our classification labels?\n",
    "labels = np.unique(roi[roi > 0])\n",
    "print('The training data include {n} classes: {classes}'.format(n=labels.size, \n",
    "                                                                classes=labels))\n",
    "x = img_noNaNs[roi > 0,:]\n",
    "y = roi[roi > 0]\n",
    "\n",
    "print('Our X matrix is sized: {sz}'.format(sz=x.shape))\n",
    "print('Our y array is sized: {sz}'.format(sz=y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Initialize our model with 500 trees\n",
    "rf = RandomForestClassifier(n_estimators=250, oob_score=True, verbose=True,\n",
    "                            n_jobs=4, max_features=\"auto\") #auto = sqrt(n_features)\n",
    "\n",
    "# Fit our model to training data\n",
    "rf = rf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "from joblib import dump, load\n",
    "dump(rf, results + 'murrumbidgee_rfModel.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our OOB prediction of accuracy is: {oob}%'.format(oob=rf.oob_score_ * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disaply the importance of the individual bands\n",
    "for b, imp in zip(names, rf.feature_importances_):\n",
    "    print('Band {b} importance: {imp}'.format(b=b, imp=imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation dataframe to check out how each class performs\n",
    "df = pd.DataFrame()\n",
    "df['truth'] = y\n",
    "df['predict'] = rf.predict(x)\n",
    "\n",
    "# Cross-tabulate predictions\n",
    "print(pd.crosstab(df['truth'], df['predict'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we need to load back in the trained RF model:\n",
    "# rf = load(results + 'murrumbidgee_rfModel.joblib')\n",
    "\n",
    "# Take our full image, and reshape into long 2d array (nrow * ncol, nband) for classification\n",
    "new_shape = (img_noNaNs.shape[0] * img_noNaNs.shape[1], img_noNaNs.shape[2])\n",
    "\n",
    "img_as_array = img_noNaNs[:, :, :z].reshape(new_shape)\n",
    "print('Reshaped from {o} to {n}'.format(o=img_noNaNs.shape,\n",
    "                                        n=img_as_array.shape))\n",
    "\n",
    "# Now predict for each pixel\n",
    "print('generating prediction')\n",
    "class_prediction = rf.predict(img_as_array)\n",
    "\n",
    "# Reshape our classification map\n",
    "class_prediction = class_prediction.reshape(img_noNaNs[:, :, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a plot of the predictions\n",
    "import matplotlib.patches as mpatches\n",
    "values = np.unique(class_prediction.ravel())\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(121)\n",
    "im = plt.imshow(class_prediction, interpolation='none')\n",
    "# colors = [im.cmap(im.norm(value)) for value in values]\n",
    "# patches = [ mpatches.Patch(color=colors[i], label=\"Level {l}\".format(l=values[i]) ) for i in range(len(values)) ]\n",
    "# plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\n",
    "\n",
    "plt.subplot(122)\n",
    "irr = np.where(class_prediction==430, 1, 0)\n",
    "plt.imshow(irr)\n",
    "plt.title('Irrigation Pixels Only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export out the results\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"classpredict.tif\",\n",
    "              class_prediction, geo_transform = transform, \n",
    "              projection = projection, nodata_val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use image segmentation polygons to filter results of RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predict = xr.open_rasterio(results + AOI + \"_\" + year + \"classpredict.tif\")\n",
    "class_predict = class_predict.squeeze()\n",
    "\n",
    "gdf = gpd.read_file(results + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "#calculate majority values inside segments\n",
    "gdf['majority'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=results + AOI + \"_\" + year + \"classpredict.tif\", stats='majority'))['majority']\n",
    "#calculate area of polygons\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "#filter by area and majority values\n",
    "smallArea = gdf['area'] <= 5500000\n",
    "irrigated = gdf['majority'] == 430.0 #filtering for irrigated areas only\n",
    "gdf = gdf[smallArea&irrigated]\n",
    "#export shapefile\n",
    "gdf.to_file(results + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(class_predict, (class_predict.x, class_predict.y), epsg=3577)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = class_predict.shape\n",
    "# rasterize vector\n",
    "gdf_raster = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                           height, width, transform, projection, raster_path=results + AOI + \"_\" + year + \"_Irrigated.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(gdf_raster, interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
