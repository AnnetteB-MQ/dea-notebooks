{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "\n",
    "import fiona\n",
    "import rasterio.features\n",
    "from osgeo import gdal, ogr\n",
    "import os\n",
    "from rsgislib.segmentation import segutils\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from imageSeg import imageSeg\n",
    "from query_from_shp import query_from_shp\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up query and analysis parameters\n",
    "Here we set a centroid for the area we want to compare, and set up CRS, resolution and resampling that will be applied to both collectiondatasets. The values below extract both collections to match the collection upgrade CRS and resolution (UTM zone 56 S and 30m pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is your data and results folder?\n",
    "data = 'data/'\n",
    "results = 'results/'\n",
    "\n",
    "#Input your area of interest's name, coords, and \n",
    "#the year you're interested in?\n",
    "AOI = 'collectionTest'\n",
    "year = 'Summer2017-18'\n",
    "# Centre point of spatial query\n",
    "lat, lon = -35.969, 145.394\n",
    "time_period = ('2017-11-01', '2018-03-31')\n",
    "\n",
    "# Desired output resolution and projection for both datasets\n",
    "output_crs = 'EPSG:28355'\n",
    "output_resolution = (30, 30)\n",
    "output_resamp_continuous = 'bilinear'\n",
    "output_resamp_categorical = 'nearest'\n",
    "\n",
    "# Bands/measurements to load\n",
    "collectionupgrade_bands = ['nbart_red','nbart_nir']\n",
    "currentcollection_bands = ['red', 'nir']\n",
    "\n",
    "# Set up query\n",
    "query = {'lon': (lon - 0.15, lon + 0.15),\n",
    "         'lat': (lat - 0.15, lat + 0.15),\n",
    "         'time': time_period}\n",
    "\n",
    "# Connect to current collection and collection upgrade databases\n",
    "dc_ard = datacube.Datacube(config='/g/data1a/u46/users/cb3058/datacube.conf', env='datacube_upgrade')\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "# Preview spatial query\n",
    "# DEAPlotting.display_map(x=query['lon'], y=query['lat'], crs='EPSG:4326')\n",
    "\n",
    "#What thresholds should I use?\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder to keep things neat\n",
    "directory = results + AOI + \"_\" + year\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "results = results + AOI + \"_\" + year + \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in collection upgrade data (DEA Landsat Collection 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray_dict = {}\n",
    "\n",
    "# Iterate through each sensor\n",
    "for sensor in ['ls8']:\n",
    "\n",
    "    # Load data \n",
    "    landsat_ds = dc_ard.load(product = f'{sensor}_ard', \n",
    "                             measurements = collectionupgrade_bands,\n",
    "                             output_crs = output_crs,\n",
    "                             resolution = output_resolution,\n",
    "                             resampling = output_resamp_continuous,\n",
    "                             align=(output_resolution[0]/2.0, output_resolution[1]/2.0),\n",
    "                             group_by = 'solar_day', \n",
    "                             **query)\n",
    "    \n",
    "    # Load PQ data seperately (this enables using a different resampling method on \n",
    "    # continuous surface reflectance values vs categorical fmask/PQ values)\n",
    "    landsat_pq = dc_ard.load(product = f'{sensor}_ard', \n",
    "                             measurements = ['fmask'],\n",
    "                             output_crs = output_crs,\n",
    "                             resolution = output_resolution,\n",
    "                             resampling = output_resamp_categorical,\n",
    "                             align=(output_resolution[0]/2.0, output_resolution[1]/2.0),\n",
    "                             group_by = 'solar_day', \n",
    "                             **query)\n",
    "\n",
    "    # Identify pixels with valid data: no nodata AND no cloud AND no cloud shadow\n",
    "    good_quality = ((landsat_pq.fmask != 0) & \n",
    "                    (landsat_pq.fmask != 2) & \n",
    "                    (landsat_pq.fmask != 3))\n",
    "\n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    landsat_ds = landsat_ds.where(good_quality)\n",
    "\n",
    "    # Add to list\n",
    "    xarray_dict[sensor] = landsat_ds\n",
    "\n",
    "# Concatenate multiple sensors into one dataset\n",
    "landsat_collectionupgrade = xr.concat(xarray_dict.values(), dim='time')\n",
    "landsat_collectionupgrade = landsat_collectionupgrade.sortby('time')\n",
    "landsat_collectionupgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in current collection data (DEA Landsat Collection 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray_dict = {}\n",
    "\n",
    "for sensor in ['ls8']:\n",
    "\n",
    "    # Load data \n",
    "    landsat_ds = dc.load(product = f'{sensor}_nbart_albers', \n",
    "                         measurements = currentcollection_bands,\n",
    "                         output_crs = output_crs,\n",
    "                         resolution = output_resolution,\n",
    "                         resampling = output_resamp_continuous,\n",
    "                         group_by = 'solar_day', \n",
    "                         **query)\n",
    "\n",
    "    # Load PQ data \n",
    "    landsat_pq = dc.load(product = f'{sensor}_pq_albers', \n",
    "                         measurements = ['pixelquality'],\n",
    "                         output_crs = output_crs,\n",
    "                         resolution = output_resolution,\n",
    "                         resampling = output_resamp_categorical,\n",
    "                         group_by = 'solar_day', \n",
    "                         **query)                       \n",
    "\n",
    "    # Filter to subset of Landsat observations that have matching PQ data \n",
    "    time = (landsat_ds.time - landsat_pq.time).time\n",
    "    landsat_ds = landsat_ds.sel(time=time)\n",
    "    landsat_pq = landsat_pq.sel(time=time)\n",
    "\n",
    "    # Create PQ mask\n",
    "    good_quality = masking.make_mask(landsat_pq.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True) \n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    landsat_ds = landsat_ds.where(good_quality)\n",
    "\n",
    "    # Add result to dict\n",
    "    xarray_dict[sensor] = landsat_ds\n",
    "\n",
    "# Concatenate multiple sensors into one dataset\n",
    "landsat_currentcollection = xr.concat(xarray_dict.values(), dim='time')\n",
    "landsat_currentcollection = landsat_ds.sortby('time')\n",
    "landsat_currentcollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round both dataset timestamps to daily because both datasets have different timestamps\n",
    "landsat_collectionupgrade.coords['time'] = landsat_collectionupgrade.time.astype('datetime64[D]')\n",
    "landsat_currentcollection.coords['time'] = landsat_currentcollection.time.astype('datetime64[D]')\n",
    "\n",
    "# Rename current collection bands to match collection upgrade\n",
    "band_rename_dict = {'red': 'nbart_red', \n",
    "                    'nir': 'nbart_nir' }\n",
    "landsat_currentcollection.rename(band_rename_dict, inplace=True)\n",
    "\n",
    "# Unfortunately due to the different pixel definitions for the DEA Landsat Collections 2 and 3,\n",
    "# the same spatial query can result in a different number of rows and columns. Because of this,\n",
    "# the only way we can directly compare the two collections is to force them to match by resampling\n",
    "# one to the rows x columns of the other. This will introduce some uncertainty in the comparison.\n",
    "landsat_currentcollection = landsat_currentcollection.interp_like(landsat_collectionupgrade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MaxNDVI irrigated extent algorithm on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#band indices calculation\n",
    "def ndvi_func(nir, red):\n",
    "    return ((nir - red)/(nir + red))\n",
    "\n",
    "def ndvi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        ndvi_func, ds.nbart_nir, ds.nbart_red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "print(\"calculating NDVI\")\n",
    "NDVI_landsat = ndvi_ufunc(landsat_currentcollection)\n",
    "\n",
    "#calculate per pixel summary stats\n",
    "print(\"calculating summary stats\")\n",
    "NDVI_max = NDVI_landsat.groupby('x','y').max('time').rename('NDVI_max')\n",
    "# NDVI_max = NDVI_max.chunk({'x':1000, 'y':1000})\n",
    "\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=28355)\n",
    "print(\"exporting MaxNDVI GTiff\")\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \".tif\",\n",
    "              NDVI_max.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=-9999.)\n",
    "\n",
    "# ## Image Segmentation\n",
    "\n",
    "# setup input filename\n",
    "InputNDVIStats = results + AOI + \"_\" + year + \".tif\"\n",
    "KEAFile = results + AOI + '_' + year + '.kea'\n",
    "SegmentedKEAFile = results + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "SegmentedTiffFile = results + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "SegmentedPolygons = results + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "print(\"calculating imageSegmentation\")\n",
    "imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, epsg='28355')\n",
    "\n",
    "# ### Zonal Statistics & filtering\n",
    "\n",
    "gdf = gpd.read_file(results + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "#calculate zonal mean of NDVI\n",
    "print(\"Calculating zonal stats over the polygons\")\n",
    "gdf['mean'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=InputNDVIStats, stats='mean'))['mean']\n",
    "#calculate area of polygons\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "#filter by area and mean NDVI\n",
    "highNDVI = gdf['mean'] >= threshold\n",
    "smallArea = gdf['area'] <= 5500000\n",
    "gdf = gdf[highNDVI & smallArea]\n",
    "#export shapefile\n",
    "gdf.to_file(results + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=28355)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = NDVI_max.shape\n",
    "# rasterize vector\n",
    "gdf_raster = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                           height, width, transform, projection, raster_path=None)\n",
    "# Mask the xarray\n",
    "NDVI_max_Irrigated = NDVI_max.where(gdf_raster)\n",
    "\n",
    "#remove areas below our threshold that are at the edges of the rasterized polygons\n",
    "NDVI_max_Irrigated = NDVI_max_Irrigated.where(NDVI_max_Irrigated >= threshold)\n",
    "\n",
    "#What is the area of irrigation?\n",
    "ones = np.count_nonzero(~np.isnan(NDVI_max_Irrigated.values))\n",
    "area = (ones*(25*25)) / 1000000\n",
    "print(\"Around \" + AOI + \" during \" + str(year) + \", \" + str(area) + \" km2 was under irrigated cultivation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xr.open_rasterio(InputNDVIStats)\n",
    "a.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_collectionupgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = xr.open_rasterio(KEAFile)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract collection upgrade \n",
    "collection_diffs = landsat_currentcollection - landsat_collectionupgrade\n",
    "\n",
    "# Plot differences for each band. Blue = current collection was higher than collection upgrade\n",
    "collection_diffs.to_array().plot(col='variable', row='time', size=4, vmin=-200, vmax=200, cmap='RdBu', aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year='Summer2017'\n",
    "\"NDVIArgMaxMin_\" + year[6:10] + \"1101_mosaic.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the OEH workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import dask\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "import rasterio.features\n",
    "from osgeo import gdal, ogr\n",
    "import os\n",
    "from rasterstats import zonal_stats\n",
    "from matplotlib import pyplot as plt\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from imageSeg import imageSeg\n",
    "from query_from_shp import query_from_shp\n",
    "\n",
    "from rsgislib.rastergis import ratutils\n",
    "from rsgislib.segmentation import segutils\n",
    "from rsgislib import imageutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "starting processing of ndvi_max_20151101.tif\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Stretch Input Image\n",
      "Add 1 to stretched file to ensure there are no all zeros (i.e., no data) regions created.\n",
      "Create Input Image Mask.\n",
      "Mask stretched Image.\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_stchdonly.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_stchdonlyOff.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_stchdmaskonly.kea\n",
      "Performing KMeans.\n",
      "Apply KMeans to image.\n",
      "Eliminate Single Pixels.\n",
      "Perform clump.\n",
      "Eliminate small pixels.\n",
      "Relabel clumps.\n",
      "Calculate image statistics and build pyramids.\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_kmeansclusters.gmtxt\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_kmeans.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_kmeans.kea.aux.xml\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_kmeans_nosgl.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_kmeans_nosglTMP.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_clumps.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_clumps_elim.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2015_16_stchd.kea\n",
      "exporting the multithreshold as Gtiff\n",
      "converting multithreshold tiff to polygons...\n",
      "exporting the timeofmaxmin Gtiffs\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "starting processing of ndvi_max_20171101.tif\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Stretch Input Image\n",
      "Add 1 to stretched file to ensure there are no all zeros (i.e., no data) regions created.\n",
      "Create Input Image Mask.\n",
      "Mask stretched Image.\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_stchdonly.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_stchdonlyOff.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_stchdmaskonly.kea\n",
      "Performing KMeans.\n",
      "Apply KMeans to image.\n",
      "Eliminate Single Pixels.\n",
      "Perform clump.\n",
      "Eliminate small pixels.\n",
      "Relabel clumps.\n",
      "Calculate image statistics and build pyramids.\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_kmeansclusters.gmtxt\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_kmeans.kea.aux.xml\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_kmeans.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_kmeans_nosgl.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_kmeans_nosglTMP.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_clumps.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_clumps_elim.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2017_18_stchd.kea\n",
      "exporting the multithreshold as Gtiff\n",
      "converting multithreshold tiff to polygons...\n",
      "exporting the timeofmaxmin Gtiffs\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "starting processing of ndvi_max_20161101.tif\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Stretch Input Image\n",
      "Add 1 to stretched file to ensure there are no all zeros (i.e., no data) regions created.\n",
      "Create Input Image Mask.\n",
      "Mask stretched Image.\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_stchdonly.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_stchdonlyOff.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_stchdmaskonly.kea\n",
      "Performing KMeans.\n",
      "Apply KMeans to image.\n",
      "Eliminate Single Pixels.\n",
      "Perform clump.\n",
      "Eliminate small pixels.\n",
      "Relabel clumps.\n",
      "Calculate image statistics and build pyramids.\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_kmeansclusters.gmtxt\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_kmeans.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_kmeans.kea.aux.xml\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_kmeans_nosgl.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_kmeans_nosglTMP.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_clumps.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_clumps_elim.kea\n",
      "Deleting file: ./rsgislibe_test_Summer2016_17_stchd.kea\n",
      "exporting the multithreshold as Gtiff\n",
      "converting multithreshold tiff to polygons...\n",
      "exporting the timeofmaxmin Gtiffs\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "#User Inputs\n",
    "############\n",
    "\n",
    "# where are the dcStats MaxNDVI tifs?\n",
    "MaxNDVItiffs = \"/g/data/r78/cb3058/dea-notebooks/dcStats/results/mdb_NSW/New folder/maxndvi/\"\n",
    "\n",
    "# where are the dcStats NDVIArgMaxMin tifs?\n",
    "NDVIArgMaxMintiffs = \"/g/data/r78/cb3058/dea-notebooks/dcStats/results/mdb_NSW/New folder/argmaxndvi/\"\n",
    "\n",
    "# where should I put the results?\n",
    "results = '/g/data/r78/cb3058/dea-notebooks/dcStats/results/mdb_NSW/summer/previous_run/testing_mosaics/results/'\n",
    "\n",
    "#what season are we processing?\n",
    "season = 'Summer'\n",
    "\n",
    "#Input your area of interest's name\n",
    "AOI = 'rsgislibe_test'\n",
    "\n",
    "#What thresholds should I use for NDVI?\n",
    "threshold = 0.8\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "#script proper------------------------------\n",
    "\n",
    "#loop through raster files and do the analysis\n",
    "maxNDVItiffFiles = os.listdir(MaxNDVItiffs)\n",
    "# NDVIArgMaxMintiffFiles = os.listdir(NDVIArgMaxMintiffs)\n",
    "\n",
    "for tif in maxNDVItiffFiles:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"starting processing of \" + tif)\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    results_ = results\n",
    "    if season == 'Summer':\n",
    "        year = tif[9:13]\n",
    "        nextyear = str(int(year) + 1)[2:] \n",
    "        year = year + \"_\" + nextyear\n",
    "        year = season + year\n",
    "        argmaxminyear = \"NDVIArgMaxMin_\" + year[6:10] + \"1101.tif\" \n",
    "    if season == 'Winter':\n",
    "        year = tif[7:11]\n",
    "        year = season + year\n",
    "        argmaxminyear = \"NDVIArgMaxMin_\" + year[6:10] + \"0501.tif\" \n",
    "\n",
    "    #Creating a folder to keep things neat\n",
    "    directory = results_ + AOI + \"_\" + year\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    results_ = results_ + AOI + \"_\" + year + \"/\"\n",
    "    \n",
    "    #inputs to GDAL and RSGISlib\n",
    "    InputNDVIStats = MaxNDVItiffs + tif\n",
    "    KEAFile = results_ + AOI + '_' + year + '.kea'\n",
    "    SegmentedKEAFile = results_ + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "    meanImage = results_ + AOI + '_' + year + \"_ClumpMean.kea\"\n",
    "     \n",
    "    # Change the tiff to a kea file\n",
    "    gdal.Translate(KEAFile, InputNDVIStats, format='KEA', outputSRS='EPSG:3577')\n",
    "    \n",
    "    # Run segmentation, with creation of clump means\n",
    "    segutils.runShepherdSegmentation(KEAFile, SegmentedKEAFile,\n",
    "                        meanImage, numClusters=20, minPxls=100)\n",
    "\n",
    "    segment_means= xr.open_rasterio(meanImage).squeeze()\n",
    "    \n",
    "    #reclassify and threshold by different values\n",
    "    a = np.where(segment_means.values>=0.8, 80, segment_means)\n",
    "    b = np.where((a>=0.75) & (a<0.8), 75, a)\n",
    "    c = np.where((b>=0.70) & (b<0.75), 70, b)\n",
    "    d = np.where(c>=70, c, np.nan)\n",
    "    #rebuild xarray\n",
    "    d_xr = xr.DataArray(d, coords = [segment_means.y, segment_means.x], dims = ['y', 'x'], attrs = segment_means.attrs)\n",
    "    \n",
    "    print('exporting the multithreshold as Gtiff')\n",
    "    transform, projection = transform_tuple(segment_means, (segment_means.x, segment_means.y), epsg=3577)\n",
    "    #find the width and height of the xarray dataset we want to mask\n",
    "    width,height = segment_means.shape\n",
    "    \n",
    "    SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_multithreshold.tif\",\n",
    "                  d, geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=np.nan)\n",
    "    \n",
    "    #converting irrigated areas results to polygons\n",
    "    print('converting multithreshold tiff to polygons...')\n",
    "    multithresholdTIFF = results_ + AOI + \"_\" + year + \"_multithreshold.tif\"\n",
    "    multithresholdPolygons = results_ + AOI + '_' + year + '_multithreshold.shp'\n",
    "    \n",
    "    os.system('gdal_polygonize.py ' + multithresholdTIFF + ' -f' + ' ' + '\"ESRI Shapefile\"' + ' ' + multithresholdPolygons)\n",
    "    \n",
    "    #filter by the area of the polygons to get rid of any forests etc\n",
    "    print('filtering polygons by size, exporting, then rasterizing')\n",
    "    gdf = gpd.read_file(multithresholdPolygons)\n",
    "    gdf['area'] = gdf['geometry'].area\n",
    "    smallArea = gdf['area'] <= 10000000\n",
    "    gdf = gdf[smallArea]\n",
    "    #export shapefile\n",
    "    gdf.to_file(results_ + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "    \n",
    "    gdf_raster = SpatialTools.rasterize_vector(results_ + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                               height, width, transform, projection, raster_path=None)\n",
    "    \n",
    "    print('loading, then masking timeof rasters')\n",
    "    argmaxmin = xr.open_rasterio(NDVIArgMaxMintiffs+argmaxminyear)\n",
    "    timeofmax = argmaxmin[0] \n",
    "    timeofmin = argmaxmin[1]\n",
    "\n",
    "    # mask timeof layers by irrigated extent\n",
    "    timeofmax = timeofmax.where(gdf_raster)\n",
    "    timeofmin = timeofmin.where(gdf_raster)\n",
    "\n",
    "    # export masked timeof layers.\n",
    "    print('exporting the timeofmaxmin Gtiffs')\n",
    "    SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_timeofmaxNDVI.tif\",\n",
    "                  timeofmax.values,\n",
    "                  geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=-9999)\n",
    "\n",
    "    SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_timeofminNDVI.tif\",\n",
    "                  timeofmin.values,\n",
    "                  geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=-9999)\n",
    "\n",
    "print('Success!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "#User Inputs\n",
    "############\n",
    "\n",
    "# where are the dcStats MaxNDVI tifs?\n",
    "MaxNDVItiffs = \"/g/data/r78/cb3058/dea-notebooks/dcStats/results/mdb_NSW/summer/previous_run/testing_mosaics/ndvi_max/\"\n",
    "\n",
    "# where are the dcStats NDVIArgMaxMin tifs?\n",
    "NDVIArgMaxMintiffs = \"/g/data/r78/cb3058/dea-notebooks/dcStats/results/mdb_NSW/New folder/argmaxndvi/\"\n",
    "\n",
    "#Is there an irrigatable area shapefile we're using for masking?\n",
    "irrigatable_area = False\n",
    "irrigatable_area_shp_fpath = \"/g/data/r78/cb3058/dea-notebooks/ICE_project/data/spatial/NSW_OEH_irrigated_2013.shp\"\n",
    "\n",
    "#is there a shapefile we're using for clipping the extent? e.g. just the northern basins\n",
    "clip_extent = True\n",
    "northernBasins_shp = \"/g/data/r78/cb3058/dea-notebooks/ICE_project/data/spatial/northern_basins.shp\"\n",
    "\n",
    "# where should I put the results?\n",
    "results = '/g/data/r78/cb3058/dea-notebooks/dcStats/results/mdb_NSW/summer/previous_run/testing_mosaics/results/'\n",
    "\n",
    "#what season are we processing?\n",
    "season = 'Summer'\n",
    "\n",
    "#Input your area of interest's name\n",
    "AOI = 'smalltest'\n",
    "\n",
    "#What thresholds should I use for NDVI?\n",
    "threshold = 0.8\n",
    "\n",
    "#-----------------------------------------\n",
    "\n",
    "#script proper------------------------------\n",
    "\n",
    "#loop through raster files and do the analysis\n",
    "maxNDVItiffFiles = os.listdir(MaxNDVItiffs)\n",
    "# NDVIArgMaxMintiffFiles = os.listdir(NDVIArgMaxMintiffs)\n",
    "\n",
    "for tif in maxNDVItiffFiles:\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"starting processing of \" + tif)\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    results_ = results\n",
    "    if season == 'Summer':\n",
    "        year = tif[9:13]\n",
    "        nextyear = str(int(year) + 1)[2:] \n",
    "        year = year + \"_\" + nextyear\n",
    "        year = season + year\n",
    "        argmaxminyear = \"NDVIArgMaxMin_\" + year[6:10] + \"1101.tif\" \n",
    "    if season == 'Winter':\n",
    "        year = tif[7:11]\n",
    "        year = season + year\n",
    "        argmaxminyear = \"NDVIArgMaxMin_\" + year[6:10] + \"0501.tif\" \n",
    "\n",
    "    #Creating a folder to keep things neat\n",
    "    directory = results_ + AOI + \"_\" + year\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "    results_ = results_ + AOI + \"_\" + year + \"/\"\n",
    "\n",
    "    # set up input filename\n",
    "    InputNDVIStats = MaxNDVItiffs + tif\n",
    "    KEAFile = results_ + AOI + '_' + year + '.kea'\n",
    "    SegmentedKEAFile = results_ + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "    SegmentedTiffFile = results_ + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "    SegmentedPolygons = results_ + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "    \n",
    "    print(\"calculating imageSegmentation\")\n",
    "    imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, minPxls=100)\n",
    "\n",
    "    gdf = gpd.read_file(results_ + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "    \n",
    "    #calculate zonal mean of NDVI\n",
    "    print(\"calculating zonal stats\")\n",
    "    gdf['mean'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=InputNDVIStats, stats='mean'))['mean']\n",
    "    #calculate area of polygons\n",
    "    gdf['area'] = gdf['geometry'].area\n",
    "    #filter by area and mean NDVI\n",
    "    highNDVI = gdf['mean'] >= threshold\n",
    "    smallArea = gdf['area'] <= 5500000\n",
    "    gdf = gdf[highNDVI & smallArea]\n",
    "    #export shapefile\n",
    "    gdf.to_file(results_ + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "    \n",
    "    print('performing masking and raster math')\n",
    "    NDVI_max = xr.open_rasterio(InputNDVIStats).squeeze()\n",
    "    #get the transform and projection of our gtiff\n",
    "    transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "    #find the width and height of the xarray dataset we want to mask\n",
    "    width,height = NDVI_max.shape\n",
    "    # rasterize vector\n",
    "    gdf_raster = SpatialTools.rasterize_vector(results_ + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                               height, width, transform, projection, raster_path=None)\n",
    "    # Mask the xarray\n",
    "    NDVI_max_Irrigated = NDVI_max.where(gdf_raster)\n",
    "\n",
    "    #remove areas below our threshold that are at the edges of the rasterized polygons\n",
    "    NDVI_max_Irrigated = NDVI_max_Irrigated.where(NDVI_max_Irrigated >= threshold)\n",
    "    \n",
    "    print('exporting the irrigatation Gtiff')\n",
    "    SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_Irrigated.tif\",\n",
    "                  NDVI_max_Irrigated.values,\n",
    "                  geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=-9999)\n",
    "    \n",
    "    # import timeofmax and timeofmin rasters\n",
    "    argmaxmin = xr.open_rasterio(NDVIArgMaxMintiffs+argmaxminyear)\n",
    "    timeofmax = argmaxmin[0] \n",
    "    timeofmin = argmaxmin[1]\n",
    "\n",
    "    # mask timeof layers by irrigated extent\n",
    "    timeofmax = timeofmax.where(~np.isnan(NDVI_max_Irrigated))\n",
    "    timeofmin = timeofmin.where(~np.isnan(NDVI_max_Irrigated))\n",
    "\n",
    "    # export masked timeof layers.\n",
    "    print('exporting the timeofmaxmin Gtiffs')\n",
    "    SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_timeofmaxNDVI.tif\",\n",
    "                  timeofmax.values,\n",
    "                  geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=-9999)\n",
    "\n",
    "    SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_timeofminNDVI.tif\",\n",
    "                  timeofmin.values,\n",
    "                  geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=-9999)\n",
    "    \n",
    "    \n",
    "    if irrigatable_area == True:\n",
    "        print('limiting analysis to the irrigatable area polygon')\n",
    "        # rasterize Irrigatable vector file\n",
    "        oeh_raster = SpatialTools.rasterize_vector(irrigatable_area_shp_fpath,height, width, \n",
    "                                                   transform, projection, raster_path=None)\n",
    "        #mask\n",
    "        NDVI_max_Irrigated_oeh = NDVI_max_Irrigated.where(oeh_raster)\n",
    "\n",
    "        #export as GTiff\n",
    "        SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_OEHMasked_Irrigated.tif\",\n",
    "                  NDVI_max_Irrigated_oeh.values,\n",
    "                  geo_transform = transform, \n",
    "                  projection = projection, \n",
    "                  nodata_val=-9999)\n",
    "    \n",
    "        # mask timeof layers by irrigated extent\n",
    "        timeofmax_oeh = timeofmax.where(~np.isnan(NDVI_max_Irrigated_oeh))\n",
    "        timeofmin_oeh = timeofmin.where(~np.isnan(NDVI_max_Irrigated_oeh))\n",
    "\n",
    "        # export masked timeof layers.\n",
    "        print('exporting the timeofmaxmin Gtiff')\n",
    "        SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_OEHMasked_timeofmaxNDVI.tif\",\n",
    "                      timeofmax_oeh.values,\n",
    "                      geo_transform = transform, \n",
    "                      projection = projection, \n",
    "                      nodata_val=-9999)\n",
    "\n",
    "        SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_OEHMasked_timeofminNDVI.tif\",\n",
    "                      timeofmin_oeh.values,\n",
    "                      geo_transform = transform, \n",
    "                      projection = projection, \n",
    "                      nodata_val=-9999)\n",
    "        \n",
    "    if clip_extent == True:\n",
    "        print('clipping extent to provided polygon')\n",
    "        clip_raster = SpatialTools.rasterize_vector(northernBasins_shp,\n",
    "                                               height, width, transform, projection, raster_path=None)\n",
    "        #mask all outputs to the clip extent\n",
    "        NDVI_max_Irrigated_clipped  = NDVI_max_Irrigated.where(clip_raster)\n",
    "        timeofmax_clipped = timeofmax.where(~np.isnan(NDVI_max_Irrigated_clipped))\n",
    "        timeofmin_clipped = timeofmin.where(~np.isnan(NDVI_max_Irrigated_clipped))\n",
    "        \n",
    "        SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_Irrigated_clipped.tif\",\n",
    "              NDVI_max_Irrigated_clipped.values,\n",
    "              geo_transform = transform, \n",
    "              projection = projection, \n",
    "              nodata_val=-9999)\n",
    "        \n",
    "        SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_timeofmaxNDVI_clipped.tif\",\n",
    "                      timeofmax_clipped.values,\n",
    "                      geo_transform = transform, \n",
    "                      projection = projection, \n",
    "                      nodata_val=-9999)\n",
    "\n",
    "        SpatialTools.array_to_geotiff(results_ + AOI + \"_\" + year + \"_timeofminNDVI_clipped.tif\",\n",
    "                      timeofmin_clipped.values,\n",
    "                      geo_transform = transform, \n",
    "                      projection = projection, \n",
    "                      nodata_val=-9999)\n",
    "    \n",
    "    print(\"Finished processing of \" + tif)\n",
    "    \n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_rasterio('path/to/data').squeeze()\n",
    "area = np.count_nonzero(~np.isnan(data.values))*(25*25) / 10000\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
