{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "\n",
    "import fiona\n",
    "import rasterio.features\n",
    "from osgeo import gdal, ogr\n",
    "import os\n",
    "from rsgislib.segmentation import segutils\n",
    "from rasterstats import zonal_stats\n",
    "\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from imageSeg import imageSeg\n",
    "from query_from_shp import query_from_shp\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up query and analysis parameters\n",
    "Here we set a centroid for the area we want to compare, and set up CRS, resolution and resampling that will be applied to both collectiondatasets. The values below extract both collections to match the collection upgrade CRS and resolution (UTM zone 56 S and 30m pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is your data and results folder?\n",
    "data = 'data/'\n",
    "results = 'results/'\n",
    "\n",
    "#Input your area of interest's name, coords, and \n",
    "#the year you're interested in?\n",
    "AOI = 'collectionTest'\n",
    "year = 'Summer2017-18'\n",
    "# Centre point of spatial query\n",
    "lat, lon = -35.969, 145.394\n",
    "time_period = ('2017-11-01', '2018-03-31')\n",
    "\n",
    "# Desired output resolution and projection for both datasets\n",
    "output_crs = 'EPSG:28355'\n",
    "output_resolution = (30, 30)\n",
    "output_resamp_continuous = 'bilinear'\n",
    "output_resamp_categorical = 'nearest'\n",
    "\n",
    "# Bands/measurements to load\n",
    "collectionupgrade_bands = ['nbart_red','nbart_nir']\n",
    "currentcollection_bands = ['red', 'nir']\n",
    "\n",
    "# Set up query\n",
    "query = {'lon': (lon - 0.15, lon + 0.15),\n",
    "         'lat': (lat - 0.15, lat + 0.15),\n",
    "         'time': time_period}\n",
    "\n",
    "# Connect to current collection and collection upgrade databases\n",
    "dc_ard = datacube.Datacube(config='/g/data1a/u46/users/cb3058/datacube.conf', env='datacube_upgrade')\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "# Preview spatial query\n",
    "# DEAPlotting.display_map(x=query['lon'], y=query['lat'], crs='EPSG:4326')\n",
    "\n",
    "#What thresholds should I use?\n",
    "threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder to keep things neat\n",
    "directory = results + AOI + \"_\" + year\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "results = results + AOI + \"_\" + year + \"/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in collection upgrade data (DEA Landsat Collection 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray_dict = {}\n",
    "\n",
    "# Iterate through each sensor\n",
    "for sensor in ['ls8']:\n",
    "\n",
    "    # Load data \n",
    "    landsat_ds = dc_ard.load(product = f'{sensor}_ard', \n",
    "                             measurements = collectionupgrade_bands,\n",
    "                             output_crs = output_crs,\n",
    "                             resolution = output_resolution,\n",
    "                             resampling = output_resamp_continuous,\n",
    "                             align=(output_resolution[0]/2.0, output_resolution[1]/2.0),\n",
    "                             group_by = 'solar_day', \n",
    "                             **query)\n",
    "    \n",
    "    # Load PQ data seperately (this enables using a different resampling method on \n",
    "    # continuous surface reflectance values vs categorical fmask/PQ values)\n",
    "    landsat_pq = dc_ard.load(product = f'{sensor}_ard', \n",
    "                             measurements = ['fmask'],\n",
    "                             output_crs = output_crs,\n",
    "                             resolution = output_resolution,\n",
    "                             resampling = output_resamp_categorical,\n",
    "                             align=(output_resolution[0]/2.0, output_resolution[1]/2.0),\n",
    "                             group_by = 'solar_day', \n",
    "                             **query)\n",
    "\n",
    "    # Identify pixels with valid data: no nodata AND no cloud AND no cloud shadow\n",
    "    good_quality = ((landsat_pq.fmask != 0) & \n",
    "                    (landsat_pq.fmask != 2) & \n",
    "                    (landsat_pq.fmask != 3))\n",
    "\n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    landsat_ds = landsat_ds.where(good_quality)\n",
    "\n",
    "    # Add to list\n",
    "    xarray_dict[sensor] = landsat_ds\n",
    "\n",
    "# Concatenate multiple sensors into one dataset\n",
    "landsat_collectionupgrade = xr.concat(xarray_dict.values(), dim='time')\n",
    "landsat_collectionupgrade = landsat_collectionupgrade.sortby('time')\n",
    "landsat_collectionupgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in current collection data (DEA Landsat Collection 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xarray_dict = {}\n",
    "\n",
    "for sensor in ['ls8']:\n",
    "\n",
    "    # Load data \n",
    "    landsat_ds = dc.load(product = f'{sensor}_nbart_albers', \n",
    "                         measurements = currentcollection_bands,\n",
    "                         output_crs = output_crs,\n",
    "                         resolution = output_resolution,\n",
    "                         resampling = output_resamp_continuous,\n",
    "                         group_by = 'solar_day', \n",
    "                         **query)\n",
    "\n",
    "    # Load PQ data \n",
    "    landsat_pq = dc.load(product = f'{sensor}_pq_albers', \n",
    "                         measurements = ['pixelquality'],\n",
    "                         output_crs = output_crs,\n",
    "                         resolution = output_resolution,\n",
    "                         resampling = output_resamp_categorical,\n",
    "                         group_by = 'solar_day', \n",
    "                         **query)                       \n",
    "\n",
    "    # Filter to subset of Landsat observations that have matching PQ data \n",
    "    time = (landsat_ds.time - landsat_pq.time).time\n",
    "    landsat_ds = landsat_ds.sel(time=time)\n",
    "    landsat_pq = landsat_pq.sel(time=time)\n",
    "\n",
    "    # Create PQ mask\n",
    "    good_quality = masking.make_mask(landsat_pq.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True) \n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    landsat_ds = landsat_ds.where(good_quality)\n",
    "\n",
    "    # Add result to dict\n",
    "    xarray_dict[sensor] = landsat_ds\n",
    "\n",
    "# Concatenate multiple sensors into one dataset\n",
    "landsat_currentcollection = xr.concat(xarray_dict.values(), dim='time')\n",
    "landsat_currentcollection = landsat_ds.sortby('time')\n",
    "landsat_currentcollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round both dataset timestamps to daily because both datasets have different timestamps\n",
    "landsat_collectionupgrade.coords['time'] = landsat_collectionupgrade.time.astype('datetime64[D]')\n",
    "landsat_currentcollection.coords['time'] = landsat_currentcollection.time.astype('datetime64[D]')\n",
    "\n",
    "# Rename current collection bands to match collection upgrade\n",
    "band_rename_dict = {'red': 'nbart_red', \n",
    "                    'nir': 'nbart_nir' }\n",
    "landsat_currentcollection.rename(band_rename_dict, inplace=True)\n",
    "\n",
    "# Unfortunately due to the different pixel definitions for the DEA Landsat Collections 2 and 3,\n",
    "# the same spatial query can result in a different number of rows and columns. Because of this,\n",
    "# the only way we can directly compare the two collections is to force them to match by resampling\n",
    "# one to the rows x columns of the other. This will introduce some uncertainty in the comparison.\n",
    "landsat_currentcollection = landsat_currentcollection.interp_like(landsat_collectionupgrade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MaxNDVI irrigated extent algorithm on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#band indices calculation\n",
    "def ndvi_func(nir, red):\n",
    "    return ((nir - red)/(nir + red))\n",
    "\n",
    "def ndvi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        ndvi_func, ds.nbart_nir, ds.nbart_red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "print(\"calculating NDVI\")\n",
    "NDVI_landsat = ndvi_ufunc(landsat_currentcollection)\n",
    "\n",
    "#calculate per pixel summary stats\n",
    "print(\"calculating summary stats\")\n",
    "NDVI_max = NDVI_landsat.groupby('x','y').max('time').rename('NDVI_max')\n",
    "# NDVI_max = NDVI_max.chunk({'x':1000, 'y':1000})\n",
    "\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=28355)\n",
    "print(\"exporting MaxNDVI GTiff\")\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \".tif\",\n",
    "              NDVI_max.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=-9999.)\n",
    "\n",
    "# ## Image Segmentation\n",
    "\n",
    "# setup input filename\n",
    "InputNDVIStats = results + AOI + \"_\" + year + \".tif\"\n",
    "KEAFile = results + AOI + '_' + year + '.kea'\n",
    "SegmentedKEAFile = results + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "SegmentedTiffFile = results + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "SegmentedPolygons = results + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "print(\"calculating imageSegmentation\")\n",
    "imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, epsg='28355')\n",
    "\n",
    "# ### Zonal Statistics & filtering\n",
    "\n",
    "gdf = gpd.read_file(results + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "#calculate zonal mean of NDVI\n",
    "print(\"Calculating zonal stats over the polygons\")\n",
    "gdf['mean'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=InputNDVIStats, stats='mean'))['mean']\n",
    "#calculate area of polygons\n",
    "gdf['area'] = gdf['geometry'].area\n",
    "#filter by area and mean NDVI\n",
    "highNDVI = gdf['mean'] >= threshold\n",
    "smallArea = gdf['area'] <= 5500000\n",
    "gdf = gdf[highNDVI & smallArea]\n",
    "#export shapefile\n",
    "gdf.to_file(results + AOI + \"_\" + year + \"_Irrigated.shp\")\n",
    "\n",
    "#get the transform and projection of our gtiff\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=28355)\n",
    "#find the width and height of the xarray dataset we want to mask\n",
    "width,height = NDVI_max.shape\n",
    "# rasterize vector\n",
    "gdf_raster = SpatialTools.rasterize_vector(results + AOI + \"_\" + year + \"_Irrigated.shp\",\n",
    "                                           height, width, transform, projection, raster_path=None)\n",
    "# Mask the xarray\n",
    "NDVI_max_Irrigated = NDVI_max.where(gdf_raster)\n",
    "\n",
    "#remove areas below our threshold that are at the edges of the rasterized polygons\n",
    "NDVI_max_Irrigated = NDVI_max_Irrigated.where(NDVI_max_Irrigated >= threshold)\n",
    "\n",
    "#What is the area of irrigation?\n",
    "ones = np.count_nonzero(~np.isnan(NDVI_max_Irrigated.values))\n",
    "area = (ones*(25*25)) / 1000000\n",
    "print(\"Around \" + AOI + \" during \" + str(year) + \", \" + str(area) + \" km2 was under irrigated cultivation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xr.open_rasterio(InputNDVIStats)\n",
    "a.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_collectionupgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = xr.open_rasterio(KEAFile)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract collection upgrade \n",
    "collection_diffs = landsat_currentcollection - landsat_collectionupgrade\n",
    "\n",
    "# Plot differences for each band. Blue = current collection was higher than collection upgrade\n",
    "collection_diffs.to_array().plot(col='variable', row='time', size=4, vmin=-200, vmax=200, cmap='RdBu', aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
