{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vegetation Change Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notebook currently compatible with the `DE Africa` Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "Detecting regions of deforestation and afforestation in satellite imagery is complicated by the need to distinguish real land use changes from the natural variability introduced by climate variability.  To determine regions that have undergone significant change, we can apply hypotheisis testing techniques to stacks of remotely sensed satellite images to determine where change is statistically significant relative to the natural, background variability.\n",
    "\n",
    "\n",
    "### Description\n",
    "In this example, we measure the presence of vegetation from USGS Landsat Collection 2 imagery and apply a hypothesis test to identify areas of significant change (along with the direction of the change).  Here we use the [Fractional Cover (FC)](https://d28rz98at9flks.cloudfront.net/79676/Fractional_Cover_FC25_v1_5.PDF) dataset. FC represents the proportion of the land surface that is bare (BS), covered by photosynthetic vegetation (PV), or non-photosynthetic vegetation (NPV).  The Fractional Cover product was generated using the spectral unmixing algorithm developed by the Joint Remote Sensing Research Program [(JRSRP)](http://data.auscover.org.au/xwiki/bin/view/Product+pages/Landsat+Fractional+Cover), which used the spectral signature for each pixel to break it up into three fractions, based on field work that determined the spectral characteristics of these fractions.\n",
    "\n",
    "The green (PV) fraction includes leaves and grass, the non-photosynthetic fraction (NPV) includes branches, dry grass and dead leaf litter, and the bare soil (BS) fraction includes bare soil or rock.\n",
    "\n",
    "The worked example takes users through the code required to do the following:\n",
    "1. Load cloud-free Fractional Cover images (derived from Landsat 8) for an area of interest (AOI).\n",
    "2. Apply a statistical hypothesis test to find areas of significant change.\n",
    "3. Visualise the statistically significant areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical details\n",
    "* **Products used:** Landsat Collection 2, Fractional Cover\n",
    "* **Analyses used:** image differencing, hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "**To run this analysis**, run all the cells in the notebook, starting with the \"Load packages\" cell.\n",
    "\n",
    "**After finishing the analysis**, return to the \"Analysis parameters\" cell, modify some values (e.g. choose a different location or time period to analyse) and re-run the analysis.\n",
    "There are additional instructions on modifying the notebook at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Load key Python packages and any supporting functions for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "import datacube.utils.rio\n",
    "from datacube.utils.dask import start_local_dask\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.storage.masking import make_mask\n",
    "from datacube.utils.geometry import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sys\n",
    "import xarray as xr\n",
    "import dask\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../../examples/Scripts\")\n",
    "from deafrica_datahandling import mostcommon_crs\n",
    "from deafrica_plotting import display_map\n",
    "\n",
    "#This will speed up loading data\n",
    "datacube.utils.rio.set_default_rio_config(aws='auto', cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster\n",
    "\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"distributed.dashboard.link\": \n",
    "        os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"})\n",
    "\n",
    "client = start_local_dask(n_workers=2, threads_per_worker=2, memory_limit='7G')\n",
    "display(client)\n",
    "\n",
    "creds = configure_s3_access(client=client, \n",
    "                            region_name='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "Activate the datacube database, which provides functionality for loading and displaying stored Earth observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Change_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "\n",
    "The following cell sets the parameters, which define the area of interest and the length of time to conduct the analysis over.\n",
    "There is also a parameter to define how the data is split in time; the split yields two non-overlapping samples, which is a requirement of the hypothesis test we want to run (more detail below).\n",
    "The parameters are:\n",
    "\n",
    "* `latitude`: The latitude at the centre of your AOI (e.g. `-35.271`).\n",
    "* `longitude`: The longitude at the centre of your AOI (e.g. `19.620`).\n",
    "* `buffer`: The size, in decimal degrees, of the box to draw around your lat, lon point, this will determine the size of your AOI. \n",
    "For reasonable loading times, make sure the range spans less than ~0.1 degrees.\n",
    "* `time`: The date range to analyse (e.g. `('2015-01-01', '2019-09-01')`).\n",
    "For reasonable results, the range should span at least two years to prevent detecting seasonal changes.\n",
    "* `time_baseline`: The date at which to split the total sample into two non-overlapping samples (e.g. `'2017-12-01'`).\n",
    "For reasonable results, pick a date that is about halfway between the start and end dates specified in `time`.\n",
    "\n",
    "**If running the notebook for the first time**, keep the default settings below.\n",
    "This will demonstrate how the analysis works and provide meaningful results.\n",
    "The example covers a location in the Western Cape, South Africa, where a fire removed some forest cover. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the area of interest\n",
    "latitude = -34.643\n",
    "longitude = 19.620\n",
    "buffer = 0.045\n",
    "\n",
    "# Set the range of dates for the complete sample\n",
    "time = ('2013-01-01', '2017-12-01')\n",
    "\n",
    "# Set the date to separate the data into two samples for comparison\n",
    "time_baseline = '2015-12'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the selected location\n",
    "The next cell will display the selected area on an interactive map.\n",
    "The red border represents the area of interest of the study.\n",
    "Zoom in and out to get a better understanding of the area of interest.\n",
    "Clicking anywhere on the map will reveal the latitude and longitude coordinates of the clicked point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_map(x=(longitude-buffer, longitude+buffer), y=(latitude+buffer, latitude-buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and view Fractional Cover (FC) data\n",
    "\n",
    "The first step in the analysis is to load fractional cover data for the specified area of interest and time range. Fractional cover data includes three bands, the `Photosynthetic Vegetation (PV)` fraction, the `Non-Photosynthtic Vegetation (NPV)` fraction, and the `Bare Soil (BS)` fraction.  For this analysis, we are only intersted in the PV fraction, as this is the band we'll use to determine changes in forest cover.\n",
    "\n",
    "The code below will first create a query dictionary for our region of interest, find the correct `crs` object for the AOI, load the FC data, and then load pixel quality data to determine pixels affected by cloud which we will use to mask the FC data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the right crs for the location\n",
    "crs = mostcommon_crs(dc=dc,\n",
    "                     product='usgs_ls8c_level2_2',\n",
    "                     query=query)\n",
    "\n",
    "# Create a query object\n",
    "query = {\n",
    "    'x': (longitude-buffer, longitude+buffer),\n",
    "    'y': (latitude+buffer, latitude-buffer),\n",
    "    'time': time,\n",
    "    'output_crs': crs,\n",
    "    'resolution': (-30, 30),\n",
    "    'group_by': 'solar_day'\n",
    "}\n",
    "\n",
    "\n",
    "# set the size of the dask chunks, this will determine how big each\n",
    "# spatial chunk is that gets sent to each cpu during parallelization. \n",
    "chunks={'x':100, 'y':100}\n",
    "\n",
    "# load fractional cover\n",
    "fc = dc.load(**query,\n",
    "             product=\"ga_ls8c_fractional_cover_2\",\n",
    "             dask_chunks=chunks,\n",
    "             align=(15, 15),\n",
    "             measurements=[\"pv\"],\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loud the clouds dataset\n",
    "clouds = dc.load(**query,\n",
    "                 dask_chunks=chunks,\n",
    "                 align=(15, 15),\n",
    "                 output_crs=crs,\n",
    "                 product='usgs_ls8c_level2_2',\n",
    "                 measurements=['quality_l2_aerosol'])\n",
    "\n",
    "# create a cloud mask\n",
    "valid_data = make_mask(clouds[\"quality_l2_aerosol\"],\n",
    "                       cloud_shadow=\"not_cloud_shadow\",\n",
    "                       cloud_or_cirrus=\"not_cloud_or_cirrus\")\n",
    "\n",
    "# remove cloudy pixels\n",
    "fc = fc.where(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once the load is complete**, examine the data by printing it in the next cell.\n",
    "The `Dimensions` argument revels the number of time steps in the data set, as well as the number of pixels in the x (longitude) and y (latitude) dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot example timesteps of photosynthetic vegetation fraction\n",
    "\n",
    "Feel free to experiment with the values for the `initial_timestep` and `final_timestep` variables; re-run the cell to plot the images for the new timesteps.\n",
    "The values for the timesteps can be `0` to one fewer than the number of time steps loaded in the data set.\n",
    "The number of time steps is the same as the total number of observations listed as the output of the cell used to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the timesteps to visualise\n",
    "initial_timestep = 5\n",
    "final_timestep = 112\n",
    "\n",
    "fc.pv.isel(time=[initial_timestep, final_timestep]).plot.imshow(\n",
    "    'x', 'y', col='time', cmap='RdYlGn',  vmin=0, vmax=70, figsize=(12, 6))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform hypothesis test\n",
    "While it is possible to visually detect change between the two timesteps, it is important to consider how to rigorously check for both positive change in the PV (afforestation) and negative change in the PV (deforestation).\n",
    "\n",
    "This can be done through hypothesis testing.\n",
    "In this case, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{null hypothesis } (H_0) &: \\text{no change occurred,} \\\\\n",
    "\\text{alternative hypothesis } (H_1) &: \\text{some change occurred.}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The hypothesis test will indicate where there is evidence for rejecting the null hypothesis.\n",
    "From this, we may identify areas of signficant change, according to a given significance level (covered in more detail below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make samples\n",
    "\n",
    "To perform the test, the total sample will be split in two: a `baseline` sample and a `postbaseline` sample, which respectively contain the data before and after the `time_baseline` date.\n",
    "Then, we can test for a difference in the average PV % between the samples for each pixel in the sample.\n",
    "\n",
    "The samples are made by selecting the `NDVI` band from the dataset and filtering it based on whether it was observed before or after the `time_baseline` date.\n",
    "The number of observations in each sample will be printed.\n",
    "If one sample is much larger than the other, consider changing the `time_baseline` parameter in the \"Analysis parameters\" cell, and then re-run this cell.\n",
    "Coordinates are recorded for later use.\n",
    "\n",
    "**Note**: This may take a couple of minutes to run because at this stage we fully compute the arrays we _lazily_ loaded using dask earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make samples\n",
    "baseline_sample = fc.pv.sel(time=fc['time'] <= np.datetime64(time_baseline))\n",
    "baseline_sample = baseline_sample.compute()\n",
    "print(f\"Number of observations in baseline sample: {len(baseline_sample.time)}\")\n",
    "\n",
    "postbaseline_sample = fc.pv.sel(time=fc['time'] > np.datetime64(time_baseline))\n",
    "postbaseline_sample = postbaseline_sample.compute()\n",
    "print(f\"Number of observations in postbaseline sample: {len(postbaseline_sample.time)}\")\n",
    "\n",
    "# Record coodrinates for reconstructing xarray objects\n",
    "sample_lat_coords = fc.coords['y']\n",
    "sample_lon_coords = fc.coords['x']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for change\n",
    "To look for evidence that the average PV % has changed between the two samples (either positively or negatively), we use Welch's t-test.\n",
    "This is used to test the hypothesis that two populations have equal averages.\n",
    "In this case, the populations are the area of interest before and after the `time_baseline` date, and the average being tested is the average PV %.\n",
    "Welch's *t*-test is used (as opposed to Student's *t*-test) because the two samples in the study may not necessarily have equal variances. \n",
    "\n",
    "The test is run using the Scipy package's statistcs library, which provides the `ttest_ind` function for running *t*-tests. \n",
    "Setting `equal_var=False` means that the function will run Welch's *t*-test.\n",
    "The function returns the *t*-statistic and *p*-value for each pixel after testing the difference in the average NDVI.\n",
    "These are stored as `t_stat` and `p_val` inside the `t_test` dataset for use in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the t-test on the postbaseline and baseline samples\n",
    "tstat, p_tstat = stats.ttest_ind(\n",
    "    postbaseline_sample.values,\n",
    "    baseline_sample.values,\n",
    "    equal_var=False,\n",
    "    nan_policy='omit',\n",
    ")\n",
    "\n",
    "# Convert results to an xarray for further analysis\n",
    "t_test = xr.Dataset(\n",
    "    {'t_stat': (['y', 'x'], tstat),\n",
    "     'p_val': (['y', 'x'], p_tstat)},\n",
    "    coords={\n",
    "     'x': (['x'], sample_lon_coords.values),\n",
    "     'y': (['y'], sample_lat_coords.values)\n",
    "    }, attrs={'crs': 'EPSG:3577'})\n",
    "\n",
    "print(t_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise change\n",
    "From the test, we're interested in two conditions: whether the change is significant (rejection of the null hypothesis) and whether the change was positive (afforestation) or negative (deforestation).\n",
    "\n",
    "The null hypothesis can be rejected if the $p$-value (`p_val`) is less than the chosen significance level, which is set as `sig_level = 0.05` for this analysis.\n",
    "If the null hypothesis is rejected, the pixel will be classified as having undergone significant change.\n",
    "\n",
    "The direction of the change can be inferred from the difference in the average PV % of each sample, which is calculated as $$\\text{diff mean} = \\text{mean(post baseline)} - \\text{mean(baseline)}.$$\n",
    "This means that\n",
    "- if the average PV % for a given pixel is **higher** in the `post baseline` sample compared to the `baseline` sample, then `diff_mean` for that pixel will be **positive**.\n",
    "- if the average PV % for a given pixel is **lower** in the `post baseline` sample compared to the `baseline` sample, then `diff_mean` for that pixel will be **negative**.\n",
    "\n",
    "Run the cell below to first plot the difference in the mean between the two samples, then plot only the differences that were marked as signficant. \n",
    "**Positive change is shown in blue and negative change is shown in red.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the significance level\n",
    "sig_level = 0.05\n",
    "\n",
    "# Plot any difference in the mean\n",
    "diff_mean = postbaseline_sample.mean(\n",
    "    dim=['time']) - baseline_sample.mean(dim=['time'])\n",
    "\n",
    "# Plot any difference in the mean classified as significant\n",
    "sig_diff_mean = postbaseline_sample.mean(dim=['time']).where(\n",
    "    t_test.p_val < sig_level) - baseline_sample.mean(dim=['time']).where(t_test.p_val < sig_level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "diff_mean.plot(cmap='RdBu', ax=ax[0])\n",
    "sig_diff_mean.plot(cmap='RdBu', ax=ax[1])\n",
    "ax[0].set_title('Any difference in the mean')\n",
    "ax[1].set_title('Statistically significant difference in the mean')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing conclusions\n",
    "Here are some questions to think about:\n",
    "- What has happened in the forest over the time covered by the dataset?\n",
    "- Were there any statistically significant changes that the test found that you didn't see in the true-colour images? \n",
    "- What kind of activities/events might explain the significant changes?\n",
    "- What kind of activities/events might explain non-significant changes?\n",
    "- What other information might you need to draw conclusions about the cause of the statistically significant changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data\n",
    "To explore the data further in a desktop GIS program, the data can be output as a GeoTiff.\n",
    "This requires data to be converted to an xarray and tagged with the appropriate coordinate reference system (`crs`). \n",
    "The `diff_mean` product will be saved as \"ttest_diff_mean.tif\", and the `sig_diff_mean` product will be saved as \"ttest_sig_diff_mean.tif\".\n",
    "These files can be downloaded from the file explorer to the left of this window ([see these instructions](https://jupyterlab.readthedocs.io/en/stable/user/files.html#uploading-and-downloading))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make datasets into xarrays for output\n",
    "diff_mean_out = xr.Dataset({'diff_mean': diff_mean}, attrs={\n",
    "                           'crs': CRS('EPSG:3577')})\n",
    "\n",
    "sig_diff_mean_out = xr.Dataset({'sig_diff_mean': sig_diff_mean}, attrs={\n",
    "                               'crs': CRS('EPSG:3577')})\n",
    "\n",
    "# Write output to geotiffs\n",
    "write_geotiff(filename=\"ttest_diff_mean.tif\", dataset=diff_mean_out)\n",
    "write_geotiff(filename=\"ttest_sig_diff_mean.tif\", dataset=sig_diff_mean_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "When you are done, return to the \"Analysis parameters\" section, modify some values (e.g. `latitude`, `longitude`, `time` or `time_baseline`) and re-run the analysis.\n",
    "You can use the interactive map in the \"View the selected location\" section to find new central latitude and longitude values by panning and zooming, and then clicking on the area you wish to extract location values for.\n",
    "You can also use Google maps to search for a location you know, then return the latitude and longitude values by clicking the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** September 2019\n",
    "\n",
    "**Compatible `datacube` version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`Africa Sandbox compatible`, :index:`fractional cover`, :index:`dea_datahandling`, :index:`calculate_indices`, :index:`display_map`, :index:`real world`, :index:`forestry`, :index:`change detection`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
