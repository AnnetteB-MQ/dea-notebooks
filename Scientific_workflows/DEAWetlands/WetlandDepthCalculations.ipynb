{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth analysis <img align=\"right\" src=\"../../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatability:** Notebook currently compatible with the `NCI` and `DEA Sandbox` environment \n",
    "* **Products used:** \n",
    "* **Special requirements:** \n",
    "* **Prerequisites:** This notebook assumes you have previously run the [WetlandWaterDepthAnalyses.ipynb](./WetlandWaterDepthAnalyses.ipynb) notebook and saved out a netCDF file containing all the variables at the end of the notebook. A csv file containing the daily maximum water discharge of a relevant stream gauge is also required. The gauge data can be downloaded via the Bureau of Meteorology's [Water Data Online](http://www.bom.gov.au/waterdata/) portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "The Commonwealth Environmental Water Office (CEWO) is obliged to act in accordance with the Water Act and the Basin Plan.  This requires Basin water resources to be managed in a way that gives effect to relevant international agreements (to the extent to which those agreements are relevant to the use and management of the Basin water resources). The Basin Plan has a specific objective to protect and restore a subset of all water-dependent ecosystems of the Murray-Darling Bain including by ensuring that: \n",
    "* Declared Ramsar wetlands that depend on Basin water resources maintain their ecological character. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook produces the following deliverables for each Ramsar site for each 10 year time period: \n",
    "\n",
    "* Maps in the form of .jpg, polygons and rasters showing: (these maps should also include key vegetation types, including open water, as provided by the CEWO and also the Ramsar site boundary.)\n",
    "    1. Inundation extent for each month across the time period. (This could be presented so that you can slide along a hydrograph and see the resultant inundation extent on a map below. This should also identify key vegetation types. \n",
    "    2. Inundation extent for each vegetation type for each event (an event comprises the time between commencement of inundation of the vegetation type and conclusion of inundation of the vegetation type i.e. vegetation type completely dry) across the period event for critical durations (1, 3, 5, 7, >9 months)(these inundation extents are critical for different types of vegetation) \n",
    "        * Derived from remote sensing <img align='right' src=\"./InputData/TOR-AttachmentA.png\" style=\"width: 400px;\"/>\n",
    "        2.a. Comparison of remotely sensed data with ground-truthed data and OEH datasets (NSW only). \n",
    "    3. Maps of depth for critical durations, only if DEM data is available within GA or supplied by CEWO (1, 3, 5, 7, >9 months)  \n",
    "    4. Vegetation condition over the entire four year period. \n",
    "5. Graph and .csv file of inundation duration for each different vegetation type, for all natural and environmental flows, as per image to the right\n",
    "6. Table identifying: \n",
    "    * % of different vegetation classes inundated for each specified time period for each event and maximum and average depth of inundation.\n",
    "    * Time between each inundation event for 10%, 25%, 50%, 75% and 100% of each vegetation type \n",
    "e.g. \n",
    "\n",
    "| Site | Sub-site | Decade | Vegetation Type | % inundated 1 month | % inundated 3 months | % inundated 5 months | % inundated 7 months | % inundated > 9 months | Max depth | Average depth |\n",
    "|-----|----|----|----|----|----|----|----|----|----|----|\n",
    "|Banrock Station | | |River redgum | | | | | | |\n",
    "| | | |Black box | | | | | | |\n",
    "| | | |Lignum| | | | | | |\n",
    "| | | |Saltmarsh| | | | | | |\n",
    "| | | |Tall emergent aquatics| | | | | | |\n",
    "| | | |Grass/forb | | | | | | |\n",
    "\n",
    "\n",
    "7. Annual vegetation condition for each Ramsar site for the whole archive period\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code assumptions\n",
    "\n",
    "A few assumptions and decisions were made within the code that affect the outcome of the results:\n",
    "1. Maps of inundation extent across the time period 2014-2019\n",
    "2. Maps of inundation extent for each vegetation type for each event\n",
    "    a. Comparison of remotely sensed data maps with ground-truthed data and OEH datasets \n",
    "3. Maps of depth for critical durations\n",
    "4. Vegetation condition over the entire four year period\n",
    "5. Graph and .csv file of inundation duration for each different vegetation type\n",
    "6. Table identifying % of different vegetation classes inundated for each specified time period for each event and maximum and average depth of inundation and Time between each inundation event for 10%, 25%, 50%, 75% and 100% of each vegetation type\n",
    "    - Events were defined as starting when at least `EventThreshold`(defined below)% of the total wetland area is seen as wet, and ends when no more than `EventThreshold`% of the wetland remains 'wet'. \n",
    "    - If the first time a wetland is observed by Landsat 5 (1986/7), the wetland meets the event threshold, we choose NOT to include this event in the list of events\n",
    "        - We don't know how long the wetland was wet before we first observed it, and so can't provide uncertainties on event duration, so we do not include it in the results\n",
    "    - For analysis of inundation WITHIN an event, we assume that if a single pixels wets, dries, wets etc more than once, it's all related to the same wetland-wide event (which is defined above). e.g. If a pixel has a within event inundation history of 5 days wet, then is dry for 5 days, then wet for 45 days, then we consider this pixel to have been wet for 50 days during this event. \n",
    "    - We use days to define the requested metric periods, so 1 month = 30 days, 3 months = 90 days etc.\n",
    "    - Metrics for wet percentage for the 30, 90, 150, 210 and 270 day periods are calculated using >= each period. For example, a pixel that was wet for 110 days during an event will be counted in the 30 and 90 day metrics, but not the 150 day metric. \n",
    "    - Results for the duration of different percentages of inundation are presented in days. \n",
    "7. Annual vegetation condition for each Ramsar site for the whole archive period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Progress tracking)\n",
    "\n",
    "| Wetland | LiDAR DEM | Vegetation shapefile | NetCDF created? | \n",
    "|---------|-----------|----------------------|-----------------|\n",
    "| Narran Lake | <ul><li>Data are a combination of [GA's aggregated 5m DEM product](https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/89644), and NSW Spatial Services 5m DEM scenes from the couple of bits of the eastern edge of the Ramsar polygon that are cut out of the GA product</li><li>The combination happened by clipping the GA and required additional NSW scenes to the extent of the Ramsar polygon.</li><li> These were then merged together using the `mosaic` tool in ArcGIS, where the GA scene was given preference over the NSW scenes where data existed in both</li></ul>  | `VegShapeFileVegNameColumn = 'AggCOMM'` `VegShapeFileVegNameUIDColumn = None` | Yes |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Provide any particular instructions that the user might need, e.g. To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n",
    "Begin with any `iPython` magic commands, followed by standard Python packages, then any additional functionality you need from the `Scripts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import itertools\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rasterio import features, transform, mask\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape as shapelyshape\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "sys.path.append('../../Scripts')\n",
    "from dea_spatialtools import interpolate_2d\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_water_level(geom, dem, x_offsets=[0], y_offsets=[0]):  \n",
    "        \n",
    "    '''\n",
    "    Takes a shapely geometry (either LineString or MultiLineString) and a Digital Elevation\n",
    "    Model (DEM), and samples values from the DEM based on line vertices. These values are used\n",
    "    to return the median and standard deviation of elevation, which can subsequently be used\n",
    "    to compute volume (and uncertainty) based on the same DEM.\n",
    "    \n",
    "    Optionally, this function can be run with a set of x and y offset values, which shuffle \n",
    "    the geometry object left, right, up and down, and returns values for the combination \n",
    "    with the lowest standard deviation. This is intended to account for 'jitter' caused by \n",
    "    poor co-registration/georeferencing in the Sentinel 2 imagery. This is experimental, and\n",
    "    should be used with caution.\n",
    "    \n",
    "    Last modified: March 2019\n",
    "    Author: Robbi Bishop-Taylor\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    geom : Shapely LineString or MultiLineString\n",
    "        Shapely geometry for the waterline being analysed\n",
    "    dem : array\n",
    "        A 2D digital elevation model (DEM) in the form of a numpy or xarray array.\n",
    "    x_offsets : list, optional\n",
    "        A list giving a series of amounts in metre units to shuffle the geometry east and west.\n",
    "        The default is [0], which does no shuffling. E.g. setting `x_offsets=[-10, 0, 10]`\n",
    "        will shuffle the geometry west and east by 10 m, and return results for the offset which\n",
    "        produces the lowest standard deviation in extracted elevation values.\n",
    "    y_offsets : list, optional\n",
    "        A list giving a series of amounts in metre units to shuffle the geometry north and south.\n",
    "        The default is [0], which does no shuffling. E.g. setting `y_offsets=[-10, 0, 10]`\n",
    "        will shuffle the geometry north and south by 10 m, and return results for the offset which\n",
    "        produces the lowest standard deviation in extracted elevation values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    offset_min_std : Pandas DataFrame\n",
    "        A Pandas dataframe with columns giving `x_offset', 'y_offset`, `water_level` and \n",
    "        `water_level_std` values. `x_offset', 'y_offset` give the offset values which resulted\n",
    "        in the lowest standard deviation of elevation values, while `water_level` and \n",
    "        `water_level_std` return the corresponding median and standard deviation elevation values\n",
    "        themselves.\n",
    "    '''\n",
    "    \n",
    "    # First try extracting line vertex coordinates from a Shapely LineString\n",
    "    try: \n",
    "        xx, yy = geom.exterior.coords.xy\n",
    "        xx = np.array(xx)\n",
    "        yy = np.array(yy)\n",
    "\n",
    "    # If this fails, extract line vertex coordinates as if data is a Shapely MultiLineString\n",
    "    except:\n",
    "\n",
    "        xx = np.concatenate([i.coords.xy[0] for i in geom])\n",
    "        yy = np.concatenate([i.coords.xy[1] for i in geom]) \n",
    "        \n",
    "    # Convert coordinates to xarray DataArrays to facilitate fast extraction of elevation values\n",
    "    xx = xr.DataArray(xx, dims='z')\n",
    "    yy = xr.DataArray(yy, dims='z')\n",
    "    \n",
    "    # Iterate through all combinations of x and y offsets\n",
    "    all_offsets = []    \n",
    "\n",
    "    for x_offset, y_offset in itertools.product(x_offsets, y_offsets):        \n",
    "        # Sample DEM to extract elevations for each vertex coordinate\n",
    "        sampled_elevs = dem.interp(x=xx + x_offset, y=yy + y_offset).values\n",
    "\n",
    "        # Compute median and standard deviation elevation values, and add to list\n",
    "        all_offsets.append(((x_offset, y_offset, np.nanmedian(sampled_elevs), np.nanstd(sampled_elevs))))\n",
    "\n",
    "    # From the dataframe of resulting water_levels and water_level_stds for different xy offsets, \n",
    "    # select the row with the lowest standard deviation in elevation values.\n",
    "    offset_df = pd.DataFrame(all_offsets, columns=['x_offset', 'y_offset', \n",
    "                                                   'water_level', 'water_level_std'])\n",
    "    offset_min_std = offset_df.iloc[offset_df.water_level_std.idxmin()]\n",
    "\n",
    "    return offset_min_std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "\n",
    "* `AnalysisLocation`: A string descriptor of the analysis region used to label graphs.\n",
    "* `StreamGaugeName`: A string name of the stream gauge used in the analysis, used to label graphs.\n",
    "* `OutSaveLocation`: A string folder structure for the location of files written out by the [WetlandWaterDepthAnalyses.ipynb](./WetlandWaterDepthAnalyses.ipynb) notebook, and this notebook. It makes use of the `AnalysisLocation` string to generate the filepath.\n",
    "\n",
    "#### Files to analyse\n",
    "* `DatasetToLoad`: The filename of the netCDF written out by the [first notebook](./WetlandWaterDepthAnalyses.ipynb) to read in here. Note that this file must have been written out by the first notebook, as we assume the variables created in that notebook exist here. If you try to read in something else, this code will fail.\n",
    "* `SpeciesDictToLoad`: The filename of the text file written out by the [first notebook](./WetlandWaterDepthAnalyses.ipynb) that contains the species dictionary of vegetation type and numerical ID. This is used to label plots.\n",
    "* `StreamGaugeToLoad`: The filename of a csv file containing the daily maximum water discharge of a relevant stream gauge. The gauge data can be downloaded via the Bureau of Meteorology's [Water Data Online](http://www.bom.gov.au/waterdata/) portal. The code below assumes that commented lines begin with `#`. \n",
    "\n",
    "#### Analysis options\n",
    "* `EventThreshold`: e.g. 5. Used to determine the threshold for starting/ending a watering event. Watering events are defined as starting when at least `EventThreshold`(defined below)% of the total wetland area is seen as wet, and ends when no more than `EventThreshold`% of the wetland remains 'wet'. If the wetland completely dries out between events, set this threshold low e.g. 5. If the wetland contains refugial pools that rarely dry out, a wet event may need to be defined using a higher threshold. \n",
    "* `AreasToCalc`: e.g. [0.1, 0.25, 0.50, 0.75, 0.100]. A list of numbers of the decimal vegetation area thresholds to calculate. E.g. 0.1 will calculate a value for 10% of the vegetation area. These are used in section 6 below.\n",
    "* `Colormaps`: e.g. ['Blues', 'YlOrBr', 'Greens', 'Purples', 'Reds']. A list of [Matplotlib named colormaps](https://matplotlib.org/3.2.1/tutorials/colors/colormaps.html) to use to plot the different vegetation communities. \n",
    "\n",
    "#### Output filenames\n",
    "\n",
    "##### Section 2\n",
    "* `EventMapsFileName` : e.g. string. File path for the output plots for each wet event. The complete file path is created in the code below, since it adds in the event dates as it plots. \n",
    "\n",
    "##### Section 5\n",
    "* `InundationPctPerVegFigFileName`: e.g. string.jpg Output file name for the all time vegetation community based inundation plot.\n",
    "* `InundationPctPerVegDataFileName`: e.g. string.csv Output file name for the all time vegetation community based inundation csv.\n",
    "\n",
    "##### Section 6\n",
    "* `EventListFileName`: e.g. string.csv Output file name for the start and end dates and duration of each defined watering event.\n",
    "* `EventInundationTableFileName`: e.g. string.csv Output file name for the event-based inundation % metrics for each vegetation community.\n",
    "* `EventDurationTableFileName` : e.g. string.csv Output file name for the event-based duration metrics for each vegetation community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, if your `AnalysisLocation` contains special characters other than spaces, you will\n",
    "# need to modify the `replace` function used to generate the output file names.\n",
    "AnalysisLocation = 'Narran Lake'\n",
    "StreamGaugeName = 'Wilby Wilby'\n",
    "OutSaveLocation = f'OutputData/{AnalysisLocation.replace(\" \", \"\")}/'\n",
    "\n",
    "DatasetToLoad = f'{OutSaveLocation}AllDataCombined{AnalysisLocation.replace(\" \", \"\")}.nc'\n",
    "HighResDatasetToLoad = LidarVegOutputName = f'{OutSaveLocation}LiDARVeg{AnalysisLocation.replace(\" \", \"\")}.nc'\n",
    "SpeciesDictToLoad = f'{OutSaveLocation}SpeciesDict{AnalysisLocation.replace(\" \", \"\")}.txt'\n",
    "StreamGaugeToLoad = 'InputData/NarranLake/Narran_WilbyWilby_422016DailyMax.csv'\n",
    "RamsarBoundaryFile = 'InputData/NarranLake/NarranRamsarBoundary.shp'\n",
    "\n",
    "EventThreshold = 5\n",
    "AreasToCalc = [0.1, 0.25, 0.50, 0.75, 1]\n",
    "Colormaps = ['Blues', 'YlOrBr', 'Greens', 'Purples', 'Reds', 'Greys']\n",
    "\n",
    "#Outputs for section 2\n",
    "EventMapsFileName = f'{OutSaveLocation}EventMaps/{AnalysisLocation}Event' # Name completed and appended in the script below\n",
    "\n",
    "# Outputs for section 5\n",
    "InundationPctPerVegFigFileName = f'{OutSaveLocation}InundationPctPerVeg{AnalysisLocation.replace(\" \", \"\")}.jpg'\n",
    "InundationPctPerVegDataFileName = f'{OutSaveLocation}InundationPctPerVeg{AnalysisLocation.replace(\" \", \"\")}.csv'\n",
    "# Outputs for section 6\n",
    "EventListFileName = f'{OutSaveLocation}WetEventsIn{AnalysisLocation.replace(\" \", \"\")}With{str(EventThreshold)}%AreaThreshold.csv'\n",
    "EventInundationTableFileName = f'{OutSaveLocation}EventInundationMetricsFor{AnalysisLocation.replace(\" \", \"\")}.csv'\n",
    "EventDurationTableFileName = f'{OutSaveLocation}EventDurationMetricsFor{AnalysisLocation.replace(\" \", \"\")}.csv'\n",
    "EventResultsPickleFileName = f'{OutSaveLocation}EventResults/' # Name completed and appended in the script below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in our pre-analysed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDataCombined = xr.open_dataset(DatasetToLoad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a transform object from our dataset so that we can do some of the code calculations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDataCombined.attrs['transform'] = transform.from_bounds(AllDataCombined.x.min().item(),\n",
    "                                                           AllDataCombined.y.min().item(), \n",
    "                                                           AllDataCombined.x.max().item(), \n",
    "                                                           AllDataCombined.y.max().item(),\n",
    "                                                           len(AllDataCombined.x),\n",
    "                                                           len(AllDataCombined.y)\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And load in the high res version of the LiDAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HighResLidar = xr.open_dataset(HighResDatasetToLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HighResLidar.attrs['transform'] = transform.from_bounds(HighResLidar.x.min().item(),\n",
    "                                                           HighResLidar.y.min().item(), \n",
    "                                                           HighResLidar.x.max().item(), \n",
    "                                                           HighResLidar.y.max().item(),\n",
    "                                                           len(HighResLidar.x),\n",
    "                                                           len(HighResLidar.y)\n",
    "                                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate the satellite observation onto the resolution of the high res LiDAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputArray = HighResLidar['DEM'].copy()\n",
    "outputArray[:] = np.nan\n",
    "outputArray = outputArray.expand_dims({'time': AllDataCombined.time}).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1986-08-19T23:28:42.692285000, 0\n",
      "Working on 1987-05-27T23:25:36.184280000, 1\n",
      "Working on 1987-08-31T23:27:57.140299000, 2\n",
      "Working on 1987-09-16T23:28:20.938918000, 3\n",
      "Working on 1987-09-23T23:34:17.500077000, 4\n",
      "Working on 1987-10-09T23:35:00.670285000, 5\n",
      "Working on 1987-10-18T23:28:58.886491000, 6\n",
      "Working on 1987-10-25T23:34:51.652669000, 7\n",
      "Working on 1987-11-03T23:29:17.269711000, 8\n",
      "Working on 1987-11-19T23:29:37.322540000, 9\n",
      "Working on 1987-11-26T23:35:31.220454000, 10\n",
      "Working on 1987-12-05T23:29:51.470568000, 11\n",
      "Working on 1987-12-12T23:35:44.349970000, 12\n",
      "Working on 1987-12-21T23:30:05.893703000, 13\n",
      "Working on 1988-01-06T23:30:21.943656000, 14\n",
      "Working on 1988-01-13T23:36:17.098603000, 15\n",
      "Working on 1988-01-29T23:36:35.073469000, 16\n",
      "Working on 1988-02-07T23:30:58.085889000, 17\n",
      "Working on 1988-02-14T23:36:51.400570000, 18\n",
      "Working on 1988-02-23T23:31:11.332551000, 19\n",
      "Working on 1988-03-01T23:37:02.173220000, 20\n",
      "Working on 1988-03-10T23:31:20.654816000, 21\n",
      "Working on 1988-03-17T23:37:10.795774000, 22\n",
      "Working on 1988-03-26T23:31:28.694407000, 23\n",
      "Working on 1988-04-11T23:31:32.942860000, 24\n",
      "Working on 1988-05-04T23:37:31.017727000, 25\n",
      "Working on 1988-05-13T23:31:50.067703000, 26\n",
      "Working on 1988-05-29T23:31:56.566040000, 27\n",
      "Working on 1988-07-16T23:32:09.484866000, 28\n",
      "Working on 1988-08-08T23:37:56.632275000, 29\n",
      "Working on 1988-08-24T23:38:01.058928000, 30\n",
      "Working on 1988-09-02T23:32:15.233149000, 31\n",
      "Working on 1988-09-25T23:37:54.701686000, 32\n",
      "Working on 1988-10-04T23:32:06.204013000, 33\n",
      "Working on 1988-10-11T23:37:48.548723992, 34\n",
      "Working on 1988-10-20T23:31:59.636540992, 35\n",
      "Working on 1988-10-27T23:37:47.574212000, 36\n",
      "Working on 1988-11-21T23:32:03.309612008, 37\n",
      "Working on 1988-12-07T23:31:54.414556008, 38\n",
      "Working on 1988-12-14T23:37:35.365701000, 39\n",
      "Working on 1988-12-23T23:31:45.854535000, 40\n",
      "Working on 1988-12-30T23:37:33.541016008, 41\n",
      "Working on 1989-01-08T23:31:43.690836008, 42\n",
      "Working on 1989-01-15T23:37:21.806236008, 43\n",
      "Working on 1989-01-24T23:31:24.733640008, 44\n",
      "Working on 1989-01-31T23:37:16.556857000, 45\n",
      "Working on 1989-02-09T23:31:26.289789992, 46\n",
      "Working on 1989-02-25T23:31:17.682347000, 47\n",
      "Working on 1989-03-04T23:36:54.919531000, 48\n",
      "Working on 1989-03-20T23:36:15.188500008, 49\n",
      "Working on 1989-04-14T23:30:32.830728008, 50\n",
      "Working on 1989-04-30T23:30:15.161764008, 51\n",
      "Working on 1989-05-23T23:35:38.338824008, 52\n",
      "Working on 1989-06-17T23:29:17.713941000, 53\n",
      "Working on 1989-06-24T23:34:59.340537992, 54\n",
      "Working on 1989-07-03T23:29:04.059240008, 55\n",
      "Working on 1989-07-10T23:34:42.131697000, 56\n",
      "Working on 1989-07-19T23:28:43.487707000, 57\n",
      "Working on 1989-07-26T23:34:23.266309000, 58\n",
      "Working on 1989-08-04T23:28:24.638543000, 59\n",
      "Working on 1989-08-11T23:34:01.704561000, 60\n",
      "Working on 1989-08-20T23:28:02.325949000, 61\n",
      "Working on 1989-08-27T23:33:41.927380008, 62\n",
      "Working on 1989-09-05T23:27:42.490475000, 63\n",
      "Working on 1989-09-12T23:33:10.574543000, 64\n",
      "Working on 1989-09-21T23:27:11.215455000, 65\n",
      "Working on 1989-09-28T23:32:57.539004008, 66\n",
      "Working on 1989-10-14T23:32:48.452284008, 67\n",
      "Working on 1989-12-01T23:31:05.396421000, 68\n",
      "Working on 1989-12-10T23:25:02.899419000, 69\n",
      "Working on 1989-12-26T23:24:35.058664008, 70\n",
      "Working on 1990-03-07T23:28:01.337409992, 71\n",
      "Working on 1990-03-16T23:21:57.591889992, 72\n",
      "Working on 1990-05-03T23:21:34.702407000, 73\n",
      "Working on 1990-05-19T23:21:43.062120008, 74\n",
      "Working on 1990-06-04T23:21:38.364432008, 75\n",
      "Working on 1990-06-20T23:21:36.997925000, 76\n",
      "Working on 1990-07-06T23:21:35.387168008, 77\n",
      "Working on 1990-07-29T23:27:16.994405000, 78\n",
      "Working on 1990-08-14T23:27:12.860761992, 79\n",
      "Working on 1990-08-23T23:21:21.348541992, 80\n",
      "Working on 1990-09-08T23:21:17.443357992, 81\n",
      "Working on 1990-10-01T23:26:51.311744008, 82\n",
      "Working on 1990-10-10T23:21:03.809465992, 83\n",
      "Working on 1990-10-17T23:26:46.700100008, 84\n",
      "Working on 1990-10-26T23:20:57.490437992, 85\n",
      "Working on 1990-11-11T23:20:57.856521992, 86\n",
      "Working on 1990-11-27T23:20:44.453496984, 87\n",
      "Working on 1990-12-13T23:20:48.436593000, 88\n",
      "Working on 1991-01-30T23:22:01.749656024, 89\n",
      "Working on 1991-02-15T23:22:18.240513000, 90\n",
      "Working on 1991-03-03T23:22:35.041649016, 91\n",
      "Working on 1991-03-19T23:22:57.556789992, 92\n",
      "Working on 1991-04-04T23:23:07.145389000, 93\n",
      "Working on 1991-04-20T23:23:26.262901000, 94\n",
      "Working on 1991-05-06T23:23:46.256669000, 95\n",
      "Working on 1991-06-23T23:24:29.041025000, 96\n",
      "Working on 1991-07-25T23:24:55.073961992, 97\n",
      "Working on 1991-08-26T23:25:06.339133992, 98\n",
      "Working on 1991-09-11T23:25:18.698633992, 99\n",
      "Working on 1991-09-27T23:25:23.554789000, 100\n",
      "Working on 1991-10-13T23:25:34.961195016, 101\n",
      "Working on 1991-11-30T23:25:52.104782984, 102\n",
      "Working on 1991-12-16T23:25:50.609969000, 103\n",
      "Working on 1992-01-01T23:25:43.325840008, 104\n",
      "Working on 1992-01-17T23:25:42.716965992, 105\n",
      "Working on 1992-02-18T23:25:36.421401992, 106\n",
      "Working on 1992-03-05T23:25:37.558843016, 107\n",
      "Working on 1992-03-21T23:25:37.301541000, 108\n",
      "Working on 1992-04-06T23:25:27.833877992, 109\n",
      "Working on 1992-04-22T23:25:19.340749000, 110\n",
      "Working on 1992-05-08T23:25:11.595101992, 111\n",
      "Working on 1992-05-24T23:25:03.155302984, 112\n",
      "Working on 1992-06-25T23:24:42.170324008, 113\n",
      "Working on 1992-07-11T23:24:30.054237000, 114\n",
      "Working on 1992-07-27T23:24:17.582724008, 115\n",
      "Working on 1992-08-19T23:29:42.519185992, 116\n",
      "Working on 1992-09-04T23:29:29.446828008, 117\n",
      "Working on 1992-09-13T23:23:34.917670984, 118\n",
      "Working on 1992-09-20T23:29:13.928697992, 119\n",
      "Working on 1992-09-29T23:23:14.519212008, 120\n",
      "Working on 1992-10-15T23:22:57.484344008, 121\n",
      "Working on 1992-10-22T23:28:40.855437992, 122\n",
      "Working on 1993-01-19T23:23:09.358481992, 123\n",
      "Working on 1993-02-27T23:29:15.813502984, 124\n",
      "Working on 1993-03-08T23:23:36.246446984, 125\n",
      "Working on 1993-03-24T23:23:35.754134984, 126\n",
      "Working on 1993-04-09T23:23:43.612060008, 127\n",
      "Working on 1993-04-25T23:23:52.666077992, 128\n",
      "Working on 1993-06-12T23:23:59.693161992, 129\n",
      "Working on 1993-06-19T23:29:44.479704008, 130\n",
      "Working on 1993-06-28T23:23:53.340561000, 131\n",
      "Working on 1993-07-14T23:23:51.337542984, 132\n",
      "Working on 1993-07-21T23:29:37.795269992, 133\n",
      "Working on 1993-08-06T23:29:37.569488008, 134\n",
      "Working on 1993-08-15T23:23:50.733872008, 135\n",
      "Working on 1993-08-22T23:29:37.124053000, 136\n",
      "Working on 1993-09-07T23:29:35.914881000, 137\n",
      "Working on 1993-09-23T23:29:32.638953000, 138\n",
      "Working on 1993-10-18T23:23:37.519021992, 139\n",
      "Working on 1993-10-25T23:29:22.035833000, 140\n",
      "Working on 1993-11-03T23:23:31.061973000, 141\n",
      "Working on 1993-11-10T23:29:13.647990984, 142\n",
      "Working on 1993-11-26T23:29:08.224269992, 143\n",
      "Working on 1993-12-12T23:29:03.358025000, 144\n",
      "Working on 1993-12-28T23:28:53.768134984, 145\n",
      "Working on 1994-01-06T23:22:59.324485000, 146\n",
      "Working on 1994-01-22T23:22:44.783277000, 147\n",
      "Working on 1994-01-29T23:28:28.572777000, 148\n",
      "Working on 1994-02-07T23:22:35.987865992, 149\n",
      "Working on 1994-02-23T23:22:21.017212008, 150\n",
      "Working on 1994-03-11T23:22:05.845053000, 151\n",
      "Working on 1994-03-18T23:27:47.715997992, 152\n",
      "Working on 1994-03-27T23:21:52.061485992, 153\n",
      "Working on 1994-04-03T23:27:31.486153992, 154\n",
      "Working on 1994-04-19T23:27:13.691336008, 155\n",
      "Working on 1994-04-28T23:21:17.390476008, 156\n",
      "Working on 1994-05-05T23:26:56.266515016, 157\n",
      "Working on 1994-05-14T23:20:59.334913992, 158\n",
      "Working on 1994-05-21T23:26:39.333806984, 159\n",
      "Working on 1994-05-30T23:20:42.461542984, 160\n",
      "Working on 1994-06-15T23:20:22.107699016, 161\n",
      "Working on 1994-07-01T23:19:57.365229992, 162\n",
      "Working on 1994-07-08T23:25:33.270009000, 163\n",
      "Working on 1994-07-17T23:19:31.572613992, 164\n",
      "Working on 1994-07-24T23:25:06.507248008, 165\n",
      "Working on 1994-08-02T23:19:04.002973000, 166\n",
      "Working on 1994-08-09T23:24:41.243270984, 167\n",
      "Working on 1994-09-03T23:18:18.569653992, 168\n",
      "Working on 1994-09-10T23:23:54.650901000, 169\n",
      "Working on 1994-09-19T23:17:53.063509000, 170\n",
      "Working on 1994-10-21T23:16:53.727057000, 171\n",
      "Working on 1994-10-28T23:22:25.536614984, 172\n",
      "Working on 1994-11-06T23:16:19.566947016, 173\n",
      "Working on 1994-11-22T23:15:45.907856008, 174\n",
      "Working on 1994-12-15T23:20:52.329696008, 175\n",
      "Working on 1994-12-24T23:14:47.971988008, 176\n",
      "Working on 1995-01-09T23:14:14.448189992, 177\n",
      "Working on 1995-01-25T23:13:38.105105992, 178\n",
      "Working on 1995-02-01T23:19:32.011825992, 179\n",
      "Working on 1995-02-17T23:18:28.319958984, 180\n",
      "Working on 1995-03-05T23:17:50.884729032, 181\n",
      "Working on 1995-03-14T23:11:41.673350984, 182\n",
      "Working on 1995-03-21T23:17:10.747843016, 183\n",
      "Working on 1995-03-30T23:11:02.610616968, 184\n",
      "Working on 1995-04-06T23:16:32.472893000, 185\n",
      "Working on 1995-04-15T23:10:23.095350984, 186\n",
      "Working on 1995-05-08T23:15:08.767288040, 187\n",
      "Working on 1995-06-18T23:07:28.627344008, 188\n",
      "Working on 1995-06-25T23:12:56.721162024, 189\n",
      "Working on 1995-07-04T23:06:45.204036008, 190\n",
      "Working on 1995-07-20T23:06:01.985830952, 191\n",
      "Working on 1995-08-12T23:10:47.052935016, 192\n",
      "Working on 1995-08-21T23:04:35.302219976, 193\n",
      "Working on 1995-08-28T23:10:02.402518984, 194\n",
      "Working on 1995-09-06T23:03:49.463131016, 195\n",
      "Working on 1995-09-13T23:09:15.549184968, 196\n",
      "Working on 1995-09-29T23:08:26.203221960, 197\n",
      "Working on 1995-10-08T23:02:10.527072008, 198\n",
      "Working on 1995-10-15T23:07:34.365614984, 199\n",
      "Working on 1995-10-31T23:06:46.517446024, 200\n",
      "Working on 1995-11-09T23:00:55.160569032, 201\n",
      "Working on 1995-11-25T23:01:14.491467976, 202\n",
      "Working on 1995-12-11T23:02:15.471905992, 203\n",
      "Working on 1995-12-27T23:03:26.487440008, 204\n",
      "Working on 1996-01-12T23:04:34.764573000, 205\n",
      "Working on 1996-01-28T23:05:42.405294024, 206\n",
      "Working on 1996-02-29T23:07:52.872904008, 207\n",
      "Working on 1996-03-16T23:08:55.455245000, 208\n",
      "Working on 1996-04-01T23:09:56.437485000, 209\n",
      "Working on 1996-04-17T23:10:55.783931016, 210\n",
      "Working on 1996-05-19T23:12:48.886262984, 211\n",
      "Working on 1996-07-06T23:15:24.246422984, 212\n",
      "Working on 1996-07-22T23:16:15.253461000, 213\n",
      "Working on 1996-08-07T23:17:05.969339016, 214\n",
      "Working on 1996-08-14T23:23:14.691945032, 215\n",
      "Working on 1996-08-23T23:17:56.376896968, 216\n",
      "Working on 1996-09-08T23:18:50.252905032, 217\n",
      "Working on 1996-09-15T23:25:00.364126024, 218\n",
      "Working on 1996-10-10T23:20:32.643342024, 219\n",
      "Working on 1996-11-02T23:27:27.564235016, 220\n",
      "Working on 1996-11-11T23:22:05.731257032, 221\n",
      "Working on 1996-11-18T23:28:11.824403976, 222\n",
      "Working on 1996-11-27T23:22:51.162117000, 223\n",
      "Working on 1996-12-13T23:23:37.045037000, 224\n",
      "Working on 1996-12-29T23:24:22.912104008, 225\n",
      "Working on 1997-01-05T23:30:29.855325000, 226\n",
      "Working on 1997-02-15T23:26:32.442761032, 227\n",
      "Working on 1997-03-03T23:27:11.653070984, 228\n",
      "Working on 1997-04-04T23:28:24.428203016, 229\n",
      "Working on 1997-04-11T23:34:26.253389000, 230\n",
      "Working on 1997-04-20T23:28:58.600377992, 231\n",
      "Working on 1997-04-27T23:35:00.525750984, 232\n",
      "Working on 1997-06-07T23:30:39.075669000, 233\n",
      "Working on 1997-06-14T23:36:39.611222024, 234\n",
      "Working on 1997-06-23T23:31:11.538334984, 235\n"
     ]
    }
   ],
   "source": [
    "for ix, timestep in enumerate(AllDataCombined['time']):\n",
    "    print(f'Working on {timestep.data}, {ix}')\n",
    "    # Interpolate the satellite data onto the grid of the Lidar\n",
    "    WetonLidar = AllDataCombined['wet'].isel(time=ix).interp(x = HighResLidar.x, y= HighResLidar.y, method='nearest')\n",
    "    # Convert to a boolean wet/not wet\n",
    "    WetonLidar = np.isfinite(WetonLidar)\n",
    "    if WetonLidar.sum() > 0:\n",
    "        # Polygonize the wet bits to turn them into polygons\n",
    "        WetPolygons = features.shapes(WetonLidar.data.astype('float32'),\n",
    "                                      transform=HighResLidar.attrs['transform'],\n",
    "                                     connectivity=8)\n",
    "        # Convert polygons into a geodataframe\n",
    "        DiscreteWetPolygons = gpd.GeoDataFrame(WetPolygons, columns =['stringgeom', 'wetyes'])\n",
    "        # Fix up the geometry attribute\n",
    "        DiscreteWetPolygons['geometry'] = None\n",
    "        for idx, poly in DiscreteWetPolygons.iterrows():\n",
    "            DiscreteWetPolygons.loc[idx,'geometry'] = shapelyshape(poly[0])\n",
    "        # Set the geometry of the dataframe to be the shapely geometry we just created\n",
    "        DiscreteWetPolygons = DiscreteWetPolygons.set_geometry('geometry')\n",
    "        # We need to add the crs back onto the dataframe\n",
    "        DiscreteWetPolygons.crs = 'EPSG:3577'\n",
    "        # Drop the nan polygon created from the nan values\n",
    "        DiscreteWetPolygons = DiscreteWetPolygons.loc[DiscreteWetPolygons['wetyes'] == 1]\n",
    "        # And drop the temporary stringgeom\n",
    "        DiscreteWetPolygons.drop('stringgeom', axis=1, inplace=True)\n",
    "        # Now loop through each polygon shape to calculate a water surface height\n",
    "        for shapez in DiscreteWetPolygons.iterrows():\n",
    "            EstWaterLevel = get_water_level(shapez[1]['geometry'], HighResLidar.DEM)\n",
    "            EstWaterHeight = EstWaterLevel['water_level'] - HighResLidar.DEM\n",
    "            EstWaterHeight.attrs['transform'] = HighResLidar.attrs['transform']\n",
    "            y, x = HighResLidar.DEM.values.shape\n",
    "            # Now convert the polgons into a numpy array\n",
    "            RasterWetPoly = features.rasterize(shapes=shapez[1],\n",
    "                                                    out_shape=(y, x),\n",
    "                                                    all_touched=False,\n",
    "                                                    fill=np.nan,\n",
    "                                                    transform=HighResLidar.attrs['transform'])\n",
    "            # Convert result to a xarray.DataArray using coords etc from the LiDAR data\n",
    "            RasterWetPoly = xr.DataArray(RasterWetPoly,\n",
    "                                     coords=[HighResLidar.y, HighResLidar.x],\n",
    "                                     dims=['y', 'x'],\n",
    "                                     name='wetBits',\n",
    "                                     attrs=HighResLidar.attrs)\n",
    "            WaterBodyHeight = EstWaterHeight.where((RasterWetPoly == 1) & (EstWaterHeight >= 0))\n",
    "            outputArray.loc[{'time': outputArray['time'][ix]}] = xr.where(np.isfinite(WaterBodyHeight), WaterBodyHeight, \n",
    "                                                                          outputArray.loc[{'time': outputArray['time'][ix]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=[6, 6]);\n",
    "HighResLidar.DEM.plot();\n",
    "#DiscreteWetPolygons.plot(color=None, edgecolor='r', linewidth = 2, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
