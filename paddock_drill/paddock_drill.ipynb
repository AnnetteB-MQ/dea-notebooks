{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal, ogr, gdal_array\n",
    "import dask\n",
    "import datacube \n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "import os\n",
    "#import custom functions\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEAPlotting, SpatialTools, BandIndices, DEADataHandling\n",
    "from load_data import load_data\n",
    "from transform_tuple import transform_tuple\n",
    "from query_from_shp import query_from_shp\n",
    "from rsgislib.segmentation import segutils\n",
    "from rasterstats import zonal_stats\n",
    "from imageSeg import imageSeg\n",
    "import fiona\n",
    "import rasterio.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is your data and results folder?\n",
    "data = 'data/'\n",
    "results = 'results/'\n",
    "\n",
    "#do I need to load in new data from the datacube\n",
    "#or have you already saved it previously?\n",
    "load_fresh_data = True\n",
    "\n",
    "sensors = ['ls5','ls7','ls8']\n",
    "\n",
    "#are we using a polygon to mask the AOI?\n",
    "polygon_mask = True\n",
    "shp_fpath = 'data/spatial/wagga_paddockDrill_AOI.shp'\n",
    "\n",
    "#If not using a polygon then enter your AOI coords\n",
    "#below:\n",
    "lat, lon = -34.578728, 146.264338\n",
    "latLon_adjust = 0.2\n",
    "\n",
    "#Input your area of interest's name, coords, and \n",
    "#the year you're interested in?\n",
    "AOI = 'WaggaWagga'\n",
    "year= '2018'\n",
    "time_period = ('2018-03-01', '2018-10-30')\n",
    "\n",
    "#-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a folder to keep things neat\n",
    "directory = results + AOI + \"_\" + year\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "\n",
    "results = results + AOI + \"_\" + year + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls5_loading...\n",
      "ls5_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls7_loading...\n",
      "ls7_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls8_loading...\n",
      "ls8_loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "src/load_data.py:24: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  if not landsat_ds:\n"
     ]
    }
   ],
   "source": [
    "if load_fresh_data == True:\n",
    "    if polygon_mask == True:\n",
    "        #set up query\n",
    "        query = query_from_shp(shp_fpath, time_period[0], time_period[1], dask_chunks = 0)\n",
    "        #landsat\n",
    "        landsat = load_data(dc_name = 'irrigated_areas', sensors=sensors,\n",
    "                  export_name = data + AOI + \"_\" + year + '.nc', query=query)\n",
    "        #wofs\n",
    "        dc = datacube.Datacube(app='wofs')\n",
    "        del query['time'] \n",
    "        wofs_alltime = dc.load(product = 'wofs_summary', **query)\n",
    "        \n",
    "        #masking the returned array to the polygon area\n",
    "        with fiona.open(shp_fpath) as shapes:\n",
    "                crs = geometry.CRS(shapes.crs_wkt)\n",
    "                first_geometry = next(iter(shapes))['geometry']\n",
    "                geom = geometry.Geometry(first_geometry, crs=crs)\n",
    "\n",
    "        mask = rasterio.features.geometry_mask([geom.to_crs(landsat.geobox.crs) for geoms in [geom]],\n",
    "                                                   out_shape=landsat.geobox.shape,\n",
    "                                                   transform=landsat.geobox.affine,\n",
    "                                                   all_touched=False,\n",
    "                                                   invert=True)\n",
    "        # Mask the xarrays\n",
    "        landsat = landsat.where(mask)\n",
    "        #wofs_alltime = wofs_alltime.where(mask)\n",
    "        #datacube.storage.storage.write_dataset_to_netcdf(landsat, results + AOI + \"_\" + year + '.nc')\n",
    "    else:\n",
    "        # Set up query\n",
    "        query = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "                 'lat': (lat - latLon_adjust, lat + latLon_adjust),\n",
    "                 'time': time_period}\n",
    "        query['dask_chunks']= {'x': 500, 'y': 500}\n",
    "\n",
    "        #landsat\n",
    "        landsat = load_data(dc_name = 'irrigated_areas', sensors=sensors,\n",
    "                  export_name = data + AOI + \"_\" + year + '.nc', query=query)\n",
    "        #wofs\n",
    "        dc = datacube.Datacube(app='wofs')\n",
    "        del query['time'] \n",
    "        wofs_alltime = dc.load(product = 'wofs_summary', **query)\n",
    "        \n",
    "else:\n",
    "    #load in data from saved netcdf file\n",
    "    landsat = xr.open_dataset(\"data/wagga_Summer2017-18.nc\")\n",
    "    \n",
    "    #landsat = xr.open_dataset('data/' + AOI +  \"_\" + year + '.nc')\n",
    "    #load wofs for masking\n",
    "    query_wofs = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "                 'lat': (lat - latLon_adjust, lat + latLon_adjust)} \n",
    "    dc = datacube.Datacube(app='wofs')\n",
    "    wofs_alltime = dc.load(product = 'wofs_summary', **query_wofs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#band indices calculation\n",
    "def ndvi_func(nir, red):\n",
    "    return ((nir - red)/(nir + red))\n",
    "\n",
    "def ndvi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        ndvi_func, ds.nir, ds.red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "NDVI_landsat = ndvi_ufunc(landsat).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1283: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input, interpolation)\n"
     ]
    }
   ],
   "source": [
    "#calculate per pixel summary stats\n",
    "# NDVI_max = NDVI_landsat.max('time').rename('NDVI_max').compute()\n",
    "NDVI_95 = NDVI_landsat.quantile(dim='time', q=[0.95], keep_attrs=True).rename('95%_ndvi')\n",
    "NDVI_95 = NDVI_95.squeeze()\n",
    "NDVI_95 = NDVI_95.drop('quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export Gtiff for use in Image segmentation\n",
    "transform, projection = transform_tuple(NDVI_max, (NDVI_max.x, NDVI_max.y), epsg=3577)\n",
    "\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"ndvi_max.tif\",\n",
    "              NDVI_max.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=np.nan)\n",
    "\n",
    "SpatialTools.array_to_geotiff(results + AOI + \"_\" + year + \"ndvi_95.tif\",\n",
    "              NDVI_95.values, geo_transform = transform, \n",
    "              projection = projection, nodata_val=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stretch Input Image\n",
      "Add 1 to stretched file to ensure there are no all zeros (i.e., no data) regions created.\n",
      "Create Input Image Mask.\n",
      "Mask stretched Image.\n",
      "Deleting file: ./WaggaWagga_2018_stchdonly.kea\n",
      "Deleting file: ./WaggaWagga_2018_stchdonlyOff.kea\n",
      "Deleting file: ./WaggaWagga_2018_stchdmaskonly.kea\n",
      "Performing KMeans.\n",
      "Apply KMeans to image.\n",
      "Eliminate Single Pixels.\n",
      "Perform clump.\n",
      "Eliminate small pixels.\n",
      "Relabel clumps.\n",
      "Calculate image statistics and build pyramids.\n",
      "Deleting file: ./WaggaWagga_2018_kmeansclusters.gmtxt\n",
      "Deleting file: ./WaggaWagga_2018_kmeans.kea\n",
      "Deleting file: ./WaggaWagga_2018_kmeans.kea.aux.xml\n",
      "Deleting file: ./WaggaWagga_2018_kmeans_nosgl.kea\n",
      "Deleting file: ./WaggaWagga_2018_kmeans_nosglTMP.kea\n",
      "Deleting file: ./WaggaWagga_2018_clumps.kea\n",
      "Deleting file: ./WaggaWagga_2018_clumps_elim.kea\n",
      "Deleting file: ./WaggaWagga_2018_stchd.kea\n"
     ]
    }
   ],
   "source": [
    "# setup input filenames\n",
    "InputNDVIStats = results + AOI + \"_\" + year + \"ndvi_95.tif\"\n",
    "KEAFile = results + AOI + '_' + year + '.kea'\n",
    "SegmentedKEAFile = results + AOI + '_' + year + '_sheperdSEG.kea'\n",
    "SegmentedTiffFile = results + AOI + '_' + year + '_sheperdSEG.tif'\n",
    "SegmentedPolygons = results + AOI + '_' + year + '_SEGpolygons.shp'\n",
    "imageSeg(InputNDVIStats, KEAFile, SegmentedKEAFile, SegmentedTiffFile, SegmentedPolygons, minPxls = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(results + AOI + '_' + year + '_SEGpolygons.shp')\n",
    "# #calculate zonal mean of NDVI\n",
    "# gdf['mean'] = pd.DataFrame(zonal_stats(vectors=gdf['geometry'], raster=InputNDVIStats, stats='mean'))['mean']\n",
    "# #calculate area of polygons\n",
    "# gdf['area'] = gdf['geometry'].area\n",
    "# #filter by area and mean NDVI\n",
    "# highNDVI = gdf['mean'] >= 0.1\n",
    "# smallArea = gdf['area'] <= 5500000\n",
    "# gdf = gdf[highNDVI & smallArea]\n",
    "# #export shapefile\n",
    "# gdf.to_file(results + AOI + \"_\" + year + \"_Irrigated.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_timeseries(dataArray, shp_loc, results_loc, feature_name, stat='mean', csv=False, netcdf=False, plot=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Given an xarray dataArray and a shapefile, generates a timeseries of zonal statistics across n number of \n",
    "    uniquely labelled polygons. The function exports a .csv of the stats, a netcdf containing the stats, and .pdf plots.\n",
    "    Requires the installation of the rasterstats module: https://pythonhosted.org/rasterstats/installation.html\n",
    "    \n",
    "    Inputs:\n",
    "    data = xarray dataarray (note dataarray, not dataset - it is a requirement the data only have a single variable).\n",
    "    shp_loc = string. Location of the shapefile used to extract the zonal timseries.\n",
    "    results_loc = string. Location of the directory where results should export.\n",
    "    feature_name = string. Name of attribute column in the shapefile that is of interest - used to label dataframe, plots etc.\n",
    "    stat = string.  The statistic you want to extract. Options include 'count', 'max', 'median', 'min', 'std', 'mean'.\n",
    "    plot = Boolean. If True, function will produce pdfs of timeseries for each polygon in the shapefile.\n",
    "    csv = Boolean. If True, function will export results as a .csv.\n",
    "    netcdf = Boolean. If True, function will export results as a netcdf.\n",
    "    \n",
    "    Last modified: May 2018\n",
    "    Author: Chad Burton    \n",
    "    \"\"\"\n",
    "\n",
    "    #use dask to chunk the data along the time axis in case its a very large dataset\n",
    "    dataArray = dataArray.chunk(chunks = {'x':500, 'y':500})\n",
    "    \n",
    "    #create 'transform' tuple to provide ndarray with geo-referencing data. \n",
    "    one = float(dataArray.x[0])\n",
    "    two = float(dataArray.y[0] - dataArray.y[1])\n",
    "    three = 0.0\n",
    "    four = float(dataArray.y[0])\n",
    "    five = 0.0\n",
    "    six = float(dataArray.x[0] - dataArray.x[1])\n",
    "\n",
    "    transform_zonal = (one, two, three, four, five, six)\n",
    "\n",
    "    #import shapefile, make sure its in the right projection to match the dataArray\n",
    "    #and set index to the feature_name\n",
    "    project_area = gpd.read_file(shp_loc)               #get the shapefile\n",
    "    reproj=int(str(dataArray.crs)[5:])                  #do a little hack to get EPSG from the dataArray \n",
    "    project_area = project_area.to_crs(epsg=reproj)     #reproject shapefile to match dataArray\n",
    "    project_area = project_area.set_index(feature_name) #set the index\n",
    "    \n",
    "    #define the general function\n",
    "    def zonalStats(dataArray, stat=stat): \n",
    "        \"\"\"extract the zonal statistics of all\n",
    "        pixel values within each polygon\"\"\"\n",
    "        stats = [] \n",
    "        for i in dataArray:\n",
    "            x = rs.zonal_stats(project_area, i, transform=transform_zonal, stats=stat)    \n",
    "            stats.append(x)\n",
    "        #extract just the values from the results, and convert 'None' values to nan\n",
    "        stats = [[t[stat] if t[stat] is not None else np.nan for t in feature] for feature in stats]\n",
    "        stats = np.array(stats)\n",
    "        return stats\n",
    "\n",
    "    #use the zonal_stats functions to extract the stats:\n",
    "    n = len(project_area) #number of polygons in the shapefile (defines the dimesions of the output)\n",
    "    statistics = dataArray.data.map_blocks(zonalStats, chunks=(-1,n), drop_axis=1, dtype=np.float64).compute()\n",
    "\n",
    "    #get unique identifier and timeseries data from the inputs \n",
    "    colnames = pd.Series(project_area.index.values)\n",
    "    time = pd.Series(dataArray['time'].values)\n",
    "\n",
    "    #define functions for cleaning up the results of the rasterstats operation\n",
    "    def tidyresults(results):\n",
    "        x = pd.DataFrame(results).T #transpose\n",
    "        x = x.rename(colnames, axis='index') #rename the columns to the timestamp\n",
    "        x = x.rename(columns = time)\n",
    "        return x\n",
    "\n",
    "    #place results into indexed dataframes using tidyresults function\n",
    "    statistics_df = tidyresults(statistics)\n",
    "    \n",
    "    #convert into xarray for merging into a dataset\n",
    "    stat_xr = xr.DataArray(statistics_df, dims=[feature_name, 'time'], coords={feature_name: statistics_df.index, 'time': time}, name= stat)\n",
    "    \n",
    "    #options for exporting results as csv, netcdf, pdf plots\n",
    "    #export results as a .csv\n",
    "    if csv:\n",
    "        statistics_df.to_csv('{0}{1}.csv'.format(results_loc, stat))\n",
    "                             \n",
    "    if netcdf:\n",
    "        #export out results as netcdf\n",
    "        stat_xr.to_netcdf('{0}zonalstats_{1}.nc'.format(results_loc, stat), mode='w',format='NETCDF4') \n",
    "\n",
    "    if plot:     \n",
    "        #place the data from the xarray into a list\n",
    "        plot_data = []\n",
    "        for i in range(0,len(stat_xr[feature_name])):\n",
    "            x = stat_xr.isel([stat], **{feature_name: i})\n",
    "            plot_data.append(x)\n",
    "\n",
    "        #extract the unique names of each polygon\n",
    "        feature_names = list(stat_xr[feature_name].values)\n",
    "\n",
    "        #zip the both the data and names together as a dictionary \n",
    "        monthly_dict = dict(zip(feature_names,plot_data))\n",
    "\n",
    "        #create a function for generating the plots\n",
    "        def plotResults(dataArray, title):\n",
    "            \"\"\"a function for plotting up the results of the\n",
    "            fractional cover change and exporting it out as pdf \"\"\"\n",
    "            x = dataArray.time.values\n",
    "            y = dataArray.data          \n",
    "\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.plot(x, y,'k', color='#228b22', linewidth = 1)\n",
    "            plt.grid(True, linestyle ='--')\n",
    "            plt.title(title)\n",
    "            plt.savefig('{0}{1}.pdf'.format(results_loc, title), bbox_inches='tight')\n",
    "\n",
    "        #loop over the dictionaries and create the plots\n",
    "        {key: plotResults(monthly_dict[key], key + \"_\"+ stat) for key in monthly_dict} \n",
    "    \n",
    "    #return the results as a dataframe\n",
    "    return statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
