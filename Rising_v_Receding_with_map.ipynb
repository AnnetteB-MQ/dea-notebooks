{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rising and falling flow rates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two images of a floodplain are generated: one representing rising water and one representing falling water. This was done by firstly generating a hydrograph, then choosing a threshold for flow (ie we are only interested in higher flows that represent flooding events). All satellite images corresponding to days when the flow level was higher than the threshold were retrieved from the WOfS database. Then it was attempted to separate the dates into 2 lists: one for days when the water was rising and one for days when the water was receeding. This was done by using a decision tree that checked if the day of the pass had a higher or lower reading than the gauge reading x days later. If the gauge reading x days after the pass was higher, that pass was put in the rising category. If it was lower, it was put in the receeding category. The user can check the accuracy of this method against the hydropgraph. Then an image of each dataset was generated to show the difference between rising water and receeding water."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This code accesses python files located in the Scripts folder. Note that the Water Data Online data has been cached and this noetbook will access the cached code. If you want the most up to date data live from the BoM Water Data Online website, simply go into the Scripts folder and move the stations.pkl file out of there so the code can't find it. Then it will go look on the website instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datacube\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry \n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.helpers import write_geotiff\n",
    "\n",
    "sys.path.append('Scripts')\n",
    "import dea_bom\n",
    "\n",
    "dc = datacube.Datacube(app='Inundation_mapping')\n",
    "\n",
    "\n",
    "#retrieving data\n",
    "stations_pkl = Path('Scripts/stations.pkl')\n",
    "\n",
    "# If cache exists, get station data from cache\n",
    "if stations_pkl.exists():\n",
    "    print('Loading from cache')\n",
    "    stations = pickle.load(open(str(stations_pkl), 'rb'))\n",
    "else:\n",
    "    print('Fetching from BoM')\n",
    "    stations = dea_bom.get_stations()\n",
    "    pickle.dump(stations, open(str(stations_pkl), 'wb'))\n",
    "\n",
    "# Filter list to stations with available data\n",
    "stations_with_data = pickle.load(open(str('Scripts/stations_with_data.pkl'), 'rb'))\n",
    "stations = [i for i in stations if i.name in stations_with_data]\n",
    "\n",
    "# Preview the first five stations loaded\n",
    "print(f'{len(stations)} stations loaded; e.g.:')\n",
    "stations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_data, station = dea_bom.ui_select_station(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the hydrograph of the chosen gauge (in cubic meters per second). Please enter a minimum threshold for flow rate data you wish to analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set threshold for flow rate\n",
    "yaxis_lower_parameter = 40\n",
    "\n",
    "\n",
    "# The lat and lon takes the location of the gauge. You can change the lat \n",
    "# and lon to a different location if necessary, just comment out out this \n",
    "# lat, lon = pos line below and define your own.\n",
    "lat, lon = station.pos\n",
    "\n",
    "# lat =\n",
    "# lon =\n",
    "\n",
    "# The buffer is how many meters radius around the location you want to display.\n",
    "buffer = 10000\n",
    "\n",
    "lat, lon\n",
    "\n",
    "#Dask loading wofs_albers data\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),    \n",
    "         'time': ('1988-01-01', '2019-08-22'), \n",
    "         'crs': 'EPSG:3577'} \n",
    "dc = datacube.Datacube(app='dc-WOfS')\n",
    "wofs_albers= dc.load(product = 'wofs_albers', dask_chunks = {}, group_by='solar_day', **query)\n",
    "\n",
    "#convert Pandas dataframe to xArray for merging with WOfS\n",
    "gauge_data_xr = gauge_data.to_xarray() \n",
    "\n",
    "#this is the xArray merge function\n",
    "merged_data = gauge_data_xr.interp(Timestamp=wofs_albers.time) \n",
    "\n",
    "#Now define the passes to load based on user input\n",
    "specified_satellite_passes = merged_data.where((merged_data.Value > yaxis_lower_parameter), drop=True)\n",
    "specified_satellite_passes = specified_satellite_passes.drop('Timestamp')\n",
    "\n",
    "date_list = specified_satellite_passes.time.values\n",
    "\n",
    "\n",
    "# load the passes that happened during the specified flow parameters\n",
    "specified_passes = wofs_albers.sel(time=date_list).compute()\n",
    "\n",
    "# Cloud mask\n",
    "cc = masking.make_mask(specified_passes.water, cloud=True)\n",
    "npixels_per_slice = specified_passes.water.shape[1]*specified_passes.water.shape[2]\n",
    "npixels_per_slice\n",
    "ncloud_pixels = cc.sum(dim='x').sum(dim='y')\n",
    "cloud_pixels_fraction = (ncloud_pixels/npixels_per_slice)\n",
    "clear_specified_passes = specified_passes.water.isel(time=cloud_pixels_fraction<0.3)\n",
    "\n",
    "#Now take the clear passes and make a pandas dataframe that lists time of clear passes and corresponding gauge value\n",
    "clear_specified_passes_pd = clear_specified_passes.time.to_dataframe()\n",
    "clear_specified_passes_pd = clear_specified_passes_pd.rename(columns = {'time': 'date'})#can't have 2 columns called time\n",
    "merged_data_pd = merged_data.to_dataframe()\n",
    "\n",
    "#Merge clear satellite passes with gauge data by the time dimension\n",
    "clear_merged_data = pd.merge(clear_specified_passes_pd, merged_data_pd, left_on= 'time', \n",
    "                            right_index=True, how='inner')\n",
    "clear_merged_data = clear_merged_data.drop(columns='date')\n",
    "clear_merged_data = clear_merged_data.drop(columns='Timestamp')\n",
    "\n",
    "\n",
    "#Plot the clear satellite passes over the hydrograph\n",
    "ax = clear_merged_data.plot(marker = 'o', color='red', linestyle='None', figsize=(18,10))\n",
    "plt.ylabel('cubic meters per second (daily)')\n",
    "gauge_data.plot(ax=ax, color='blue')\n",
    "plt.axhline(yaxis_lower_parameter, color='black') #putting a black line to show the chosen threshold\n",
    "ax.legend([\"cloud free satellite passes\", \"gauge_data\"]);\n",
    "\n",
    "#Check how many passes you are about to load. I recommend loading 100 to 400 passes.\n",
    "print(\"You have this many clear passes passes:{}\".format(clear_specified_passes.time.shape[0]))\n",
    "\n",
    "print(\"Here is the hydrograph displaying the chosen threshold and days for which there were clear satellite passes above this threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now separate the gauge data into 2 lists: rising and falling. Check the graph in the output to see how well the data was separated\n",
    "### User selects what the multiplier and procedent should be\n",
    "Splitting the data into rising and falling is quite hard. One option would be to use a numpy peak picking function and identify the passes on each side of the peak. This would require someone who can code numpy much better than I can. Instead I have used a method similar to the one MDBA has used previously for comparing rising to falling flows. This box will check whether the gauge-reading 21 days after the satellite pass was higher or lower than the day of the satellite pass. The multiplier represents by how much more the water should be lower or higher to be considered a significant change. I've just left it as 1 for simplicity. It puts the pass either into the rising or falling list accordingly. It runs a loop to do this for every single pass. The output will tell you how many passes you got in each list and show you how the passes were catagorised on a hydrogaph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET PARAMETERS\n",
    "#The multiplier specifies by how much more the flow rate has to rise by to be considered a significant rise\n",
    "multiplier = 1\n",
    "\n",
    "#The procedent is how many days in advance the algorithm checks for a rise or fall. \n",
    "#one day is not enough beacuse the data is too noisy to just do 1 day. 21 days is suggested. \n",
    "days_ahead = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MAKE THE TWO LISTS FROM THE GAUGE DATA\n",
    "#extract the flow rate values from the gauge data for the loop\n",
    "gauge_data_values = gauge_data['Value']\n",
    "gauge_data_values = gauge_data_values.where(gauge_data_values > yaxis_lower_parameter)\n",
    "gauge_data_values = gauge_data_values.dropna()\n",
    "\n",
    "i = 0\n",
    "rising_list = []\n",
    "falling_list = []\n",
    "for value in gauge_data_values[:len(gauge_data_values)-days_ahead]:\n",
    "    if value < gauge_data_values[i+days_ahead]*multiplier:\n",
    "        rising_list.append(gauge_data_values.index[i])\n",
    "    else:\n",
    "        falling_list.append(gauge_data_values.index[i])\n",
    "    i = i + 1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# CATAGORISE THE PASSES ACCORDING TO THE RISING AND FALLING LISTS\n",
    "# Get rid of the timestamp in the pass dates\n",
    "clear_merged_data.index = pd.DatetimeIndex(clear_merged_data.index.date)\n",
    "clear_merged_data.index.name = 'date'\n",
    "pass_list = clear_merged_data.index\n",
    "pass_list = pass_list.tolist()\n",
    "# Get rid of the timestamp in the rising and falling lists\n",
    "ii = 0\n",
    "rising_list_dates = []\n",
    "for time in rising_list:\n",
    "    rising_list_dates.append(rising_list[ii].replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "    ii = ii + 1\n",
    "ii = 0\n",
    "falling_list_dates = []\n",
    "for time in falling_list:\n",
    "    falling_list_dates.append(falling_list[ii].replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "    ii = ii + 1\n",
    "#match the date lists up to the satellite passes\n",
    "rising_passes1 = set(rising_list_dates) & set(pass_list)\n",
    "rising_passes = list(rising_passes1)\n",
    "falling_passes1 = set(falling_list_dates) & set(pass_list)\n",
    "falling_passes = list(falling_passes1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT BACK TO PANDAS DATAFRAME SO YOU CAN MAKE THE GRAPH:\n",
    "# get rid of the timestamp on the date index so it can matched to the rising and falling dataframes later\n",
    "clear_merged_data.index = pd.DatetimeIndex(clear_merged_data.index.date)\n",
    "clear_merged_data.index.name = 'date'\n",
    "\n",
    "# Change the list of rising passes into a pandas dataframe with the date as the index\n",
    "rising_passes_df = pd.DataFrame(rising_passes)\n",
    "rising_passes_df.columns = ['date']\n",
    "rising_passes_df['date'] = pd.to_datetime(rising_passes_df['date'])\n",
    "rising_passes_df = rising_passes_df.set_index(['date'])\n",
    "# Extract the values column from the merged data\n",
    "rising_passes_df = rising_passes_df.join(clear_merged_data)\n",
    "\n",
    "# And do the same for falling\n",
    "falling_passes_df = pd.DataFrame(falling_passes)\n",
    "falling_passes_df.columns = ['date']\n",
    "falling_passes_df['date'] = pd.to_datetime(falling_passes_df['date'])\n",
    "falling_passes_df = falling_passes_df.set_index(['date'])\n",
    "# Extract the values column from the merged data\n",
    "falling_passes_df = falling_passes_df.join(clear_merged_data)\n",
    "\n",
    "# Plot the rising and falling passes over the hydrograph\n",
    "ax = falling_passes_df.plot(marker = 'o', color='red', linestyle='None', figsize=(25,9))\n",
    "plt.ylabel('cubic meters per second (daily)')\n",
    "rising_passes_df.plot(ax=ax, marker = 'o', color = 'blue', linestyle='None')\n",
    "gauge_data.plot(ax=ax, color='blue')\n",
    "plt.axhline(yaxis_lower_parameter, color='black') #putting a black line to show the chosen threshold\n",
    "ax.legend([\"Falling\", \"Rising\"]);\n",
    "\n",
    "print(\"number of rising passes(blue): {}\".format(len(rising_passes)))\n",
    "print(\"number of falling passes(red): {}\".format(len(falling_passes)))\n",
    "print(\"Here's a graph showing the passes that were characterised as rising or falling. Do you think the data was spilt accurately?\")\n",
    "print(\"If not, try changing the days_ahead variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate an image of the rising passes and a second one of the falling passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make an image that represents the frequency of water for each list. The rising image will take all the satellite images from the rising list and layer them to create a summary image of what the river looked like on days that we specified that the water level was rising, and same same for the falling list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rising_picture = clear_specified_passes.reindex(time=rising_passes,\n",
    "                                                        method='nearest',tolerance=np.timedelta64(1,'D'))\n",
    "falling_picture = clear_specified_passes.reindex(time=falling_passes,\n",
    "                                                        method='nearest',tolerance=np.timedelta64(1,'D'))\n",
    "\n",
    "print(\"number of rising passes: {}\".format(len(rising_passes)))\n",
    "print(\"number of falling passes: {}\".format(len(falling_passes)))\n",
    "\n",
    "wet_rising = (rising_picture == 128).sum(dim='time')\n",
    "dry_rising = (rising_picture == 0).sum(dim='time')\n",
    "clear_rising = wet_rising + dry_rising\n",
    "frequency_rising = wet_rising / clear_rising\n",
    "frequency_rising = frequency_rising.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "frequency_rising = frequency_rising.where(frequency_rising!=0) #This is to tell it to make areas that were dry 100% of the time white\n",
    "\n",
    "frequency_rising.plot(figsize = (16, 12))\n",
    "plt.axis('off')\n",
    "plt.title('rising')\n",
    "\n",
    "wet_falling = (falling_picture == 128).sum(dim='time')\n",
    "dry_falling = (falling_picture == 0).sum(dim='time')\n",
    "clear_falling = wet_falling + dry_falling\n",
    "frequency_falling = wet_falling / clear_falling\n",
    "frequency_falling = frequency_falling.fillna(0) #this is to get rid of the NAs that occur due to mountain shadows\n",
    "frequency_falling = frequency_falling.where(frequency_falling!=0) #This is to tell it to make areas that were dry 100% of the time white\n",
    "\n",
    "frequency_falling.plot(figsize = (16, 12))\n",
    "plt.axis('off')\n",
    "plt.title('falling')\n",
    "\n",
    "# Let's make a delta image\n",
    "#delta = frequency_rising - frequency_falling\n",
    "\n",
    "# Plotting the image\n",
    "#delta.plot(figsize = (16, 12), vmin = -0.2, vmax = 0.2, cmap = 'RdYlBu_r')\n",
    "#plt.axis('off')\n",
    "#plt.title(\"After minus before: Blue means there was more water there in the falling phase\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
