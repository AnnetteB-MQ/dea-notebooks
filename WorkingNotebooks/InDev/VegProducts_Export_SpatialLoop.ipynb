{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads and/or generates vegetation related products (NDVI, FC, TC, WofS) for a specific area\n",
    "and exports them as jpegs for further analysis/viewing in ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import warnings\n",
    "import datacube\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.storage import masking\n",
    "import calendar\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datacube.helpers import write_geotiff\n",
    "\n",
    "\n",
    "# Load custom DEA notebook functions\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "import dea_datahandling\n",
    "import dea_plotting\n",
    "import DEADataHandling\n",
    "from dea_bandindices import calculate_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the data cube. 'app' argument is used to identify this app. It does not influence the analysis.\n",
    "Note Fractional Cover is not in the DEA Collection 3 yet so will for the time being be loaded using from\n",
    "Collection 2 using old functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dc_landsat3 = datacube.Datacube(app='VegAnalysis-WD', env='c3-samples')\n",
    "except:\n",
    "    dc_landsat3 = datacube.Datacube(app='VegAnalysis-WD')\n",
    "\n",
    "    \n",
    "dc_landsat2 = datacube.Datacube(app='VegAnalysis-WD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spatial and temporal query. This is used for both collections (Collection 3 - query_3; Collection 2 - query_2). \n",
    "If running this notebook locally, use the smaller spatial extent and subset of the time series. If running on gadi, the larger extent covers the full Western Davenport study area and the full time-series should be used. \n",
    "Note, the Fractional Cover products (defined in query_2) are from Collection 2 and only go up to 2018. This can be updated once FC is added to Collection 3. FC and Wofs also start from 1987 rather than 1984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = {\n",
    "#        'lon': (134.24, 134.34),             # small test area\n",
    "#        'lat': (-20.79, -20.86),             # small test area\n",
    "        'lon': (132.07, 135.36),             # full study area\n",
    "        'lat': (-20.31, -22.11),             # full study area\n",
    "        'time':('2015-01', '2018-12'),       # subset of time-series\n",
    "#        'time':('1984-03', '2018-12'),       # full time-series\n",
    "        'output_crs': 'EPSG:28352',\n",
    "         'resolution': (30, 30),\n",
    "         'group_by': 'solar_day'\n",
    "}\n",
    "\n",
    "query_2 = {\n",
    "        'lon': (134.24, 134.34),             # small test area\n",
    "        'lat': (-20.79, -20.86),             # small test area\n",
    "#        'lon': (132.07, 135.36),             # full study area\n",
    "#        'lat': (-20.31, -22.11),             # full study area\n",
    "        'time':('2015-01', '2018-12'),       # subset of time-series\n",
    "#        'time':('1987-01', '2018-12'),       # full time-series\n",
    "        'output_crs': 'EPSG:3577',\n",
    "        'resolution': (25, 25),\n",
    "        'group_by': 'solar_day'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the months (0-11) that represent the dry season for the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_months = [5,6,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Red and NIR Landsat data from Collection 3 using `.load_ard`. This bands will be converted to NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ga_ls5t_ard_3 data\n",
      "    No data for ga_ls5t_ard_3\n",
      "Loading ga_ls7e_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Loading ga_ls8c_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Combining and sorting data\n",
      "    Returning 65 observations \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:       (time: 65, x: 357, y: 272)\n",
       "Coordinates:\n",
       "  * x             (x) float64 1.046e+06 1.046e+06 ... 1.056e+06 1.056e+06\n",
       "  * y             (y) float64 7.684e+06 7.684e+06 ... 7.692e+06 7.692e+06\n",
       "  * time          (time) datetime64[ns] 2015-01-07T01:00:31.257836 ... 2015-12-25T01:00:36.397268\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_nir     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_green   (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_blue    (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_1  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_2  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    crs:      EPSG:28352</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (time: 65, x: 357, y: 272)\n",
       "Coordinates:\n",
       "  * x             (x) float64 1.046e+06 1.046e+06 ... 1.056e+06 1.056e+06\n",
       "  * y             (y) float64 7.684e+06 7.684e+06 ... 7.692e+06 7.692e+06\n",
       "  * time          (time) datetime64[ns] 2015-01-07T01:00:31.257836 ... 2015-12-25T01:00:36.397268\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_nir     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_green   (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_blue    (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_1  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_2  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    crs:      EPSG:28352"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = dea_datahandling.load_ard(dc=dc_landsat3,\n",
    "        products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "        measurements=['nbart_red','nbart_nir','nbart_green',\n",
    "                      'nbart_blue','nbart_swir_1','nbart_swir_2'],\n",
    "        mask_contiguity='nbart_contiguity',\n",
    "        **query_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calaculate NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate various indices using calculate_indices function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:       (time: 65, x: 357, y: 272)\n",
       "Coordinates:\n",
       "  * x             (x) float64 1.046e+06 1.046e+06 ... 1.056e+06 1.056e+06\n",
       "  * y             (y) float64 7.684e+06 7.684e+06 ... 7.692e+06 7.692e+06\n",
       "  * time          (time) datetime64[ns] 2015-01-07T01:00:31.257836 ... 2015-12-25T01:00:36.397268\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_nir     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_green   (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_blue    (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_1  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_2  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    TCW           (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    TCB           (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    TCG           (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    crs:      EPSG:28352</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (time: 65, x: 357, y: 272)\n",
       "Coordinates:\n",
       "  * x             (x) float64 1.046e+06 1.046e+06 ... 1.056e+06 1.056e+06\n",
       "  * y             (y) float64 7.684e+06 7.684e+06 ... 7.692e+06 7.692e+06\n",
       "  * time          (time) datetime64[ns] 2015-01-07T01:00:31.257836 ... 2015-12-25T01:00:36.397268\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_nir     (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_green   (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_blue    (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_1  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    nbart_swir_2  (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    TCW           (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    TCB           (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "    TCG           (time, y, x) float32 nan nan nan nan nan ... nan nan nan nan\n",
       "Attributes:\n",
       "    crs:      EPSG:28352"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tasselled Cap Wetness\n",
    "calculate_indices(ds,index = 'TCW', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Tasselled Cap Brightness\n",
    "calculate_indices(ds,index = 'TCB', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Tasselled Cap Greenness\n",
    "calculate_indices(ds,index = 'TCG', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Normalised Difference Vegetation Index\n",
    "calculate_indices(ds,index = 'NDVI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Leaf Area Index\n",
    "calculate_indices(ds,index = 'LAI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load fractional cover data and wofs using `dc.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fc = dc_landsat2.load(product='ls8_fc_albers', **query_2)\n",
    "\n",
    "dataset_wofs = dc_landsat2.load(product='wofs_albers', like=dataset_fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wofs is then used to mask out standing water from Fractional Cover. The resulting FC data is then converted to FC out of 1 (rather than 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the data\n",
    "shared_times = np.intersect1d(dataset_fc.time, dataset_wofs.time)\n",
    "\n",
    "ds_fc_matched = dataset_fc.sel(time=shared_times)\n",
    "ds_wofs_matched = dataset_wofs.sel(time=shared_times)\n",
    "\n",
    "# Mask FC\n",
    "dry_mask = masking.make_mask(ds_wofs_matched, dry=True)\n",
    "\n",
    "# Get fractional masked fc dataset (as proportion of 1, rather than 100)\n",
    "ds_fc_masked = ds_fc_matched.where(dry_mask.water == True) / 100\n",
    "\n",
    "# Resample\n",
    "ds_resampled = ds_fc_masked.resample(time=\"1M\").median()\n",
    "ds_resampled.attrs[\"crs\"] = dataset_fc.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate water frequency (percentage of wet observations) from WofS time series.\n",
    "The box below will load the selected WOfS images with `.compute()` and then cloud filter the images, meaning it will take out images that had too much cloud to see anything. \n",
    "It does this by using the `.make_mask()` function to calculate the fraction of cloud pixels in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify available WofS time-steps\n",
    "date_list = dataset_wofs.time.values\n",
    "time_steps = dataset_wofs.sel(time=date_list).compute()\n",
    "\n",
    "# Calculate the number of cloudy pixels per timestep\n",
    "cc = masking.make_mask(time_steps.water, cloud=True)\n",
    "ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "\n",
    "# Calculate the total number of pixels per timestep\n",
    "npixels_per_slice = (time_steps.water.shape[1] * \n",
    "                     time_steps.water.shape[2])\n",
    "\n",
    "# Calculate the proportion of cloudy pixels\n",
    "cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "\n",
    "# Filter out \"too cloudy\" passes (i.e. more than 50% cloud)\n",
    "clear_time_steps = time_steps.water.isel(\n",
    "    time=cloud_pixels_fraction < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all wet and dry pixels\n",
    "wet = masking.make_mask(clear_time_steps, wet=True).sum(dim='time')\n",
    "dry = masking.make_mask(clear_time_steps, dry=True).sum(dim='time')\n",
    "\n",
    "# Calculate how frequently each pixel was wet when it was observed\n",
    "clear = wet + dry\n",
    "frequency = wet / clear\n",
    "\n",
    "# Remove persistent NAs that occur due to mountain shadows\n",
    "frequency = frequency.fillna(0)  \n",
    "\n",
    "# Set pixels that remain dry 100% of the time to nodata so they appear white\n",
    "frequency = frequency.where(frequency != 0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate standard deviations and medians for the the dry season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_TCW_dry = ds.TCW['time.month'].isin(dry_months)\n",
    "median_TCW_dry = ds.TCW.groupby('time.month').median(dim = 'time')\n",
    "median_TCW_dry = median_TCW_dry.median(dim = 'month')\n",
    "\n",
    "median_TCB_dry = ds.TCB['time.month'].isin(dry_months)\n",
    "median_TCB_dry = ds.TCB.groupby('time.month').median(dim = 'time')\n",
    "median_TCB_dry = median_TCB_dry.median(dim = 'month')\n",
    "\n",
    "median_TCG_dry = ds.TCG['time.month'].isin(dry_months)\n",
    "median_TCG_dry = ds.TCG.groupby('time.month').median(dim = 'time')\n",
    "median_TCG_dry = median_TCG_dry.median(dim = 'month')\n",
    "\n",
    "ndvi = ds.NDVI\n",
    "median_ndvi = ndvi.groupby('time.month').median(dim = 'time')\n",
    "median_ndvi = median_ndvi.median(dim = 'month')\n",
    "\n",
    "std_ndvi = ndvi.groupby('time.month').std(dim = 'time')\n",
    "std_ndvi = std_ndvi.std(dim = 'month')\n",
    "\n",
    "std_ndvi_dry = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "std_ndvi_dry = std_ndvi_dry.groupby('time.month').std(dim = 'time')\n",
    "std_ndvi_dry = std_ndvi_dry.std(dim = 'month')\n",
    "\n",
    "std_ndvi_diff1 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 0)\n",
    "std_ndvi_diff2 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 7)\n",
    "std_ndvi_diff = std_ndvi_diff1 - std_ndvi_diff2\n",
    "\n",
    "median_ndvi_dry = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "median_ndvi_dry = median_ndvi_dry.groupby('time.month').median(dim = 'time')\n",
    "median_ndvi_dry = median_ndvi_dry.median(dim = 'month')\n",
    "\n",
    "median_LAI_dry = ds.LAI['time.month'].isin(dry_months)\n",
    "median_LAI_dry = ds.LAI.groupby('time.month').median(dim = 'time')\n",
    "median_LAI_dry = median_LAI_dry.median(dim = 'month')\n",
    "\n",
    "BS_dry = ds_resampled.BS[ds_resampled.BS['time.month'].isin(dry_months)]\n",
    "BS_dry = BS_dry.groupby('time.month').median(dim = 'time')\n",
    "BS_dry = BS_dry.median(dim = 'month')\n",
    "\n",
    "PV_dry = ds_resampled.PV[ds_resampled.PV['time.month'].isin(dry_months)]\n",
    "PV_dry = PV_dry.groupby('time.month').median(dim = 'time')\n",
    "PV_dry = PV_dry.median(dim = 'month')\n",
    "\n",
    "NPV_dry = ds_resampled.NPV[ds_resampled.NPV['time.month'].isin(dry_months)]\n",
    "NPV_dry = NPV_dry.groupby('time.month').median(dim = 'time')\n",
    "NPV_dry = NPV_dry.median(dim = 'month')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds).\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variable for path to save files\n",
    "savefilepath = '/g/data/zk34/ljg547/Outputs/'\n",
    "\n",
    "# Set project naming convention. Start and end dates are reformated to remove '-'.\n",
    "Proj = 'SSC_WD_'\n",
    "\n",
    "ds_startDate = str(ds.isel(time=0).time.values)[0:10]\n",
    "ds_startDate = str(ds_startDate[0:4] + f'{int(ds_startDate[6:7]):02d}' + \n",
    "              f'{int(ds_startDate[9:10]):02d}')\n",
    "\n",
    "ds_endDate = str(ds.isel(time=-1).time.values)[0:10]\n",
    "ds_endDate = str(ds_endDate[0:4] + f'{int(ds_endDate[6:7]):02d}' + \n",
    "              f'{int(ds_endDate[9:10]):02d}')\n",
    "\n",
    "fc_startDate = str(dataset_fc.isel(time=0).time.values)[0:10]\n",
    "fc_startDate = str(fc_startDate[0:4] + f'{int(fc_startDate[6:7]):02d}' + \n",
    "              f'{int(fc_startDate[9:10]):02d}')\n",
    "\n",
    "fc_endDate = str(dataset_fc.isel(time=-1).time.values)[0:10]\n",
    "fc_endDate = str(fc_endDate[0:4] + f'{int(fc_endDate[6:7]):02d}' + \n",
    "              f'{int(fc_endDate[9:10]):02d}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. Note some products (e.g. FC and Wofs) have different time-series period as they are imported from Collection 2 rather than Collection 3 (current collection of Landsat data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_TCW_dry.to_dataset(name='median_TCW_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset = arr, filename = fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Wetness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCW_dry_median is the median value of TCW over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_TCB_dry.to_dataset(name='median_TCB_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Brigthness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCB_dry_median is the median value of TCB over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_TCG_dry.to_dataset(name='median_TCG_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianTCG_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCG_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Greenness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCG_dry_median is the median value of TCG over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_ndvi.to_dataset(name='median_ndvi')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Median NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi.to_dataset(name='std_ndvi')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi_dry.to_dataset(name='std_ndvi_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for the dry season(\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi_diff.to_dataset(name='std_ndvi_diff')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Comparison between NDVI standard deviation during the wet season (January) ad at the end of the dry season (August).\" + \"\\n\" \n",
    "      \"Time series includes imagery from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Where vegetation is accessing more reliable water sources (e.g. groundwater), residual standard deviation is low.\" + \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_ndvi_dry.to_dataset(name='median_ndvi_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'mediandNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"NDVI of dry period (\" + str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "      \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"NDVI_dry_median is the median value of NDVI over the dry months.\"+ \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_LAI_dry.to_dataset(name='median_LAI_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'medianLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Leaf Area Index for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"LAI_dry_median is the median value of LAI over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = BS_dry.to_dataset(name='BS_dry')\n",
    "arr.attrs = dataset_fc.attrs\n",
    "fname = str(savefilepath + Proj + 'medianBS_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'medianBS_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Fraction of Bare Soil (as compared to photo-synthetic vegetation and non-photosynthetic vegetation) for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"The BS value was calculated for the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = PV_dry.to_dataset(name='PV_dry')\n",
    "arr.attrs = dataset_fc.attrs\n",
    "fname = str(savefilepath + Proj + 'medianPV_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'medianPV_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Fraction of photosynthetic vegetation (as compared to base soil and non-photosynthetic vegetation) for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"The PV value was calculated for the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = NPV_dry.to_dataset(name='NPV_dry')\n",
    "arr.attrs = dataset_fc.attrs\n",
    "fname = str(savefilepath + Proj + 'medianNPB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'medianNPV_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Fraction of non-photosynthetic vegetation (as compared to photo-synthetic vegetation and bare soil) for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"The NPV value was calculated for the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = frequency.to_dataset(name='WofS_WetFrequency')\n",
    "arr.attrs = dataset_wofs.attrs\n",
    "fname = str(savefilepath + Proj + 'WofS_WetFrequency_' +\n",
    "              fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'WofS_WetFrequency_' +\n",
    "              fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"The frequence of wet, as calculated for each pixel using the Water Observations from Space algorithm.\" + \n",
    "        \"Time series: \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
