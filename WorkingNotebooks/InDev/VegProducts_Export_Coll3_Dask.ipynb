{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads and/or generates vegetation related products (NDVI, FC, TC, WofS) for a specific area\n",
    "and exports them as jpegs for further analysis/viewing in ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import dask\n",
    "from dask.utils import parse_bytes\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils.dask import start_local_dask\n",
    "\n",
    "from psutil import virtual_memory, cpu_count\n",
    "\n",
    "# Load custom DEA notebook functions\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "import dea_datahandling\n",
    "import dea_plotting\n",
    "import DEADataHandling\n",
    "from dea_bandindices import calculate_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a dask cluster\n",
    "\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell.\n",
    "\n",
    "The parameters for generating the local dask cluster are automatically generated, but if you wish to alter them use the documentation here - https://distributed.dask.org/en/stable/local-cluster.html. Put simply, the code below identifies how many cpus and how much RAM the computer has and generates a local cluster using those variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33879</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:34800/status' target='_blank'>http://127.0.0.1:34800/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>30.67 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:33879' processes=1 threads=8, memory=30.67 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "# Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "cpu_limit = cpu_count()\n",
    "cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "\n",
    "mem_limit = virtual_memory()\n",
    "mem_limit = mem_limit.total\n",
    "mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# leave 3Gb for notebook itself\n",
    "mem_limit -= parse_bytes('3Gb')\n",
    "\n",
    "# close previous client if any, so we can re-run this cell without issues\n",
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "\n",
    "# start up a local cluster\n",
    "client = start_local_dask(n_workers=1,\n",
    "                          threads_per_worker=cpu_limit,\n",
    "                          memory_limit=mem_limit)\n",
    "\n",
    "# show the dask cluster settings\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the data cube. 'app' argument is used to identify this app. It does not influence the analysis.\n",
    "Note Fractional Cover is not in the DEA Collection 3 yet so will for the time being be loaded using from\n",
    "Collection 2 using old functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dc_landsat3 = datacube.Datacube(app='VegAnalysis-WD', env='c3-samples')\n",
    "except:\n",
    "    dc_landsat3 = datacube.Datacube(app='VegAnalysis-WD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spatial and temporal query. This is used for both collections (Collection 3 - query_3; Collection 2 - query_2). \n",
    "If running this notebook locally, use the smaller spatial extent and subset of the time series. If running on gadi, the larger extent covers the full Western Davenport study area and the full time-series should be used. \n",
    "Note, the Fractional Cover products (defined in query_2) are from Collection 2 and only go up to 2018. This can be updated once FC is added to Collection 3. FC and Wofs also start from 1987 rather than 1984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = {\n",
    "#        'lon': (134.24, 134.34),             # small test area - this works via vdi\n",
    "#        'lat': (-20.79, -20.86),             # small test area - this works via vdi\n",
    "        'lon': (132.07, 132.50),             # small test area - current test extent on vdi\n",
    "        'lat': (-20.31, -21.00),             # small test area - current test extent on vdi\n",
    "#        'lon': (132.07, 135.36),             # full study area\n",
    "#        'lat': (-20.31, -22.11),             # full study area\n",
    "#        'time':('2015-01', '2018-12'),       # subset of time-series\n",
    "        'time':('1987-01', '2018-12'),       # full time-series\n",
    "        'output_crs': 'EPSG:28352',\n",
    "         'resolution': (30, 30),\n",
    "         'group_by': 'solar_day'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the months (0-11) that represent the dry season for the area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_months = [5,6,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Red and NIR Landsat data from Collection 3 using `.load_ard`. This bands will be converted to NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ga_ls5t_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Loading ga_ls7e_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Loading ga_ls8c_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Combining and sorting data\n",
      "    Returning 1780 observations as a dask array\n"
     ]
    }
   ],
   "source": [
    "ds = dea_datahandling.load_ard(dc=dc_landsat3,\n",
    "        mask_dtype = np.float16,\n",
    "        products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "        measurements=['nbart_red','nbart_nir','nbart_green',\n",
    "                      'nbart_blue','nbart_swir_1','nbart_swir_2'],\n",
    "        mask_contiguity='nbart_contiguity',\n",
    "        dask_chunks = {'x':500, 'y':500},\n",
    "        **query_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate various indices using calculate_indices function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:       (time: 1780, x: 1548, y: 2580)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x             (x) float64 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05\n",
       "  * time          (time) datetime64[ns] 1987-05-25T00:30:43.112484 ... 2018-12-31T01:12:42.239841\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    nbart_nir     (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    nbart_green   (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    nbart_blue    (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    nbart_swir_1  (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    nbart_swir_2  (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    TCW           (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    TCB           (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    TCG           (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    NDVI          (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    LAI           (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "    NDWI          (time, y, x) float16 dask.array&lt;chunksize=(1, 500, 500), meta=np.ndarray&gt;\n",
       "Attributes:\n",
       "    crs:      EPSG:28352</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (time: 1780, x: 1548, y: 2580)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x             (x) float64 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05\n",
       "  * time          (time) datetime64[ns] 1987-05-25T00:30:43.112484 ... 2018-12-31T01:12:42.239841\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    nbart_nir     (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    nbart_green   (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    nbart_blue    (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    nbart_swir_1  (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    nbart_swir_2  (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    TCW           (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    TCB           (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    TCG           (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    NDVI          (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    LAI           (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "    NDWI          (time, y, x) float16 dask.array<chunksize=(1, 500, 500), meta=np.ndarray>\n",
       "Attributes:\n",
       "    crs:      EPSG:28352"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tasselled Cap Wetness\n",
    "calculate_indices(ds,index = 'TCW', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Tasselled Cap Brightness\n",
    "calculate_indices(ds,index = 'TCB', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Tasselled Cap Greenness\n",
    "calculate_indices(ds,index = 'TCG', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Normalised Difference Vegetation Index\n",
    "calculate_indices(ds,index = 'NDVI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Leaf Area Index\n",
    "calculate_indices(ds,index = 'LAI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Normalised Difference Wetness Index\n",
    "calculate_indices(ds,index = 'NDWI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate standard deviations and mean for the the dry season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_TCW_dry = ds.TCW['time.month'].isin(dry_months)\n",
    "mean_TCW_dry = ds.TCW.groupby('time.month').mean(dim = 'time')\n",
    "mean_TCW_dry = mean_TCW_dry.mean(dim = 'month')\n",
    "\n",
    "mean_TCB_dry = ds.TCB['time.month'].isin(dry_months)\n",
    "mean_TCB_dry = ds.TCB.groupby('time.month').mean(dim = 'time')\n",
    "mean_TCB_dry = mean_TCB_dry.mean(dim = 'month')\n",
    "\n",
    "mean_TCG_dry = ds.TCG['time.month'].isin(dry_months)\n",
    "mean_TCG_dry = ds.TCG.groupby('time.month').mean(dim = 'time')\n",
    "mean_TCG_dry = mean_TCG_dry.mean(dim = 'month')\n",
    "\n",
    "ndvi = ds.NDVI\n",
    "mean_ndvi = ndvi.groupby('time.month').mean(dim = 'time')\n",
    "mean_ndvi = mean_ndvi.mean(dim = 'month')\n",
    "\n",
    "std_ndvi = ndvi.groupby('time.month').std(dim = 'time')\n",
    "std_ndvi = std_ndvi.std(dim = 'month')\n",
    "\n",
    "std_ndvi_dry = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "std_ndvi_dry = std_ndvi_dry.groupby('time.month').std(dim = 'time')\n",
    "std_ndvi_dry = std_ndvi_dry.std(dim = 'month')\n",
    "\n",
    "std_ndvi_diff1 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 0)\n",
    "std_ndvi_diff2 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 7)\n",
    "std_ndvi_diff = std_ndvi_diff1 - std_ndvi_diff2\n",
    "\n",
    "mean_ndvi_dry = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "mean_ndvi_dry = mean_ndvi_dry.groupby('time.month').mean(dim = 'time')\n",
    "mean_ndvi_dry = mean_ndvi_dry.mean(dim = 'month')\n",
    "\n",
    "mean_LAI_dry = ds.LAI['time.month'].isin(dry_months)\n",
    "mean_LAI_dry = ds.LAI.groupby('time.month').mean(dim = 'time')\n",
    "mean_LAI_dry = mean_LAI_dry.mean(dim = 'month')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds).\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variable for path to save files\n",
    "savefilepath = '/g/data/zk34/ljg547/Outputs/'\n",
    "\n",
    "# Set project naming convention. Start and end dates are reformated to remove '-'.\n",
    "Proj = 'SSC_WD_'\n",
    "\n",
    "ds_startDate = str(ds.isel(time=0).time.values)[0:10]\n",
    "ds_startDate = str(ds_startDate[0:4] + f'{int(ds_startDate[6:7]):02d}' + \n",
    "              f'{int(ds_startDate[9:10]):02d}')\n",
    "\n",
    "ds_endDate = str(ds.isel(time=-1).time.values)[0:10]\n",
    "ds_endDate = str(ds_endDate[0:4] + f'{int(ds_endDate[6:7]):02d}' + \n",
    "              f'{int(ds_endDate[9:10]):02d}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;TCW&#x27; (y: 2580, x: 1548)&gt;\n",
       "dask.array&lt;mean_agg-aggregate, shape=(2580, 1548), dtype=float16, chunksize=(500, 500), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * y        (y) float64 7.674e+06 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x        (x) float64 8.192e+05 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'TCW' (y: 2580, x: 1548)>\n",
       "dask.array<mean_agg-aggregate, shape=(2580, 1548), dtype=float16, chunksize=(500, 500), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * y        (y) float64 7.674e+06 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x        (x) float64 8.192e+05 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_TCW_dry = ds.TCW['time.month'].isin(dry_months)\n",
    "mean_TCW_dry = ds.TCW.groupby('time.month').mean(dim = 'time')\n",
    "mean_TCW_dry = mean_TCW_dry.mean(dim = 'month')\n",
    "\n",
    "mean_TCW_dry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;TCW&#x27; (y: 2580, x: 1548)&gt;\n",
       "dask.array&lt;astype, shape=(2580, 1548), dtype=float32, chunksize=(500, 500), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * y        (y) float64 7.674e+06 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x        (x) float64 8.192e+05 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'TCW' (y: 2580, x: 1548)>\n",
       "dask.array<astype, shape=(2580, 1548), dtype=float32, chunksize=(500, 500), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * y        (y) float64 7.674e+06 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x        (x) float64 8.192e+05 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = mean_TCW_dry.astype(dtype='float32')\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:       (x: 1548, y: 2580)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x             (x) float64 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05\n",
       "Data variables:\n",
       "    mean_TCW_dry  (y, x) float32 dask.array&lt;chunksize=(500, 500), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (x: 1548, y: 2580)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x             (x) float64 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05\n",
       "Data variables:\n",
       "    mean_TCW_dry  (y, x) float32 dask.array<chunksize=(500, 500), meta=np.ndarray>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = arr.to_dataset(name='mean_TCW_dry')\n",
    "arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:       (x: 1548, y: 2580)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x             (x) float64 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05\n",
       "Data variables:\n",
       "    mean_TCW_dry  (y, x) float32 dask.array&lt;chunksize=(500, 500), meta=np.ndarray&gt;\n",
       "Attributes:\n",
       "    crs:      EPSG:28352</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (x: 1548, y: 2580)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.674e+06 7.674e+06 ... 7.751e+06 7.751e+06\n",
       "  * x             (x) float64 8.192e+05 8.192e+05 ... 8.655e+05 8.656e+05\n",
       "Data variables:\n",
       "    mean_TCW_dry  (y, x) float32 dask.array<chunksize=(500, 500), meta=np.ndarray>\n",
       "Attributes:\n",
       "    crs:      EPSG:28352"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.attrs = ds.attrs\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = str(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 14% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 14% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 12% CPU time recently (threshold: 10%)\n",
      "distributed.utils_perf - WARNING - full garbage collections took 13% CPU time recently (threshold: 10%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-b5885354afe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_geotiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20191127/lib/python3.6/site-packages/datacube/helpers.py\u001b[0m in \u001b[0;36mwrite_geotiff\u001b[0;34m(filename, dataset, profile_override, time_index)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_vars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbandnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mdest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbandnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetWriterBase.write\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/core.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"getitem-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m         \u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHighLevelGraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mslice_array\u001b[0;34m(out_name, in_name, blockdims, index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Pass down to next function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0mdsk_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_with_newaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mbd_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbd_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mslice_with_newaxes\u001b[0;34m(out_name, in_name, blockdims, index)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# Pass down and do work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdims2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_wrap_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwhere_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mslice_wrap_lists\u001b[0;34m(out_name, in_name, blockdims, index)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# No lists, hooray! just use slice_slices_and_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwhere_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mslice_slices_and_integers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Replace all lists with full slices  [3, 1, 0] -> slice(None, None, None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mslice_slices_and_integers\u001b[0;34m(out_name, in_name, blockdims, index)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;31m# Get a list (for each dimension) of dicts{blocknum: slice()}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mblock_slices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_slice_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblockdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0msorted_block_slices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblock_slices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36m_slice_1d\u001b[0;34m(dim_shape, lengths, index)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \"\"\"\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mchunk_boundaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_cumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/dask/array/slicing.py\u001b[0m in \u001b[0;36mcached_cumsum\u001b[0;34m(seq, initial_zero)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;31m# Look up by identity first, to avoid a linear-time __hash__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;31m# if we've seen this tuple object before.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_HashIdWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;31m# Construct a temporary tuple, and look up by value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)\n"
     ]
    }
   ],
   "source": [
    "write_geotiff(dataset = arr, filename = fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "invalid dtype: 'float16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6a71aa3c9978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdry_months\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'to'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdry_months\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               '_' + ds_startDate + '_' + ds_endDate + '.tif')\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwrite_geotiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create metadata file. w - writes, r - reads, a- appends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20191127/lib/python3.6/site-packages/datacube/helpers.py\u001b[0m in \u001b[0;36mwrite_geotiff\u001b[0;34m(filename, dataset, profile_override, time_index)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0m_calculate_blocksize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_vars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbandnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid driver: {0!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid dtype: {0!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnodata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mnodata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: invalid dtype: 'float16'"
     ]
    }
   ],
   "source": [
    "# Export data\n",
    "#arr = mean_TCW_dry.to_dataset(name='mean_TCW_dry')\n",
    "#arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset = arr, filename = fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Wetness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCW_dry_mean is the mean value of TCW over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = mean_TCB_dry.to_dataset(name='mean_TCB_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'meanTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Brigthness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCB_dry_mean is the mean value of TCB over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = mean_TCG_dry.to_dataset(name='mean_TCG_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'meanTCG_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCG_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Greenness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCG_dry_mean is the mean value of TCG over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = mean_ndvi.to_dataset(name='mean_ndvi')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'meanNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Mean NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi.to_dataset(name='std_ndvi')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi_dry.to_dataset(name='std_ndvi_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for the dry season(\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi_diff.to_dataset(name='std_ndvi_diff')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Comparison between NDVI standard deviation during the wet season (January) ad at the end of the dry season (August).\" + \"\\n\" \n",
    "      \"Time series includes imagery from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Where vegetation is accessing more reliable water sources (e.g. groundwater), residual standard deviation is low.\" + \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = mean_ndvi_dry.to_dataset(name='mean_ndvi_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'meanNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"NDVI of dry period (\" + str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "      \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"NDVI_dry_mean is the mean value of NDVI over the dry months.\"+ \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = mean_LAI_dry.to_dataset(name='mean_LAI_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'meanLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Leaf Area Index for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"LAI_dry_mean is the mean value of LAI over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
