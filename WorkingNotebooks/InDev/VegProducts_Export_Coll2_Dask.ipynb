{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads and/or generates vegetation related products (NDVI, FC, TC, WofS) for a specific area\n",
    "and exports them as jpegs for further analysis/viewing in ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import dask\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils.dask import start_local_dask\n",
    "\n",
    "# Load custom DEA notebook functions\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "import dea_datahandling\n",
    "import dea_plotting\n",
    "import DEADataHandling\n",
    "from dea_bandindices import calculate_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a dask cluster\n",
    "\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell.\n",
    "\n",
    "The parameters for generating the local dask cluster are automatically generated, but if you wish to alter them use the documentation here - https://distributed.dask.org/en/stable/local-cluster.html. Put simply, the code below identifies how many cpus and how much RAM the computer has and generates a local cluster using those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38159</li>\n",
       "  <li><b>Dashboard: </b><a href='/proxy/8787/status' target='_blank'>/proxy/8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>5.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38159' processes=1 threads=4, memory=5.00 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configure dashboard link to go over proxy\n",
    "dask.config.set({\"distributed.dashboard.link\":\n",
    "                 os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"})\n",
    "\n",
    "# Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "mem_limit = int(os.environ.get('MEM_LIMIT', '0'))\n",
    "cpu_limit = float(os.environ.get('CPU_LIMIT', '0'))\n",
    "cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# leave 3Gb for notebook itself\n",
    "mem_limit -= parse_bytes('3Gb')\n",
    "\n",
    "# close previous client if any, so we can re-run this cell without issues\n",
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "\n",
    "# start up a local cluster\n",
    "client = start_local_dask(n_workers=1,\n",
    "                          threads_per_worker=cpu_limit,\n",
    "                          memory_limit=mem_limit)\n",
    "\n",
    "# show the dask cluster settings\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the data cube. 'app' argument is used to identify this app. It does not influence the analysis.\n",
    "Note Fractional Cover is not in the DEA Collection 3 yet so will for the time being be loaded using from\n",
    "Collection 2 using old functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_landsat2 = datacube.Datacube(app='VegAnalysis-WD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spatial and temporal query. This is used for Collection 2 data (i.e query_2). \n",
    "If running this notebook locally, use the smaller spatial extent and subset of the time series. If running on gadi, the larger extent covers the full Western Davenport study area and the full time-series should be used. \n",
    "Note, the Fractional Cover products (defined in query_2) are from Collection 2 and only go up to 2018. This can be updated once FC is added to Collection 3. FC and Wofs also start from 1987."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = {\n",
    "        'lon': (132.07, 133.00),             # small test area\n",
    "        'lat': (-20.31, -21.00),             # small test area\n",
    "#        'lon': (132.07, 135.36),             # full study area\n",
    "#        'lat': (-20.31, -22.11),             # full study area\n",
    "#        'time':('2018-12', '2018-12'),       # subset of time-series\n",
    "        'time':('1987-01', '2018-12'),       # full time-series\n",
    "        'output_crs': 'EPSG:3577',\n",
    "        'resolution': (25, 25),\n",
    "        'group_by': 'solar_day'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_months = [5,6,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the months (0-11) that represent the dry season for the area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load fractional cover data and wofs using `dc.load`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fc = dc_landsat2.load(product='ls8_fc_albers',\n",
    "              dask_chunks = {'x': 500, 'y': 500}, **query_2)\n",
    "\n",
    "dataset_wofs = dc_landsat2.load(product='wofs_albers', \n",
    "              dask_chunks = {'x': 500, 'y': 500}, like=dataset_fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wofs is then used to mask out standing water from Fractional Cover. The resulting FC data is then converted to FC out of 1 (rather than 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the data\n",
    "shared_times = np.intersect1d(dataset_fc.time, dataset_wofs.time)\n",
    "\n",
    "ds_fc_matched = dataset_fc.sel(time=shared_times)\n",
    "ds_wofs_matched = dataset_wofs.sel(time=shared_times)\n",
    "\n",
    "# Mask FC\n",
    "dry_mask = masking.make_mask(ds_wofs_matched, dry=True)\n",
    "\n",
    "# Get fractional masked fc dataset (as proportion of 1, rather than 100)\n",
    "ds_fc_masked = ds_fc_matched.where(dry_mask.water == True) / 100\n",
    "\n",
    "# Resample\n",
    "#ds_resampled = ds_fc_masked.resample(time=\"1M\").median()    # median not currently supported by dask\n",
    "ds_resampled = ds_fc_masked.resample(time=\"1M\").mean()\n",
    "ds_resampled.attrs[\"crs\"] = dataset_fc.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate water frequency (percentage of wet observations) from WofS time series.\n",
    "The box below will load the selected WOfS images with `.compute()` and then cloud filter the images, meaning it will take out images that had too much cloud to see anything. \n",
    "It does this by using the `.make_mask()` function to calculate the fraction of cloud pixels in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting\n"
     ]
    }
   ],
   "source": [
    "# Identify available WofS time-steps\n",
    "date_list = dataset_wofs.time.values\n",
    "time_steps = dataset_wofs.sel(time=date_list).compute()\n",
    "\n",
    "# Calculate the number of cloudy pixels per timestep\n",
    "cc = masking.make_mask(time_steps.water, cloud=True)\n",
    "ncloud_pixels = cc.sum(dim=['x', 'y'])\n",
    "\n",
    "# Calculate the total number of pixels per timestep\n",
    "npixels_per_slice = (time_steps.water.shape[1] * \n",
    "                     time_steps.water.shape[2])\n",
    "\n",
    "# Calculate the proportion of cloudy pixels\n",
    "cloud_pixels_fraction = (ncloud_pixels / npixels_per_slice)\n",
    "\n",
    "# Filter out \"too cloudy\" passes (i.e. more than 50% cloud)\n",
    "clear_time_steps = time_steps.water.isel(\n",
    "    time=cloud_pixels_fraction < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all wet and dry pixels\n",
    "wet = masking.make_mask(clear_time_steps, wet=True).sum(dim='time')\n",
    "dry = masking.make_mask(clear_time_steps, dry=True).sum(dim='time')\n",
    "\n",
    "# Calculate how frequently each pixel was wet when it was observed\n",
    "clear = wet + dry\n",
    "frequency = wet / clear\n",
    "\n",
    "# Remove persistent NAs that occur due to mountain shadows\n",
    "frequency = frequency.fillna(0)  \n",
    "\n",
    "# Set pixels that remain dry 100% of the time to nodata so they appear white\n",
    "frequency = frequency.where(frequency != 0)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate standard deviations and medians for the the dry season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_TCW_dry = ds.TCW['time.month'].isin(dry_months)\n",
    "median_TCW_dry = ds.TCW.groupby('time.month').median(dim = 'time')\n",
    "median_TCW_dry = median_TCW_dry.median(dim = 'month')\n",
    "\n",
    "median_TCB_dry = ds.TCB['time.month'].isin(dry_months)\n",
    "median_TCB_dry = ds.TCB.groupby('time.month').median(dim = 'time')\n",
    "median_TCB_dry = median_TCB_dry.median(dim = 'month')\n",
    "\n",
    "median_TCG_dry = ds.TCG['time.month'].isin(dry_months)\n",
    "median_TCG_dry = ds.TCG.groupby('time.month').median(dim = 'time')\n",
    "median_TCG_dry = median_TCG_dry.median(dim = 'month')\n",
    "\n",
    "ndvi = ds.NDVI\n",
    "median_ndvi = ndvi.groupby('time.month').median(dim = 'time')\n",
    "median_ndvi = median_ndvi.median(dim = 'month')\n",
    "\n",
    "std_ndvi = ndvi.groupby('time.month').std(dim = 'time')\n",
    "std_ndvi = std_ndvi.std(dim = 'month')\n",
    "\n",
    "std_ndvi_dry = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "std_ndvi_dry = std_ndvi_dry.groupby('time.month').std(dim = 'time')\n",
    "std_ndvi_dry = std_ndvi_dry.std(dim = 'month')\n",
    "\n",
    "std_ndvi_diff1 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 0)\n",
    "std_ndvi_diff2 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 7)\n",
    "std_ndvi_diff = std_ndvi_diff1 - std_ndvi_diff2\n",
    "\n",
    "median_ndvi_dry = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "median_ndvi_dry = median_ndvi_dry.groupby('time.month').median(dim = 'time')\n",
    "median_ndvi_dry = median_ndvi_dry.median(dim = 'month')\n",
    "\n",
    "median_LAI_dry = ds.LAI['time.month'].isin(dry_months)\n",
    "median_LAI_dry = ds.LAI.groupby('time.month').median(dim = 'time')\n",
    "median_LAI_dry = median_LAI_dry.median(dim = 'month')\n",
    "\n",
    "BS_dry = ds_resampled.BS[ds_resampled.BS['time.month'].isin(dry_months)]\n",
    "BS_dry = BS_dry.groupby('time.month').median(dim = 'time')\n",
    "BS_dry = BS_dry.median(dim = 'month')\n",
    "\n",
    "PV_dry = ds_resampled.PV[ds_resampled.PV['time.month'].isin(dry_months)]\n",
    "PV_dry = PV_dry.groupby('time.month').median(dim = 'time')\n",
    "PV_dry = PV_dry.median(dim = 'month')\n",
    "\n",
    "NPV_dry = ds_resampled.NPV[ds_resampled.NPV['time.month'].isin(dry_months)]\n",
    "NPV_dry = NPV_dry.groupby('time.month').median(dim = 'time')\n",
    "NPV_dry = NPV_dry.median(dim = 'month')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds).\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variable for path to save files\n",
    "savefilepath = '/g/data/zk34/ljg547/Outputs/'\n",
    "\n",
    "# Set project naming convention. Start and end dates are reformated to remove '-'.\n",
    "Proj = 'SSC_WD_'\n",
    "\n",
    "ds_startDate = str(ds.isel(time=0).time.values)[0:10]\n",
    "ds_startDate = str(ds_startDate[0:4] + f'{int(ds_startDate[6:7]):02d}' + \n",
    "              f'{int(ds_startDate[9:10]):02d}')\n",
    "\n",
    "ds_endDate = str(ds.isel(time=-1).time.values)[0:10]\n",
    "ds_endDate = str(ds_endDate[0:4] + f'{int(ds_endDate[6:7]):02d}' + \n",
    "              f'{int(ds_endDate[9:10]):02d}')\n",
    "\n",
    "fc_startDate = str(dataset_fc.isel(time=0).time.values)[0:10]\n",
    "fc_startDate = str(fc_startDate[0:4] + f'{int(fc_startDate[6:7]):02d}' + \n",
    "              f'{int(fc_startDate[9:10]):02d}')\n",
    "\n",
    "fc_endDate = str(dataset_fc.isel(time=-1).time.values)[0:10]\n",
    "fc_endDate = str(fc_endDate[0:4] + f'{int(fc_endDate[6:7]):02d}' + \n",
    "              f'{int(fc_endDate[9:10]):02d}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. Note some products (e.g. FC and Wofs) have different time-series period as they are imported from Collection 2 rather than Collection 3 (current collection of Landsat data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_TCW_dry.to_dataset(name='median_TCW_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset = arr, filename = fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Wetness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCW_dry_median is the median value of TCW over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_TCB_dry.to_dataset(name='median_TCB_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Brigthness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCB_dry_median is the median value of TCB over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_TCG_dry.to_dataset(name='median_TCG_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianTCG_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCG_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Greenness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCG_dry_median is the median value of TCG over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_ndvi.to_dataset(name='median_ndvi')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'MedianNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Median NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi.to_dataset(name='std_ndvi')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi_dry.to_dataset(name='std_ndvi_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for the dry season(\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = std_ndvi_diff.to_dataset(name='std_ndvi_diff')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Comparison between NDVI standard deviation during the wet season (January) ad at the end of the dry season (August).\" + \"\\n\" \n",
    "      \"Time series includes imagery from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Where vegetation is accessing more reliable water sources (e.g. groundwater), residual standard deviation is low.\" + \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_ndvi_dry.to_dataset(name='median_ndvi_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'mediandNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"NDVI of dry period (\" + str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "      \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"NDVI_dry_median is the median value of NDVI over the dry months.\"+ \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = median_LAI_dry.to_dataset(name='median_LAI_dry')\n",
    "arr.attrs = ds.attrs\n",
    "fname = str(savefilepath + Proj + 'medianLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'MedianLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Leaf Area Index for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"LAI_dry_median is the median value of LAI over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = BS_dry.to_dataset(name='BS_dry')\n",
    "arr.attrs = dataset_fc.attrs\n",
    "fname = str(savefilepath + Proj + 'medianBS_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'medianBS_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Fraction of Bare Soil (as compared to photo-synthetic vegetation and non-photosynthetic vegetation) for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"The BS value was calculated for the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = PV_dry.to_dataset(name='PV_dry')\n",
    "arr.attrs = dataset_fc.attrs\n",
    "fname = str(savefilepath + Proj + 'medianPV_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'medianPV_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Fraction of photosynthetic vegetation (as compared to base soil and non-photosynthetic vegetation) for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"The PV value was calculated for the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = NPV_dry.to_dataset(name='NPV_dry')\n",
    "arr.attrs = dataset_fc.attrs\n",
    "fname = str(savefilepath + Proj + 'medianNPB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'medianNPV_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Fraction of non-photosynthetic vegetation (as compared to photo-synthetic vegetation and bare soil) for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"The NPV value was calculated for the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "arr = frequency.to_dataset(name='WofS_WetFrequency')\n",
    "arr.attrs = dataset_wofs.attrs\n",
    "fname = str(savefilepath + Proj + 'WofS_WetFrequency_' +\n",
    "              fc_startDate + '_' + fc_endDate + '.tif')\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Create metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'WofS_WetFrequency_' +\n",
    "              fc_startDate + '_' + fc_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"The frequence of wet, as calculated for each pixel using the Water Observations from Space algorithm.\" + \n",
    "        \"Time series: \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
