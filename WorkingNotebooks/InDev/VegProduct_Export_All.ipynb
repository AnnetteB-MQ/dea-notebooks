{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General advice (delete this cell before submitting for review)\n",
    "\n",
    "> * When choosing a location for your analysis, **select an area that has data on both the NCI and DEA Sandbox** to allow your code to be run on both environments. \n",
    "For example, you can check this for Landsat using the [DEA Explorer](https://explorer.sandbox.dea.ga.gov.au/ga_ls5t_ard_3/1990) (use the drop-down menu to view all products).\n",
    "As of September 2019, the `DEA Sandbox` has a single year of continental Landsat data for 2015-16, and the full 1987-onward time-series for three locations (Perth WA, Brisbane QLD, and western NSW).\n",
    "> * When adding **Products used**, embed the hyperlink to that specific product on the DEA Explorer using the `[product_name](product url)` syntax.\n",
    "> * When writing in Markdown cells, start each sentence on a **new line**.\n",
    "This makes it easy to see changes through git commits.\n",
    "> * Use Australian English in markdown cells and code comments.\n",
    "> * Check the [known issues](https://github.com/GeoscienceAustralia/dea-docs/wiki/Known-issues) for formatting regarding the conversion of notebooks to DEA docs using Sphinx.\n",
    "Things to be aware of:\n",
    "    * Sphinx is highly sensitive to bulleted lists:\n",
    "        * Ensure that there is an empty line between any preceding text and the list\n",
    "        * Only use the `*` bullet (`-` is not recognised)\n",
    "        * Sublists must be indented by 4 spaces\n",
    "    * Two kinds of formatting cannot be used simultaneously:\n",
    "        * Hyperlinked code: \\[\\`code_format\\`](hyperlink) fails\n",
    "        * Bolded code: \\*\\*\\`code_format\\`\\*\\* fails\n",
    "    * Headers must appear in heirachical order (`#`, `##`, `###`, `####`) and there can only be one title (`#`).\n",
    "> * Use the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. To make sure all code in the notebook is consistent, you can use the `jupyterlab_code_formatter` tool: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended). This will reformat the code in the cell to a consistent style.\n",
    "> * For additional guidance, refer to the style conventions and layouts in approved `develop` branch notebooks. \n",
    "Examples include\n",
    "    * [Frequently_used_code/Using_load_ard.ipynb](./Frequently_used_code/Using_load_ard.ipynb)\n",
    "    * [Real_world_examples/Coastal_erosion.ipynb](./Real_world_examples/Coastal_erosion.ipynb)\n",
    "    * [Scripts/dea_datahandling.py](./Scripts/dea_datahandling.py)\n",
    "> * The DEA Image placed in the title cell will display as long as the notebook is contained in one of the standard directories.\n",
    "It does not work in the highest level directory (hence why it doesn't display in the original template notebook).\n",
    "> * In the final notebook cell, include a set of relevant tags which are used to build the DEA User Guide's [Tag Index](https://docs.dea.ga.gov.au/genindex.html). \n",
    "Use all lower-case (unless the tag is an acronym), separate words with spaces (unless it is the name of an imported module), and [re-use existing tags](https://github.com/GeoscienceAustralia/dea-notebooks/wiki/List-of-tags).\n",
    "Ensure the tags cell below is in `Raw` format, rather than `Markdown` or `Code`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Landsat Collection 3 vegetation-related indicies <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatability:** Notebook currently compatible with the `NCI`|`DEA Sandbox` environment only\n",
    "* **Products used:** \n",
    "[ga_ls5t_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls5t_ard_3),\n",
    "[ga_ls7e_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls7e_ard_3),\n",
    "[ga_ls8c_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls8c_ard_3)\n",
    "* **Special requirements:** An _optional_ description of any special requirements, e.g. If running on the [NCI](https://nci.org.au/), ensure that `module load otps` is run prior to launching this notebook\n",
    "* **Prerequisites:** An _optional_ list of any notebooks that should be run or content that should be understood prior to launching this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "An *optional* overview of the scientific, economic or environmental management issue or challenge being addressed by Digital Earth Australia. \n",
    "For `Beginners_Guide` or `Frequently_Used_Code` notebooks, this may include information about why the particular technique or approach is useful or required. \n",
    "If you need to cite a scientific paper or link to a website, use a persistent DOI link if possible and link in-text (e.g. [Dhu et al. 2017](https://doi.org/10.1080/20964471.2017.1402490))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "A _compulsory_ description of the notebook, including a brief overview of how Digital Earth Australia helps to address the problem set out above.\n",
    "It can be good to include a run-down of the tools/methods that will be demonstrated in the notebook:\n",
    "\n",
    "1. First we do this\n",
    "2. Then we do this\n",
    "3. Finally we do this\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Provide any particular instructions that the user might need, e.g. To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n",
    "Begin with any `iPython` magic commands, followed by standard Python packages, then any additional functionality you need from the `Scripts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import dask\n",
    "from dask.utils import parse_bytes\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils.dask import start_local_dask\n",
    "\n",
    "from psutil import virtual_memory, cpu_count\n",
    "\n",
    "# Load custom DEA notebook functions\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "import dea_datahandling\n",
    "import dea_plotting\n",
    "import DEADataHandling\n",
    "from dea_bandindices import calculate_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dc_landsat3 = datacube.Datacube(app='VegProductsExport_Coll3', env='c3-samples')\n",
    "except:\n",
    "    dc_landsat3 = datacube.Datacube(app='VegProductsExport_Coll3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "\n",
    "* `dry_months`: Specific months of the year that correspond with dry season/low rainfall conditions. Values range from 0-11. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_months = [5,6,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dask cluster\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell.\n",
    "\n",
    "The parameters for generating the local dask cluster are automatically generated, but if you wish to alter them use the documentation [here](https://distributed.dask.org/en/stable/local-cluster.html). \n",
    "\n",
    "Put simply, the code below identifies how many cpus and how much RAM the computer has and generates a local cluster using those variables.\n",
    "\n",
    "> **Note:** Use this markdown format (sparingly) to draw particular attention to an important point or caveat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:37018</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:44898/status' target='_blank'>http://127.0.0.1:44898/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>30.67 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:37018' processes=1 threads=8, memory=30.67 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "cpu_limit = cpu_count()\n",
    "cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "\n",
    "mem_limit = virtual_memory()\n",
    "mem_limit = mem_limit.total\n",
    "mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# leave 3Gb for notebook itself\n",
    "mem_limit -= parse_bytes('3Gb')\n",
    "\n",
    "# close previous client if any, so we can re-run this cell without issues\n",
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "\n",
    "# start up a local cluster\n",
    "client = start_local_dask(n_workers=1,\n",
    "                          threads_per_worker=cpu_limit,\n",
    "                          memory_limit=mem_limit)\n",
    "\n",
    "# show the dask cluster settings\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define spatial and temporal query\n",
    "\n",
    "If running this notebook locally, use the smaller spatial extent and subset of the time series. \n",
    "\n",
    "If running on gadi, the the temporal and spatial extent can be increased.  \n",
    "\n",
    "> **Note:** Landsat imagery is available from 1987 onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_3 = {\n",
    "        'lon': (134.24, 134.34),             # small test area - this works via vdi\n",
    "        'lat': (-20.79, -20.86),             # small test area - this works via vdi\n",
    "#        'lon': (132.07, 132.50),             # small test area - current test extent on vdi\n",
    "#        'lat': (-20.31, -21.00),             # small test area - current test extent on vdi\n",
    "#        'lon': (132.07, 135.36),             # full study area\n",
    "#        'lat': (-20.31, -22.11),             # full study area\n",
    "        'time':('2015-01', '2018-12'),       # subset of time-series\n",
    "#        'time':('1987-01', '2018-12'),       # full time-series\n",
    "        'output_crs': 'EPSG:28352',\n",
    "         'resolution': (30, 30),\n",
    "         'group_by': 'solar_day'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "Load Landsat data from Collection 3 using .load_ard.\n",
    "\n",
    "mask_dtype = np.float16 helps to keep the memory down, however, the data will need to be converted back to float34 later.\n",
    "\n",
    "dask_chunks also helps to keep our memory use down and conduct the analysis in parallel. 'x' and 'y' define the pixel 'chunks' the dataset will be divided into. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ga_ls5t_ard_3 data\n",
      "    No data for ga_ls5t_ard_3\n",
      "Loading ga_ls7e_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Loading ga_ls8c_ard_3 data\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Combining and sorting data\n",
      "    Returning 255 observations \n"
     ]
    }
   ],
   "source": [
    "ds = dea_datahandling.load_ard(dc=dc_landsat3,\n",
    "#        mask_dtype = np.float16,\n",
    "        products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "        measurements=['nbart_red','nbart_nir','nbart_green',\n",
    "                      'nbart_blue','nbart_swir_1','nbart_swir_2'],\n",
    "        mask_contiguity='nbart_contiguity',\n",
    "#        dask_chunks = {'x':500, 'y':500},\n",
    "        **query_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up automated file naming and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set variable for path to save files\n",
    "savefilepath = '/g/data/zk34/ljg547/Outputs/'\n",
    "\n",
    "# Set project naming convention. Start and end dates are reformated to remove '-'.\n",
    "Proj = 'SSC_WD_'\n",
    "\n",
    "ds_startDate = str(ds.isel(time=0).time.values)[0:10]\n",
    "ds_startDate = str(ds_startDate[0:4] + f'{int(ds_startDate[6:7]):02d}' + \n",
    "              f'{int(ds_startDate[9:10]):02d}')\n",
    "\n",
    "ds_endDate = str(ds.isel(time=-1).time.values)[0:10]\n",
    "ds_endDate = str(ds_endDate[0:4] + f'{int(ds_endDate[6:7]):02d}' + \n",
    "              f'{int(ds_endDate[9:10]):02d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised Difference Vegetation Index (NDVI) \n",
    "\n",
    "Elaborate on NDVI here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:       (time: 255, x: 357, y: 272)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.684e+06 7.684e+06 ... 7.692e+06 7.692e+06\n",
       "  * x             (x) float64 1.046e+06 1.046e+06 ... 1.056e+06 1.056e+06\n",
       "  * time          (time) datetime64[ns] 2015-01-07T01:00:31.257836 ... 2018-12-25T00:55:54.633447\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float32 nan nan nan nan ... 1986.0 1933.0 1894.0\n",
       "    nbart_nir     (time, y, x) float32 nan nan nan nan ... 2958.0 2861.0 2832.0\n",
       "    nbart_green   (time, y, x) float32 nan nan nan nan ... 1053.0 1055.0 1041.0\n",
       "    nbart_blue    (time, y, x) float32 nan nan nan nan ... 545.0 546.0 532.0\n",
       "    nbart_swir_1  (time, y, x) float32 nan nan nan nan ... 4233.0 4203.0 4095.0\n",
       "    nbart_swir_2  (time, y, x) float32 nan nan nan nan ... 3655.0 3644.0 3592.0\n",
       "    NDVI          (time, y, x) float32 nan nan nan ... 0.19357532 0.19847651\n",
       "Attributes:\n",
       "    crs:      EPSG:28352</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (time: 255, x: 357, y: 272)\n",
       "Coordinates:\n",
       "  * y             (y) float64 7.684e+06 7.684e+06 ... 7.692e+06 7.692e+06\n",
       "  * x             (x) float64 1.046e+06 1.046e+06 ... 1.056e+06 1.056e+06\n",
       "  * time          (time) datetime64[ns] 2015-01-07T01:00:31.257836 ... 2018-12-25T00:55:54.633447\n",
       "Data variables:\n",
       "    nbart_red     (time, y, x) float32 nan nan nan nan ... 1986.0 1933.0 1894.0\n",
       "    nbart_nir     (time, y, x) float32 nan nan nan nan ... 2958.0 2861.0 2832.0\n",
       "    nbart_green   (time, y, x) float32 nan nan nan nan ... 1053.0 1055.0 1041.0\n",
       "    nbart_blue    (time, y, x) float32 nan nan nan nan ... 545.0 546.0 532.0\n",
       "    nbart_swir_1  (time, y, x) float32 nan nan nan nan ... 4233.0 4203.0 4095.0\n",
       "    nbart_swir_2  (time, y, x) float32 nan nan nan nan ... 3655.0 3644.0 3592.0\n",
       "    NDVI          (time, y, x) float32 nan nan nan ... 0.19357532 0.19847651\n",
       "Attributes:\n",
       "    crs:      EPSG:28352"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate NDVI. NDVI is added to ds as a new band as shown in the below display\n",
    "calculate_indices(ds,index = 'NDVI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDVI statistics for the full time series\n",
    "\n",
    "Here we calculate mean and standard deviation of NDVI values for the full time series for each pixel. \n",
    "\n",
    "This provides information about the average 'greenness' and variation in 'greenness' over the long-term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new NDVI DataArray \n",
    "ndvi = ds.NDVI\n",
    "\n",
    "# Calculate mean NDVI through time for each pixel\n",
    "mean_ndvi = ndvi.mean(dim='time')\n",
    "\n",
    "# Calculate NDVI standard deviation through time for each pixel\n",
    "std_ndvi = ndvi.std(dim='time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDVI statistics for the defined dry season\n",
    "\n",
    "Here we use the previously defined dry_months parameter to look at mean and standard deviation of NDVI during dry conditions over multiple years.\n",
    "\n",
    "Looking at the same period of time over multiple years reduces the noise and highlights longer-term trends in vegetation greenness under dry conditions.\n",
    "\n",
    "Vegetation with access to groundwater resources are hypothesised to have higher mean and lower standard deviation compared to vegetation with access to less reliable water sources (i.e. rain-fed).\n",
    "\n",
    "mean_ndvi_dry can be viewed after each step to understand how each step compresses the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group available NDVI time-steps into dry season months for later monthly averaging\n",
    "ndvi_dryMonths = ndvi[ndvi['time.month'].isin(dry_months)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean NDVI for each month in the dry period (through time and for each pixel)\n",
    "mean_ndvi_dry = ndvi_dryMonths.groupby('time.month').mean(dim = 'time')\n",
    "\n",
    "# Calculate mean NDVI for the dry period based on the mean monthly values for each month in the dry period\n",
    "mean_ndvi_dry = mean_ndvi_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation in NDVI for each month in the dry period (through time and for each pixel)\n",
    "std_ndvi_dry = ndvi_dryMonths.groupby('time.month').std(dim = 'time')\n",
    "\n",
    "# Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "std_ndvi_dry = std_ndvi_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference in NDVI standard deviation for two specific months. 0 = January; 7 = July\n",
    "std_ndvi_diff1 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 0)\n",
    "std_ndvi_diff2 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 7)\n",
    "std_ndvi_diff = std_ndvi_diff1 - std_ndvi_diff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data\n",
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. \n",
    "\n",
    "We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "\n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds). \n",
    "\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = mean_ndvi_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "#arr = arr.to_dataset(name='mean_NDVI_dry')\n",
    "arr = mean_ndvi_dry.to_dataset(name='mean_NDVI_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'meanNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset = arr, filename = fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Mean NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = std_ndvi.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = std_ndvi.to_dataset(name='std_ndvi')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for all months\" + \" from \" + ds_startDate + \n",
    "      \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = std_ndvi_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = std_ndvi_dry.to_dataset(name='std_ndvi_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Standard deviation of NDVI for the dry season(\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Higher standard deviation suggests greater variation in vegetation greenness and therefore inferred water supply.\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = std_ndvi_diff.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = std_ndvi_diff.to_dataset(name='std_ndvi_diff')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "              ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Comparison between NDVI standard deviation during the wet season (January) ad at the end of the dry season (August).\" + \"\\n\" \n",
    "      \"Time series includes imagery from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" + \n",
    "      \"Where vegetation is accessing more reliable water sources (e.g. groundwater), residual standard deviation is low.\" + \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = mean_ndvi_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = mean_ndvi_dry.to_dataset(name='mean_ndvi_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'meanNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"NDVI of dry period (\" + str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "      \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"NDVI_dry_mean is the mean value of NDVI over the dry months.\"+ \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalised Difference Wetness Index (NDWI)\n",
    "Add text about NDWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDWI. NDWI is added to ds as a new band as shown in the below display\n",
    "calculate_indices(ds,index = 'NDWI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NDWI statistics for the defined dry season\n",
    "\n",
    "Here we use the previously defined dry_months parameter to look at mean and standard deviation of NDWI during dry conditions over multiple years.\n",
    "\n",
    "Looking at the same period of time over multiple years reduces the noise and highlights longer-term trends in vegetation wetness under dry conditions.\n",
    "\n",
    "Vegetation with access to groundwater resources are hypothesised to have higher mean and lower standard deviation compared to vegetation with access to less reliable water sources (i.e. rain-fed).\n",
    "\n",
    "mean_ndwi_dry can be viewed after each step to understand how each step compresses the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group available NDWI time-steps into dry season months for later monthly averaging\n",
    "mean_ndwi_dry = ndwi[ndvi['time.month'].isin(dry_months)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation in NDWI for each month in the dry period (through time and for each pixel)\n",
    "mean_ndwi_dry = mean_ndwi_dry.groupby('time.month').mean(dim = 'time')\n",
    "\n",
    "# Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "mean_ndwi_dry = mean_ndwi_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data\n",
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. \n",
    "\n",
    "We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "\n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds). \n",
    "\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = mean_ndwi_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = mean_ndwi_dry.to_dataset(name='mean_NDWI_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'meanNDVI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "\n",
    "f.write(\"NDWI of dry period (\" + str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "      \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "      \"NDWI_dry_mean is the mean value of NDVI over the dry months.\"+ \"\\n\"\n",
    "      \"This product was derived from VegProducts_Export_Coll3.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Area Index (LAI)\n",
    "Add text about LAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate LAI. LAI is added to ds as a new band as shown in the below display\n",
    "calculate_indices(ds,index = 'LAI', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group available LAI time-steps into dry season months for later monthly averaging\n",
    "mean_LAI_dry = ds.LAI['time.month'].isin(dry_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviation in LAI for each month in the dry period (through time and for each pixel)\n",
    "mean_LAI_dry = ds.LAI.groupby('time.month').mean(dim = 'time')\n",
    "\n",
    "# Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "mean_LAI_dry = mean_LAI_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting data\n",
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. \n",
    "\n",
    "We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "\n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds). \n",
    "\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from float16 to float32\n",
    "#arr = mean_LAI_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = mean_LAI_dry.to_dataset(name='mean_LAI_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'meanLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanLAI_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Leaf Area Index for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"LAI_dry_mean is the mean value of LAI over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasselled Cap Index (TCI)\n",
    "Add text about TCI inc diff bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate different band ratios. Each are Calculate NDWI. added to ds as a new band as shown in the below display\n",
    "#Tasselled Cap Wetness\n",
    "calculate_indices(ds,index = 'TCW', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Tasselled Cap Brightness\n",
    "calculate_indices(ds,index = 'TCB', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)\n",
    "\n",
    "# Tasselled Cap Greenness\n",
    "calculate_indices(ds,index = 'TCG', collection = 'ga_ls_3',\n",
    "        normalise = True, deep_copy = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tasselled Cap Wetness\n",
    "# Group available TCW time-steps into dry season months for later monthly averaging\n",
    "mean_TCW_dry = ds.TCW['time.month'].isin(dry_months)\n",
    "\n",
    "# Calculate mean TCW for each month in the dry period (through time and for each pixel)\n",
    "mean_TCW_dry = ds.TCW.groupby('time.month').mean(dim = 'time')\n",
    "\n",
    "# Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "mean_TCW_dry = mean_TCW_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tasselled Cap Brightness\n",
    "# Group available TCB time-steps into dry season months for later monthly averaging\n",
    "mean_TCB_dry = ds.TCB['time.month'].isin(dry_months)\n",
    "\n",
    "# Calculate mean TCB for each month in the dry period (through time and for each pixel)\n",
    "mean_TCB_dry = ds.TCB.groupby('time.month').mean(dim = 'time')\n",
    "\n",
    "# Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "mean_TCB_dry = mean_TCB_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tasselled Cap Greenness\n",
    "# Group available TCG time-steps into dry season months for later monthly averaging\n",
    "mean_TCG_dry = ds.TCG['time.month'].isin(dry_months)\n",
    "\n",
    "# Calculate mean TCG for each month in the dry period (through time and for each pixel)\n",
    "mean_TCG_dry = ds.TCG.groupby('time.month').mean(dim = 'time')\n",
    "\n",
    "# Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "mean_TCG_dry = mean_TCG_dry.mean(dim = 'month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data\n",
    "In order to use the datacube.helpers write_geotiff function to export a simple single-band, single time-slice geotiff the above xarray DataArrays need to be converted to xarray Datasets. \n",
    "\n",
    "We do this be using the xarray function .to_dataset. If you don't do this, the write_geotiff fucntion will return an error. \n",
    "\n",
    "We also need to reassign the coordinate reference system before the write_geotiff function will work. This is done by the .attrs function. We take the crs from the original imported data (ds). \n",
    "\n",
    "Each file will be exported as a geotiff and saved in the same directory as this notebook. It can be downloaded from this location to the GA network using FileZilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasselled Cap Wetness\n",
    "# Convert from float16 to float32\n",
    "#arr = mean_TCW_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = mean_TCW_dry.to_dataset(name='mean_TCW_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs\n",
    "\n",
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "\n",
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCW_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Wetness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCW_dry_mean is the mean value of TCW over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3_Dask.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasselled Cap Brightness\n",
    "# Convert from float16 to float32\n",
    "#arr = mean_TCB_dry.astype(dtype='float32')\n",
    "\n",
    "# Convert from DataArray to Dataset\n",
    "arr = mean_TCB_dry.to_dataset(name='mean_TCB_dry')\n",
    "\n",
    "# Assign CRS from original DataArray\n",
    "arr.attrs = ds.attrs\n",
    "\n",
    "# Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "fname = str(savefilepath + Proj + 'meanTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.tif')\n",
    "\n",
    "# Writing data to file\n",
    "write_geotiff(dataset=arr, filename=fname)\n",
    "\n",
    "# Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "f = open(savefilepath + Proj + 'meanTCB_DrySeason' +\n",
    "              str(dry_months[0]+1) + 'to' + str(dry_months[-1]+1) +\n",
    "              '_' + ds_startDate + '_' + ds_endDate + '.txt','w')  \n",
    "\n",
    "f.write(\"Tasselled Cap Brightness for the dry season (\" + \n",
    "        str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "        \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "        \"TCB_dry_mean is the mean value of TCB over the dry months.\"+ \"\\n\" +\n",
    "        \"This product was derived from VegProducts_Export_Coll3.ipynb\"\n",
    "    )\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must close the cluster after use, particularly if moving between notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "client = None\n",
    "\n",
    "cluster.close()\n",
    "cluster = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`sentinel 2`, :index:`landsat 8`, :index:`dea_plotting`, :index:`rgb`, :index:`NDVI`, :index:`time series`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
