{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classification\n",
    "\n",
    "**What does this notebook do?** This notebook classifies remote sensing data using a random forest classifier model. Key features include being able to efficiently import training data from a set of point, line or polygon shapefiles (i.e. data is extracted from each shapefile separately to avoid slow `dc.load` on large areas), and allow flexible and consistent selection of training and analysis data using import functions (i.e. ensuring training data is consistent with analysis data). The notebook exports geotiff classification outputs and a series of evaluation figures to help fine-tune the classifier.\n",
    "\n",
    "**Date:** April 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "\"**Tags**: :index:`image_classification`, :index:`machine_learning`, :index:`random_forest`, :index:`Landsat8`, :index:`mangroves`, :index:`tasseled_cap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import datacube\n",
    "from datacube import Datacube\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "import geopandas as gpd\n",
    "\n",
    "# # Import DEA Notebooks scripts\n",
    "sys.path.append(os.path.abspath('/g/data/u46/users/ck9738/dea-notebooks/Algorithms/'))\n",
    "from ClassificationTools import randomforest_train\n",
    "from ClassificationTools import randomforest_classify\n",
    "from ClassificationTools import randomforest_eval\n",
    "from DEADataHandling import tasseled_cap\n",
    "from DEADataHandling import load_nbarx\n",
    "from DEADataHandling import load_sentinel\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Set up datacube instance\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "\n",
    "s2aws = Datacube(config='/home/547/ck9738/datacube-s2.conf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "## Functions used to import training and analysis data\n",
    "These functions import datacube data using a query, and return an xarray dataset with one time-step, multiple bands/variables and `crs` and `affine` attributes. \n",
    "This format is required as an input to both `randomforest_train` and `randomforest_classify` to ensure that both training and analysis data are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def hltc_import(query):\n",
    "    \n",
    "    \"\"\"\n",
    "    Imports high and low composite data for a given spatial query, and\n",
    "    return an xarray dataset with 'crs' and 'affine' attributes \n",
    "    \n",
    "    :attr query: spatial query for datacube.load()\n",
    "    :returns: xarray dataset with 'crs' and 'affine' attributes\n",
    "    \"\"\"\n",
    "\n",
    "    # Import data\n",
    "    low_tide = dc.load(product = 'low_tide_comp_20p', **query)\n",
    "    high_tide = dc.load(product = 'high_tide_comp_20p', **query)\n",
    "\n",
    "    # Rename variables in each high/low composite so datasets can be merged\n",
    "    data_vars = list(low_tide.data_vars)\n",
    "    low_tide.rename({var: \"lt_\" + var for var in data_vars}, inplace = True)\n",
    "    high_tide.rename({var: \"ht_\" + var for var in data_vars}, inplace = True)\n",
    "\n",
    "    # Combine into one dataset\n",
    "    output_xarray = xr.auto_combine([low_tide, high_tide]).isel(time = 0)\n",
    "    \n",
    "    # Set attributes   \n",
    "    output_xarray.attrs['crs'] = low_tide.crs\n",
    "    output_xarray.attrs['affine'] = low_tide.affine \n",
    "    \n",
    "    return output_xarray\n",
    "\n",
    "\n",
    "def tc_import(query):\n",
    "    \n",
    "    '''\n",
    "    Wrapper around load_nbarx and tasseled_cap to return an xarray dataset with \n",
    "    'crs' and 'affine' attributes\n",
    "    \n",
    "    :attr query: query for datacube call; for training, supply only\n",
    "    non-spatial queries as spatial are generated from training data\n",
    "    :returns: xarray dataset with 'crs' and 'affine' attributes\n",
    "    '''\n",
    "    \n",
    "    # Import cleaned Landsat bands data\n",
    "    nbart_data, _, _ = load_nbarx(dc, \"ls8\", query)\n",
    "    \n",
    "    # Compute tasseled cap indices and take median of multiple timesteps\n",
    "    output_xarray = tasseled_cap(sensor_data = nbart_data, \n",
    "                                 sensor = 'ls8',\n",
    "                                 drop = True).median(\"time\", keep_attrs = True)\n",
    "    \n",
    "    return output_xarray   \n",
    "\n",
    "\n",
    "def nbart_import(query):\n",
    "    \n",
    "    '''\n",
    "    Takes median of a set of nbart Landsat data; could be replaced with geomedian or any other \n",
    "    temporal aggregation method\n",
    "    \n",
    "    :attr query: query for datacube call; for training, supply only\n",
    "    non-spatial queries as spatial are generated from training data\n",
    "    :returns: xarray dataset with 'crs' and 'affine' attributes\n",
    "    '''\n",
    "    \n",
    "    # Import cleaned Landsat bands data\n",
    "    nbart_data, _, _ = load_nbarx(dc, \"ls8\", query)\n",
    "    \n",
    "    # Combine into one temporally aggregated layer\n",
    "    aggregated_data = nbart_data.median(dim = \"time\", keep_attrs = True)\n",
    "    \n",
    "    return aggregated_data\n",
    "\n",
    "\n",
    "\n",
    "def sent_GM_import(shapefile):\n",
    "    \"\"\"\n",
    "    Takes geomedian of a set of nbart sentinal data; \"\"\"\n",
    "    \n",
    "                # Open vector of training points with gdal \n",
    "    data_source = gpd.read_file(shapefile)\n",
    "            #data_source = gdal.OpenEx(train_shp) #gdal.OF_VECTOR)\n",
    "            #layer = data_source.GetLayer(0)\n",
    "            #convert the shapefile to GDA94 lat-long coords so we can query dc_load using lat long\n",
    "    data_source['geometry'] = data_source['geometry'].to_crs(epsg=4283)\n",
    "            #alfa = project_area.total_bounds\n",
    "            # Compute extents and generate spatial query\n",
    "    coords = data_source.total_bounds\n",
    "    xmin, ymax, xmax, ymin = coords\n",
    "    \n",
    "        # Import cleaned Landsat bands data\n",
    "    #nbart_data, _, _ = load_sentinel(s2aws, \"s2a_ard_granule\", query, bands_of_interest=['t_red', 't_nir1', 't_swir1'])\n",
    "                                     \n",
    "    sensors= ['s2a_ard_granule'] #pick the sentinal satelites you want\n",
    "\n",
    "    bands_of_int =['t_red', 't_nir1', 't_swir1'] #pick the sentinal bands that you want, here i am just using visible light  \n",
    "\n",
    "    \n",
    "    \n",
    "    query = {\n",
    "            'lat': (ymin, ymax),\n",
    "            'lon': (xmin, xmax),\n",
    "            'output_crs': 'EPSG:3577',\n",
    "            'resolution': (-10, 10),\n",
    "            'time':('2017-07-20', '2017-08-30')\n",
    "             }\n",
    "# use s2b_ard_granule \t for S2B\n",
    "    data_sent=s2aws.load(product='s2a_ard_granule', measurements=bands_of_int,group_by='solar_day', **query)                                  \n",
    "                            \n",
    "#             ['t_aerosol', 't_blue', 't_green', 't_red', t_rededge1, \n",
    "#             t_rededge2, t_rededge3, 't_nir1', t_nir2, t_swir1, t_swir2] )\n",
    "    \n",
    "    \n",
    "    \n",
    "#     data_sent = xr.concat(nbart_data.values(),'time')\n",
    "#     data_sent = data_sent.sortby('time')\n",
    "#     data_sent.attrs['crs'] = crs\n",
    "#     data_sent.attrs['affin|e'] = affine\n",
    "    \n",
    "    print(data_sent)\n",
    "    data_sent_flt = data_sent.astype(float,casting='safe')\n",
    "    \n",
    "    #create geomedian for data\n",
    "    Sentinal_gm=GeoMedian().compute(data_sent_flt)\n",
    "    \n",
    "    return(Sentinal_gm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Set up analysis\n",
    "Defines parameters used for analysis. This notebook contains example analyses for three classification tasks: classifying mangroves using high-low tide composites (HLTC), tasseled cap classification and classification of water vs non-water using median Landsat nbart data. Run one of the cells below to select the desired analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# For Tree Type classification using sentinal geomedian: #\n",
    "#####################################################\n",
    "\n",
    "# Working directory\n",
    "os.chdir(\"/g/data/u46/users/ck9738/Random_forest\")\n",
    "\n",
    "# List of training files to import. Shapefiles can be either points, lines or polygons, \n",
    "# but must be in the same projection system as the remote sensing dataset being analysed. \n",
    "# Each file should cover a small enough spatial area so as to not slow dc.load function \n",
    "# excessively (e.g. 100 x 100km max)\n",
    "train_shps = ['/g/data/u46/users/ck9738/Random_forest/raw_data/CER_trees/forrest_training_cer.shp']\n",
    "\n",
    "# Output path for classified geotiff\n",
    "classification_output = \"output_data/classification_CER1.tif\"\n",
    "\n",
    "# Optional dict to re-map training shapefile classes; useful for combining classes\n",
    "# ('3:2' re-maps class 3 to class 2)\n",
    "train_reclass = None\n",
    "\n",
    "# Names of output classification classes\n",
    "classification_names = ['not_trees','trees']\n",
    "\n",
    "# Set data function used to import data and optional parameters (e.g. time for temporal data).\n",
    "# For example, 'tc_import' needs an additional 'time' query as it draws on Landsat time-series \n",
    "# data, while 'hltc_import' uses high-low tide composites that have no temporal dimension\n",
    "data_func = sent_GM_import\n",
    "data_func_params = {'time': ('2017-01-01', '2017-01-30')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 9,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Import training data and fit model\n",
    "Uses `randomforest_train` to extract training data from potentially multiple training shapefiles, and returns a trained classifier (and optionally, training label and training sample arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 14,
        "hidden": false,
        "row": 13,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am densified (external_values, 25 elements)\n",
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 2, x: 327, y: 272)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 2017-07-20T00:21:11.026000 ...\n",
      "  * y        (y) float64 -3.395e+06 -3.395e+06 -3.395e+06 -3.395e+06 ...\n",
      "  * x        (x) float64 1.264e+06 1.264e+06 1.264e+06 1.264e+06 1.264e+06 ...\n",
      "Data variables:\n",
      "    t_red    (time, y, x) int16 2183 2013 1826 1658 1760 2172 1985 1492 1429 ...\n",
      "    t_nir1   (time, y, x) int16 3124 2931 2796 2540 2640 2971 2846 2502 2427 ...\n",
      "    t_swir1  (time, y, x) int16 4601 4601 4474 4474 4508 4686 4428 4428 3996 ...\n",
      "Attributes:\n",
      "    crs:      EPSG:3577\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in method 'RasterizeLayer', argument 4 of type 'OGRLayerShadow *'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7e21900ec3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                                        \u001b[0mdata_func_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_func_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                        \u001b[0mclassifier_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                                        train_reclass = train_reclass)\n\u001b[0m",
      "\u001b[0;32m/g/data/u46/users/ck9738/dea-notebooks/Algorithms/ClassificationTools.py\u001b[0m in \u001b[0;36mrandomforest_train\u001b[0;34m(train_shps, train_field, data_func, data_func_params, classifier_params, train_reclass)\u001b[0m\n\u001b[1;32m     76\u001b[0m             training_pixels = rasterize_vector(layer, cols_train, rows_train,\n\u001b[1;32m     77\u001b[0m                                                \u001b[0mgeo_transform_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                                field=train_field)\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Extract matching image sample data for each labelled pixel location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/u46/users/ck9738/dea-notebooks/Algorithms/SpatialTools.py\u001b[0m in \u001b[0;36mrasterize_vector\u001b[0;34m(input_data, cols, rows, geo_transform, projection, field, raster_path, array_dtype)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Rasterise by taking attributes from supplied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mgdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRasterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ATTRIBUTE=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20180405/lib/python3.6/site-packages/osgeo/gdal.py\u001b[0m in \u001b[0;36mRasterizeLayer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRasterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m     \u001b[0;34m\"\"\"RasterizeLayer(Dataset dataset, int bands, Layer layer, void * pfnTransformer=None, void * pTransformArg=None, int burn_values=0, char ** options=None, GDALProgressFunc callback=0, void * callback_data=None) -> int\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_gdal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRasterizeLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPolygonize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'RasterizeLayer', argument 4 of type 'OGRLayerShadow *'"
     ]
    }
   ],
   "source": [
    "# Dict of classifier parameters\n",
    "classifier_params = {'n_jobs': -1,                                    \n",
    "                     'n_estimators': 100,\n",
    "                     'max_features': \"auto\",\n",
    "                     'min_samples_leaf': 1,\n",
    "                     'oob_score': True }\n",
    "\n",
    "# Extract training data for each training shapefile and train classifier\n",
    "classifier, train_lab, train_samp = randomforest_train(train_shps = train_shps,\n",
    "                                                       train_field = \"class\",\n",
    "                                                       data_func = data_func,\n",
    "                                                       data_func_params = data_func_params,\n",
    "                                                       classifier_params = classifier_params,\n",
    "                                                       train_reclass = train_reclass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 27,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Import analysis data and classify\n",
    "Classifies and exports an analysis dataset using a previously trained random forest classifier, provided this dataset has the same number of bands/variables as the data used to train the classifier. Using the same data function used to train the classifier (e.g. `data_func` previously defined as either `tc_import`, `hltc_import` or `nbart_import`) will ensure this is the case. By setting `class_prob = True`, you can optionally export a geotiff of predicted class probabilities in addition to the classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open  shapfile and data loat from that\n",
    "\n",
    "shapefile_loc='/g/data/u46/users/ck9738/Random_forest/raw_data/CER_trees/cer_project_test_area.shp'\n",
    "\n",
    "\n",
    "#import project area shapefiles\n",
    "project_area = gpd.read_file(shapefile_loc)\n",
    "\n",
    "#convert the shapefile to GDA94 lat-long coords so we can query dc_load using lat long\n",
    "project_area['geometry'] = project_area['geometry'].to_crs(epsg=4283)\n",
    "\n",
    "#find the bounding box that contains all the queried projects\n",
    "#addig 0.025 degrees to the bounding box to be certain the polygons are captured\n",
    "coords = project_area.total_bounds\n",
    "xmin, ymax, xmax, ymin =coords\n",
    "\n",
    "            \n",
    "query = {'lat': (ymin, ymax),\n",
    "        'lon': (xmin, xmax),         \n",
    "        'crs': 'EPSG:3577',\n",
    "        **data_func_params}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Set up analysis data query\n",
    "#lat_point, lon_point, buffer = -12.5798399926, 130.782907919, 30000\n",
    "# lat_point, lon_point, buffer = -12.5693189393, 135.033955268, 20000\n",
    "#x, y = geometry.point(lon_point, lat_point, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "# query = {'lat': (-35.253939, -35.335371),\n",
    "#         'lon': (149.092760, 149.284286),         \n",
    "#          'crs': 'EPSG:3577',\n",
    "#           **data_func_params}\n",
    "\n",
    "# Load data from datacube\n",
    "analysis_xarray = data_func(query)\n",
    "\n",
    "# Run classification and export to file   \n",
    "class_array, prob_array = randomforest_classify(classifier = classifier,\n",
    "                                                analysis_data = analysis_xarray,\n",
    "                                                classification_output = classification_output,\n",
    "                                                class_prob = True)\n",
    "\n",
    "\n",
    "# Plot output classification\n",
    "class_xarray = xr.DataArray(class_array, \n",
    "                   coords = [analysis_xarray.y, analysis_xarray.x], \n",
    "                   dims = ['y', 'x'],\n",
    "                   name = \"Classification output\")\n",
    "class_xarray.plot(levels = list(np.unique(class_array)) + [len(np.unique(class_array)) + 1], \n",
    "                  figsize = (8, 8))\n",
    "\n",
    "# Plot predicted class probability (proportion of trees agreeing with classification)\n",
    "plt.plot()\n",
    "prob_xarray = xr.DataArray(prob_array, \n",
    "                           coords = [analysis_xarray.y, analysis_xarray.x], \n",
    "                           dims = ['y', 'x'],\n",
    "                           name = \"Predicted class probability\")\n",
    "prob_xarray.plot(cmap = \"plasma_r\",  \n",
    "                 vmin = np.percentile(prob_array, 3), \n",
    "                 vmax = np.percentile(prob_array, 97),\n",
    "                 figsize = (8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 32,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Feature/band/variable importance\n",
    "Extract classifier estimates of the relative importance of each band/variable for training the classifier. Useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#  Extract feature importances from trained classifier\n",
    "importance = classifier.feature_importances_\n",
    "importance_df = pd.DataFrame({'variable': analysis_xarray.data_vars,\n",
    "                              'importance': importance})\n",
    "importance_df.set_index(\"variable\", inplace = True)\n",
    "importance_df.plot.bar(title = \"Variable importance (global)\")\n",
    "display(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "scrolled": false
   },
   "source": [
    "## Plot performance of model by parameter values\n",
    "Random forest classifiers contain many modifiable parameters that can strongly affect the performance of the model. This section evaluates the effect of these parameters by plotting out-of-bag (OOB) error for a set of classifier parameter scenarios, and exports the resulting plots to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test effect of max features\n",
    "classifier_scenario1 = [(\"max_features = 'sqrt'\",\n",
    "                        RandomForestClassifier(warm_start = True,\n",
    "                                               oob_score = True,\n",
    "                                               max_features = \"sqrt\")),\n",
    "                       (\"max_features = 'log2'\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               oob_score = True,\n",
    "                                               max_features = \"log2\")),\n",
    "                        \n",
    "                       (\"max_features = '0.1'\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               max_features = 0.1,\n",
    "                                               oob_score = True)),\n",
    "                        \n",
    "                       (\"max_features = '0.5'\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               max_features = 0.5,\n",
    "                                               oob_score = True)),\n",
    "                       (\"max_features = None\",\n",
    "                        RandomForestClassifier(warm_start = True,\n",
    "                                               max_features = None,\n",
    "                                               oob_score = True))]\n",
    "\n",
    "# Test effect of minimum samples per leaf\n",
    "classifier_scenario2 = [(\"Leaf = 1\",\n",
    "                         RandomForestClassifier(warm_start = True,\n",
    "                                                min_samples_leaf = 1,\n",
    "                                                oob_score = True)),                       \n",
    "                       (\"Leaf = 10\",\n",
    "                        RandomForestClassifier(warm_start = True,\n",
    "                                               min_samples_leaf = 10,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Leaf = 20\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               min_samples_leaf = 20,\n",
    "                                               oob_score = True)),\n",
    "                       (\"Leaf = 40\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               min_samples_leaf = 40,\n",
    "                                               oob_score = True))]\n",
    "\n",
    "# Test effect of max depth\n",
    "classifier_scenario3 = [(\"Max depth = 1\",\n",
    "                         RandomForestClassifier(warm_start = True,\n",
    "                                                max_depth = 1,\n",
    "                                                oob_score = True)),                       \n",
    "                       (\"Max depth = 2\",\n",
    "                        RandomForestClassifier(warm_start = True,\n",
    "                                               max_depth = 2,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Max depth = 5\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               max_depth = 5,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Max depth = 10\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               max_depth = 10,\n",
    "                                               oob_score = True))]\n",
    "\n",
    "# Test effect of max leaf node\n",
    "classifier_scenario4 = [(\"Max leaf node = 5\",\n",
    "                         RandomForestClassifier(warm_start = True,\n",
    "                                                max_leaf_nodes = 5,\n",
    "                                                oob_score = True)),                       \n",
    "                       (\"Max leaf node = 10\",\n",
    "                        RandomForestClassifier(warm_start = True,\n",
    "                                               max_leaf_nodes = 10,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Max leaf node = 20\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               max_leaf_nodes = 20,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Max leaf node = 40\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               max_leaf_nodes = 40,\n",
    "                                               oob_score = True))]\n",
    "\n",
    "# Test effect of max leaf node\n",
    "classifier_scenario5 = [(\"Min samples split = 5\",\n",
    "                         RandomForestClassifier(warm_start = True,\n",
    "                                                min_samples_split = 5,\n",
    "                                                oob_score = True)),                       \n",
    "                       (\"Min samples split = 10\",\n",
    "                        RandomForestClassifier(warm_start = True,\n",
    "                                               min_samples_split = 10,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Min samples split = 20\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               min_samples_split = 20,\n",
    "                                               oob_score = True)),                       \n",
    "                       (\"Min samples split = 40\",\n",
    "                        RandomForestClassifier(warm_start = True, \n",
    "                                               min_samples_split = 40,\n",
    "                                               oob_score = True))]\n",
    "\n",
    "# Produce figures and export plots for each set of classification scenarios\n",
    "for i, classifier_scenario in enumerate([classifier_scenario1, \n",
    "                                         classifier_scenario2, \n",
    "                                         classifier_scenario3,\n",
    "                                         classifier_scenario4,\n",
    "                                         classifier_scenario5]):\n",
    "    \n",
    "    # Plot OOB error by classifier scenario\n",
    "    randomforest_eval(training_labels = train_lab, \n",
    "                      training_samples = train_samp, \n",
    "                      classifier_scenario = classifier_scenario,\n",
    "                      output_path = \"figures/random_forest_params_{}.png\".format(i + 1),\n",
    "                      max_estimators = 200)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "## Export tree diagrams\n",
    "Export .png plots of each decision tree in the random forest ensemble. Useful for inspecting the splits used by the classifier to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot output random forest trees to file\n",
    "for n, tree_in_forest in enumerate(classifier.estimators_):    \n",
    "\n",
    "    # Create graph and save to dot file\n",
    "    export_graphviz(tree_in_forest,\n",
    "                    out_file = \"figures/tree_graphs/tree.dot\",\n",
    "                    feature_names = list(analysis_xarray.data_vars),\n",
    "                    class_names = classification_names,\n",
    "                    filled = True,\n",
    "                    rounded = True)\n",
    "\n",
    "    # Plot as figure\n",
    "    os.system('dot -Tpng figures/tree_graphs/tree.dot -o ' + \\\n",
    "              'figures/tree_graphs/tree' + str(n + 1) + '.png')    \n",
    "    \n",
    "# Plot first resulting tree\n",
    "img = mpimg.imread('figures/tree_graphs/tree1.png')\n",
    "plt.figure(figsize = (15,12))\n",
    "plt.imshow(img, interpolation = \"bilinear\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
