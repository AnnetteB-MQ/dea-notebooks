{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHTS testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Take full time series of NDWI in dask\n",
    "* Compute 2D tides for every timestep\n",
    "* Compute median of tides from entire tide timeseries\n",
    "* For each year in dask NDWI timeseries:\n",
    "    * Mask pixels where tide > overall median\n",
    "    * `.compute()` and take median \n",
    "    \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import otps\n",
    "import datacube\n",
    "import shapely.wkt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from skimage.morphology import disk\n",
    "from skimage.morphology import square\n",
    "from skimage.morphology import binary_dilation\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.virtual import catalog_from_file\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_plotting import rgb\n",
    "from dea_spatialtools import interpolate_2d\n",
    "from dea_plotting import display_map\n",
    "from dea_spatialtools import contour_extract\n",
    "from dea_plotting import map_shapefile\n",
    "\n",
    "\n",
    "def largest_region(bool_array, **kwargs):\n",
    "    \n",
    "    '''\n",
    "    Takes a boolean array and identifies the largest contiguous region of \n",
    "    connected True values. This is returned as a new array with cells in \n",
    "    the largest region marked as True, and all other cells marked as False.\n",
    "    \n",
    "    Last modified: August 2019\n",
    "    Author: Robbi Bishop-Taylor\n",
    "    \n",
    "    Parameters\n",
    "    ----------  \n",
    "    bool_array : boolean array\n",
    "        A boolean array (numpy or xarray DataArray) with True values for\n",
    "        the areas that will be inspected to find the largest group of \n",
    "        connected cells\n",
    "    **kwargs : \n",
    "        Optional keyword arguments to pass to `measure.label`\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    largest_region : boolean array\n",
    "        A boolean array with cells in the largest region marked as True, \n",
    "        and all other cells marked as False.       \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # First, break boolean array into unique, descrete regions/blobs\n",
    "    blobs_labels = measure.label(bool_array, background=0, **kwargs)\n",
    "    \n",
    "    # Count the size of each blob, excluding the background class (0)\n",
    "    ids, counts = np.unique(blobs_labels[blobs_labels > 0], \n",
    "                            return_counts=True) \n",
    "    \n",
    "    # Identify the region ID of the largest blob\n",
    "    largest_region_id = ids[np.argmax(counts)]\n",
    "    \n",
    "    # Produce a boolean array where 1 == the largest region\n",
    "    largest_region = blobs_labels == largest_region_id\n",
    "    \n",
    "    return largest_region\n",
    "\n",
    "\n",
    "def mask_ocean(bool_array, **kwargs):\n",
    "    '''\n",
    "    Identifies ocean by selecting the largest connected area of water\n",
    "    pixels, then dilating this region by 1 pixel to include mixed pixels\n",
    "    '''\n",
    "    ocean_mask = largest_region(bool_array, **kwargs)\n",
    "    ocean_mask = binary_dilation(ocean_mask, \n",
    "                                 selem=square(3))\n",
    "    \n",
    "    return ocean_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='MAHTS_testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load virtual products catalogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog_from_file('MAHTS_virtual_products.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tidal points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = gpd.read_file('input_data/tide_points_coastal.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gold Coast\n",
    "# query = {'x': (153.49,  153.68),\n",
    "#          'y': (-28.08, -28.2681),\n",
    "#          'time': ('2000', '2018'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80]}\n",
    "\n",
    "\n",
    "# # Moreton Bay\n",
    "# query = {'x': (153.00, 153.25),\n",
    "#          'y': (-26.81, -27.44),\n",
    "#          'time': ('2008', '2008'),\n",
    "#          'cloud_cover': [0, 80]}\n",
    "\n",
    "# # Moreton Bay seam test\n",
    "# query = {'x': (153.10, 153.18),\n",
    "#          'y': (-26.70, -26.83),\n",
    "#          'time': ('2000', '2010'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80]}\n",
    "\n",
    "# Moreton Bay small\n",
    "query = {'x': (153.16, 153.2840),\n",
    "         'y': (-27.315, -27.4416),\n",
    "         'time': ('2000', '2010'),\n",
    "         'dask_chunks': {'time': 1},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "\n",
    "# Preview study area\n",
    "display_map(x=query['x'], y=query['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load virtual product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = catalog['ls_nbart_indices']\n",
    "ds = product.load(dc, **query)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.awei_ns.isel(time=slice(0, 8)).plot.imshow(col='time', col_wrap=4, robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out subset of modelling points for region around satellite data\n",
    "bounds = shapely.wkt.loads(ds.geobox.geographic_extent.buffer(0.1).wkt)\n",
    "subset_gdf = points_gdf[points_gdf.geometry.intersects(bounds)]\n",
    "\n",
    "# Extract lon, lat from tides, and time from satellite data\n",
    "x_vals = subset_gdf.geometry.centroid.x\n",
    "y_vals = subset_gdf.geometry.centroid.y\n",
    "observed_datetimes = ds.time.data.astype('M8[s]').astype('O').tolist()\n",
    "\n",
    "# Create list of lat/lon/time scenarios to model\n",
    "observed_timepoints = [otps.TimePoint(lon, lat, date) \n",
    "                       for date in observed_datetimes\n",
    "                       for lon, lat in zip(x_vals, y_vals)]\n",
    "\n",
    "# Model tides for each scenario\n",
    "observed_predictedtides = otps.predict_tide(observed_timepoints)\n",
    "\n",
    "# Output results into pandas.DataFrame\n",
    "tidepoints_df = pd.DataFrame([(i.timepoint.timestamp, \n",
    "                               i.timepoint.lon, \n",
    "                               i.timepoint.lat, \n",
    "                               i.tide_m) for i in observed_predictedtides], \n",
    "                             columns=['time', 'lon', 'lat', 'tide_m']) \n",
    "\n",
    "# Convert data to spatial geopandas.GeoDataFrame\n",
    "tidepoints_gdf = gpd.GeoDataFrame(data={'i': np.unique(tidepoints_df.time, \n",
    "                                                       return_inverse=True)[1], \n",
    "                                        'tide_m': tidepoints_df.tide_m}, \n",
    "                                  geometry=gpd.points_from_xy(tidepoints_df.lon, \n",
    "                                                              tidepoints_df.lat), \n",
    "                                  crs={'init': 'EPSG:4326'})\n",
    "\n",
    "# Reproject to satellite data CRS\n",
    "tidepoints_gdf = tidepoints_gdf.to_crs(epsg=ds.crs.epsg)\n",
    "\n",
    "# Plot a sample timestep\n",
    "ds.isel(time=1).awei_ns.plot()\n",
    "tidepoints_gdf.plot(ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate tides into each satellite timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "\n",
    "for i in range(0, len(ds.time)):\n",
    "\n",
    "    # Get lists of x, y and z (tide height) data to interpolate\n",
    "    x_coords = tidepoints_gdf[tidepoints_gdf.i == i].geometry.x,\n",
    "    y_coords = tidepoints_gdf[tidepoints_gdf.i == i].geometry.y,\n",
    "    z_coords = tidepoints_gdf[tidepoints_gdf.i == i].tide_m\n",
    "\n",
    "    # Interpolate tides into the extent of the satellite timestep\n",
    "    out_tide = interpolate_2d(ds=ds.isel(time=i),\n",
    "                              x_coords=x_coords,\n",
    "                              y_coords=y_coords,\n",
    "                              z_coords=z_coords,\n",
    "                              fill_nearest=True,\n",
    "                              sigma=10)\n",
    "\n",
    "    # Convert to float32 to save memory and add to list\n",
    "    out.append(out_tide.astype(np.float32))\n",
    "    \n",
    "# Create xarray.DataArray and add as a data variable in satellite data\n",
    "tide_da = (xr.DataArray(data=np.dstack(out), \n",
    "                        coords=[ds.y, ds.x, ds.time], \n",
    "                        dims=['y', 'x', 'time'],\n",
    "                        attrs={'units': 'm'})\n",
    "                \n",
    "           # Re-order dimensions to match satellite data\n",
    "           .transpose('time', 'y', 'x'))\n",
    "\n",
    "# Determine tide cutoff\n",
    "tide_cutoff = tide_da.median(dim='time')\n",
    "\n",
    "# Add as measurement in satellite dataset\n",
    "ds['tide_m'] = tide_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "ds['tide_m'].isel(time=58).plot.imshow(robust=True, cmap='viridis', size=12)\n",
    "tidepoints_gdf.plot(ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tide_cutoff = tide_da.quantile(dim='time', q=[0.20, 0.50, 0.80])\n",
    "# da_year = ds.sel(time='2005').ndwi.compute()\n",
    "\n",
    "# low_da = da_year.where(tide_da < tide_cutoff.sel(quantile=0.2)).median(dim='time')\n",
    "# med_da = da_year.where(tide_da > tide_cutoff.sel(quantile=0.5)).median(dim='time')\n",
    "# high_da = da_year.where(tide_da > tide_cutoff.sel(quantile=0.8)).median(dim='time')\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 10))\n",
    "# low_da.plot.imshow(ax=axes[0], vmin=-0.8, vmax=0.8, \n",
    "#                    add_colorbar=False, add_labels=False, cmap='RdBu')\n",
    "# med_da.plot.imshow(ax=axes[1], vmin=-0.8, vmax=0.8, \n",
    "#                    add_colorbar=False, add_labels=False, cmap='RdBu')\n",
    "# high_da.plot.imshow(ax=axes[2], vmin=-0.8, vmax=0.8, \n",
    "#                     add_colorbar=False, add_labels=False, cmap='RdBu')\n",
    "\n",
    "# # test = ds.where(ds.tide_m < tide_cutoff.sel(quantile=0.2))[['ndwi']].median(dim='time', keep_attrs=True)\n",
    "# # write_geotiff(filename='continuous_tides_test3.tif', dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate yearly composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_list = []\n",
    "\n",
    "for year in np.unique(ds.time.dt.year):\n",
    "    \n",
    "    print(f'Processing {year}')\n",
    "\n",
    "    # Pull out a single year and mask to keep above median tide pixels\n",
    "    year_ds = ds.sel(time=str(year)).compute()\n",
    "    year_ds = year_ds.where(year_ds.tide_m > tide_cutoff)\n",
    "    \n",
    "    # Compute median water indices and counts of valid pixels\n",
    "    median_ds = year_ds.median(dim='time', keep_attrs=True)\n",
    "    median_ds['count'] = (year_ds.mndwi\n",
    "                          .count(dim='time', keep_attrs=True)\n",
    "                          .astype('int16'))\n",
    "    \n",
    "    # Write each variable to file\n",
    "    for i in median_ds:\n",
    "        write_geotiff(filename=f'output_data/{i}_{str(year)}.tif', \n",
    "                      dataset=median_ds[[i]])\n",
    "\n",
    "    # Assign year dimension and append to list\n",
    "    median_ds = median_ds.assign_coords(year=str(year)).expand_dims('year')    \n",
    "    yearly_list.append(median_ds)\n",
    "\n",
    "# Concatenate all annual layers unto a single ds\n",
    "yearly_ds = xr.concat(yearly_list, dim='year')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ds.mndwi.isel(time=slice(0, 9)).plot(col='time', col_wrap=3)   #.count(dim=['x', 'y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract shoreline contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_index = 'mndwi'\n",
    "index_threshold = 0.0\n",
    "\n",
    "# Apply index and threshold\n",
    "thresholded_ds = yearly_ds[water_index] > index_threshold\n",
    "\n",
    "# Generate all time 600 m buffer from ocean\n",
    "full_sea_mask = mask_ocean(thresholded_ds.median(dim='year'))\n",
    "buffer_ocean = binary_dilation(full_sea_mask, disk(20))\n",
    "buffer_land = binary_dilation(~full_sea_mask, disk(20))\n",
    "sea_buffer = buffer_ocean & buffer_land\n",
    "\n",
    "# Generate sea mask for each timestep\n",
    "yearly_sea_mask = ((yearly_ds[water_index] > index_threshold)\n",
    "                   .groupby('year')\n",
    "                   .apply(mask_ocean))\n",
    "\n",
    "# Add in equivelent tjkgjf jf anylithing data\n",
    "\n",
    "# Keep only pixels that are within 600 m of the ocean in the\n",
    "# full stack, and directly connected to ocean in each yearly timestep\n",
    "masked_ds = yearly_ds[water_index].where(yearly_sea_mask & sea_buffer)\n",
    "\n",
    "# Prepare attributes as input to contour extract\n",
    "attribute_data = {'year': yearly_ds.year.values.tolist()}  \n",
    "attribute_dtypes = {'year': 'str'}\n",
    "\n",
    "# Extract contours with custom attribute fields:\n",
    "output_path = f'output_data/contours_{water_index}_{index_threshold:.2f}'\n",
    "contours_gdf = contour_extract(z_values=[index_threshold],\n",
    "                               ds_array=masked_ds,\n",
    "                               ds_crs=f'EPSG:{ds.crs.epsg}',\n",
    "                               ds_affine=ds.geobox.transform,\n",
    "                               output_shp=f'{output_path}.shp',\n",
    "                               attribute_data=attribute_data,\n",
    "                               attribute_dtypes=attribute_dtypes,\n",
    "                               min_vertices=2,                                 \n",
    "                               verbose=False,                                   \n",
    "                               dim='year')\n",
    "\n",
    "# Export geojson\n",
    "contours_gdf.to_crs(epsg=4326).to_file(filename=f'{output_path}.geojson', \n",
    "                                       driver='GeoJSON')\n",
    "\n",
    "# Plot\n",
    "contours_gdf.plot(column='year', figsize=(12, 8), cmap='YlOrRd', linewidth=3)\n",
    "# plt.gca().set_xlim(547500, 565500) \n",
    "# plt.gca().set_ylim(-3125000, -3109000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_shapefile(gdf=contours_gdf, hover_col='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`sentinel 2`, :index:`dea_plotting`, :index:`rgb`, :index:`virtual products`, :index:`NDVI`, :index:`tasseled cap`, :index:`cloud masking`, :index:`dask`, :index:`image compositing`, :index:`statistics`, :index:`pixel quality`, :index:`combining data`, :index:`native load`, :index:`reprojecting`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
