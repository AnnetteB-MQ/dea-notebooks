{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHTS stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# !pip install --user ruptures\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# import glob\n",
    "# import shutil\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "import geopandas as gpd\n",
    "# import ruptures as rpt\n",
    "# from scipy import stats\n",
    "# from affine import Affine\n",
    "# import matplotlib.pyplot as plt\n",
    "# from shapely.wkt import loads\n",
    "from shapely.geometry import box\n",
    "# from rasterio.features import rasterize\n",
    "from rasterio.transform import array_bounds\n",
    "# from skimage.measure import label\n",
    "# from skimage.morphology import disk\n",
    "# from skimage.morphology import square\n",
    "# from skimage.morphology import binary_opening\n",
    "# from skimage.morphology import binary_dilation\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_spatialtools import subpixel_contours\n",
    "\n",
    "\n",
    "import deacoastlines_statistics as dcl_stats\n",
    "\n",
    "\n",
    "from shapely.ops import nearest_points\n",
    "import xarray as xr\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in contours\n",
    "study_area = 7832\n",
    "output_name = 'test2'\n",
    "water_index = 'mndwi'\n",
    "index_threshold = 0.00\n",
    "\n",
    "# Create output folder\n",
    "output_dir = f'output_data/{output_name}_{study_area}/vectors'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DEA Coastline rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_ds = dcl_stats.load_rasters(output_name, study_area, water_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding box to load data for\n",
    "bbox = gpd.GeoSeries(box(*array_bounds(height=yearly_ds.sizes['y'], \n",
    "                                       width=yearly_ds.sizes['x'], \n",
    "                                       transform=yearly_ds.transform)), \n",
    "                     crs=yearly_ds.crs)\n",
    "\n",
    "# Estaury mask\n",
    "estuary_gdf = (gpd.read_file('input_data/estuary_mask.shp', bbox=bbox)\n",
    "               .to_crs(yearly_ds.crs))\n",
    "\n",
    "# Rocky shore mask\n",
    "smartline_gdf = (gpd.read_file('input_data/Smartline.gdb', bbox=bbox)\n",
    "                 .to_crs(yearly_ds.crs))\n",
    "\n",
    "# Tide points\n",
    "points_gdf = (gpd.read_file('input_data/tide_points_coastal.geojson', bbox=bbox)\n",
    "          .to_crs(yearly_ds.crs))\n",
    "\n",
    "# Study area polygon\n",
    "comp_gdf = (gpd.read_file('input_data/50km_albers_grid.shp', bbox=bbox)\n",
    "            .set_index('id')\n",
    "            .to_crs(str(yearly_ds.crs)))\n",
    "\n",
    "# Mask to study area\n",
    "study_area_poly = comp_gdf.loc[study_area]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract shoreline contours\n",
    "\n",
    "### Extract ocean-masked contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating in single z-value, multiple arrays mode\n"
     ]
    }
   ],
   "source": [
    "# Mask dataset to focus on coastal zone only\n",
    "masked_ds = dcl_stats.contours_preprocess(yearly_ds, \n",
    "                                          water_index, \n",
    "                                          index_threshold, \n",
    "                                          estuary_gdf, \n",
    "                                          points_gdf)\n",
    "\n",
    "# Extract contours\n",
    "contours_gdf = subpixel_contours(da=masked_ds,\n",
    "                                 z_values=index_threshold,\n",
    "                                 min_vertices=10,\n",
    "                                 dim='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics\n",
    "### Measure distances from baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get array of water index values for baseline time period \n",
    "baseline_year = '2018'\n",
    "baseline_array = yearly_ds[water_index].sel(year=int(baseline_year))\n",
    "\n",
    "# Import contours and project to local CRS\n",
    "# contours_gdf = contours_clean_gdf\n",
    "contours_index_gdf = contours_gdf.set_index('year')\n",
    "\n",
    "# Set annual shoreline to use as a baseline\n",
    "baseline_contour = contours_index_gdf.loc[[baseline_year]].geometry\n",
    "\n",
    "# Generate points along line and convert to geopandas.GeoDataFrame\n",
    "points_line = [baseline_contour.iloc[0].interpolate(i) \n",
    "               for i in range(0, int(baseline_contour.length), 30)]\n",
    "points_gdf = gpd.GeoDataFrame(geometry=points_line, crs=baseline_array.crs)\n",
    "\n",
    "\n",
    "# Make a copy of the GeoDataFrame to hold tidal data\n",
    "tide_points_gdf = points_gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n"
     ]
    }
   ],
   "source": [
    "# Copy geometry to baseline point\n",
    "points_gdf['p_baseline'] = points_gdf.geometry\n",
    "baseline_x_vals = points_gdf.geometry.x\n",
    "baseline_y_vals = points_gdf.geometry.y\n",
    "\n",
    "# Iterate through all comparison years in contour gdf\n",
    "for comp_year in contours_index_gdf.index.unique().values[0:32]:\n",
    "\n",
    "    print(comp_year)\n",
    "\n",
    "    # Set comparison contour\n",
    "    comp_contour = contours_index_gdf.loc[[comp_year]].geometry.iloc[0]\n",
    "\n",
    "    # Find nearest point on comparison contour\n",
    "    points_gdf[f'p_{comp_year}'] = points_gdf.apply(lambda x: \n",
    "                                                    nearest_points(x.p_baseline, comp_contour)[1], axis=1)\n",
    "\n",
    "    # Compute distance between baseline and comparison year points\n",
    "    points_gdf[f'{comp_year}'] = points_gdf.apply(lambda x: \n",
    "                                                  x.geometry.distance(x[f'p_{comp_year}']), axis=1)\n",
    "\n",
    "    # Extract comparison array\n",
    "    comp_array = yearly_ds[water_index].sel(year=int(comp_year))\n",
    "\n",
    "    # Convert baseline and comparison year points to geoseries to allow easy access to x and y coords\n",
    "    comp_x_vals = gpd.GeoSeries(points_gdf[f'p_{comp_year}']).x\n",
    "    comp_y_vals = gpd.GeoSeries(points_gdf[f'p_{comp_year}']).y\n",
    "\n",
    "    # Sample NDWI values from arrays based on baseline and comparison points\n",
    "    baseline_x_vals = xr.DataArray(baseline_x_vals, dims='z')\n",
    "    baseline_y_vals = xr.DataArray(baseline_y_vals, dims='z')\n",
    "    comp_x_vals = xr.DataArray(comp_x_vals, dims='z')\n",
    "    comp_y_vals = xr.DataArray(comp_y_vals, dims='z')   \n",
    "    points_gdf['index_comp_p1'] = comp_array.interp(x=baseline_x_vals, y=baseline_y_vals)\n",
    "    points_gdf['index_baseline_p2'] = baseline_array.interp(x=comp_x_vals, y=comp_y_vals)\n",
    "\n",
    "    # Compute directionality of change (negative = erosion, positive = accretion)    \n",
    "    points_gdf['loss_gain'] = np.where(points_gdf.index_baseline_p2 > \n",
    "                                       points_gdf.index_comp_p1, 1, -1)\n",
    "    points_gdf[f'{comp_year}'] = points_gdf[f'{comp_year}'] * points_gdf.loss_gain\n",
    "\n",
    "    # Add tide data\n",
    "    tide_array = yearly_ds['tide_m'].sel(year=int(comp_year))\n",
    "    tide_points_gdf[f'{comp_year}'] = tide_array.interp(x=baseline_x_vals, y=baseline_y_vals)\n",
    "\n",
    "# Keep required columns\n",
    "points_gdf = points_gdf[['geometry'] + \n",
    "                        contours_index_gdf.index.unique().values.tolist()]\n",
    "points_gdf = points_gdf.round(2)\n",
    "\n",
    "# Zero values to 1988\n",
    "points_gdf.iloc[:,1:] = points_gdf.iloc[:,1:].subtract(points_gdf['1988'], axis=0)\n",
    "\n",
    "# Identify dates for regression\n",
    "x_years = yearly_ds.year.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',\n",
       "       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003',\n",
       "       '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011',\n",
       "       '2012', '2013', '2014', '2015', '2016', '2017', '2018'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contours_index_gdf.index.unique().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998,\n",
       "       1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009,\n",
       "       2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995',\n",
       "       '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003',\n",
       "       '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011',\n",
       "       '2012', '2013', '2014', '2015', '2016', '2017', '2018'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearly_ds.year.values.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify SOI values for regression\n",
    "climate_df = pd.read_csv('input_data/climate_indices.csv', index_col='year')\n",
    "climate_df = climate_df.loc[x_years,:]\n",
    "\n",
    "# Compute change rates\n",
    "rate_out = points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=x, \n",
    "                                                     x_vals = x_years, \n",
    "                                                     x_labels = x_years, \n",
    "                                                     std_dev=2), axis=1)\n",
    "points_gdf[['rate_time', 'incpt_time', 'sig_time', 'outl_time']] = rate_out\n",
    "\n",
    "\n",
    "# Compute tide flag\n",
    "tide_out = tide_points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=points_gdf[x_years.astype(str)].iloc[x.name], \n",
    "                                               x_vals=x, \n",
    "                                               x_labels=x_years, \n",
    "                                               std_dev=2), axis=1)\n",
    "points_gdf[['rate_tide', 'incpt_tide', 'sig_tide', 'outl_tide']] = tide_out \n",
    "\n",
    "\n",
    "# Compute stats for each index\n",
    "for ci in climate_df:\n",
    "\n",
    "    print(ci)\n",
    "\n",
    "    # Compute stats for each row\n",
    "    ci_out = points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=x,\n",
    "                                                       x_vals = climate_df[ci].values, \n",
    "                                                       x_labels = x_years, \n",
    "#                                                        detrend_params=[x.rate_time, x.incpt_time],\n",
    "                                                       std_dev=2), axis=1)\n",
    "\n",
    "    # Add data as columns  \n",
    "    points_gdf[[f'rate_{ci}', f'incpt_{ci}', f'sig_{ci}', f'outl_{ci}']] = ci_out\n",
    "\n",
    "\n",
    "# # Add breakpoints\n",
    "# print('Identifying breakpoints')\n",
    "# points_gdf['breakpoint'] = points_gdf.apply(lambda x: breakpoints(x=x[x_years.astype(str)], \n",
    "#                                                                   labels=x_years, \n",
    "#                                                                   pen=10), axis=1)\n",
    "\n",
    "# Set CRS\n",
    "points_gdf.crs = baseline_array.crs\n",
    "\n",
    "# Custom sorting\n",
    "points_towrite = points_gdf.loc[:, [\n",
    "    'rate_time', 'rate_SOI', 'rate_IOD', 'rate_SAM', 'rate_IPO', 'rate_PDO', 'rate_tide',\n",
    "    'sig_time', 'sig_SOI', 'sig_IOD', 'sig_SAM', 'sig_IPO', 'sig_PDO', 'sig_tide',\n",
    "    'outl_time', 'outl_SOI', 'outl_IOD', 'outl_SAM', 'outl_IPO', 'outl_PDO', 'outl_tide',\n",
    "#     'breakpoint', \n",
    "    *x_years.astype(str).tolist(), 'geometry'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip points to extent of polygon\n",
    "stats_path = f'{output_dir}/{study_area}_stats_{water_index}_{index_threshold:.2f}_{output_name}'\n",
    "points_gdf = points_gdf[points_gdf.intersects(study_area_poly['geometry'])]\n",
    "points_gdf.to_file(f'{stats_path}.geojson', driver='GeoJSON')\n",
    "\n",
    "# Overwrite contours after clipping to study area\n",
    "contour_path = f'{output_dir}/{study_area}_contours_{water_index}_{index_threshold:.2f}_{output_name}'\n",
    "contours_gdf['geometry'] = contours_gdf.intersection(study_area_poly['geometry'])\n",
    "contours_gdf.to_file(f'{contour_path}.geojson', driver='GeoJSON')\n",
    "\n",
    "# Export as shapefile\n",
    "contours_gdf.to_file(f'{contour_path}.shp')\n",
    "points_towrite.to_file(f'{stats_path}.shp')\n",
    "\n",
    "shutil.make_archive(base_name=f'output_data/outputs_{study_area}_{output_name}', \n",
    "                    format='zip', \n",
    "                    root_dir=output_dir)\n",
    "\n",
    "rocky_shore_buffer = rocky_shores_buffer(smartline_gdf=smartline_gdf, buffer=50)\n",
    "points_gdf = points_gdf[~points_gdf.intersects(rocky_shore_buffer.geometry.unary_union)]\n",
    "points_gdf.to_file(f'{stats_path}_nonrocky.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Time history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = points_gdf[x_years.astype(str)].iloc[[5129]] \n",
    "\n",
    "test.apply(lambda x: change_regress(row=x,\n",
    "                                    x_vals = x_years, \n",
    "                                    x_labels = x_years, \n",
    "                                    std_dev=2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = 5129\n",
    "plot_df = pd.DataFrame({\n",
    "    'mov': points_gdf.loc[id_val, contours_gdf.year.to_list()].values.astype(float),\n",
    "    'time': x_years,\n",
    "    'soi': climate_df['SOI'].values,\n",
    "    'tide': tide_points_gdf.mean(axis=0),\n",
    "})\n",
    "\n",
    "plot_df.plot.scatter(x='time',\n",
    "                     y='mov',\n",
    "                     c='soi',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")\n",
    "\n",
    "plot_df.plot.scatter(x='soi',\n",
    "                     y='mov',\n",
    "                     c='soi',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")\n",
    "\n",
    "plot_df.plot.scatter(x='tide',\n",
    "                     y='mov',\n",
    "                     c='tide',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.plot.scatter(x='time',\n",
    "                     y='tide',\n",
    "                     c='tide',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_val = 6015\n",
    "# signal = points_gdf.loc[id_val,contours_gdf.index.to_list()].values\n",
    "\n",
    "# # detection\n",
    "# algo = rpt.Pelt(model=\"rbf\", min_size=2, jump=1).fit(signal)\n",
    "# result = algo.predict(pen=8)\n",
    "# print(contours_gdf.index.to_list()[result[0]])\n",
    "# rpt.display(signal, [32], result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = climate_df.corr()\n",
    "# corr.style.background_gradient(cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "\n",
    "# Use the tidal model to compute tide heights for each observation:\n",
    "obs_datetimes = [datetime.datetime(1986, 8, 23, 1, 36, 23),\n",
    "                 datetime.datetime(1987, 5, 29, 1, 44, 59)]\n",
    "obs_timepoints = [TimePoint(115.35, -20.86, dt) \n",
    "                  for dt in obs_datetimes]\n",
    "obs_predictedtides = predict_tide(obs_timepoints)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "comp_gdf = gpd.read_file('input_data/item_polygons.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(env='c3-samples')\n",
    "\n",
    "from dea_datahandling import load_ard\n",
    "from dea_coastaltools import tidal_tag\n",
    "\n",
    "\n",
    "def tidal_sync(row):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "    #     lon, lat = row.geometry.centroid.coords.xy\n",
    "        lon = row.lon\n",
    "        lat = row.lat\n",
    "\n",
    "        # Load available data from all three Landsat satellites\n",
    "        ds = load_ard(dc=dc,\n",
    "                      products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "                      x=(lon - 0.01, lon + 0.01),\n",
    "                      y=(lat - 0.01, lat + 0.01),\n",
    "                      time=('1988', '2018'),\n",
    "                      measurements=['nbart_green'],\n",
    "                      output_crs='EPSG:3577',\n",
    "                      gqa_iterative_mean_xy=[0, 1],\n",
    "                      cloud_cover=[0, 80],\n",
    "                      resolution=(-30, 30),\n",
    "                      group_by='solar_day',\n",
    "                      dask_chunks={})\n",
    "\n",
    "        ds = tidal_tag(ds=ds)\n",
    "\n",
    "\n",
    "        annual_ht_mean = ds.tide_height.sel(time = ds.tide_height > ds.tide_height.median()).resample(time='Y').mean()\n",
    "        annual_ht_stats = change_regress(row=annual_ht_mean,\n",
    "                       x_vals=annual_ht_mean.time.dt.year,\n",
    "                       x_labels=annual_ht_mean.time.dt.year,\n",
    "                       std_dev=5,\n",
    "                       detrend_params=None,\n",
    "                       slope_var='slope',\n",
    "                       interc_var='intercept',\n",
    "                       pvalue_var='pvalue',\n",
    "                       outliers_var='outliers')\n",
    "\n",
    "        print(annual_ht_stats)\n",
    "        return(row.append(annual_ht_stats))\n",
    "    \n",
    "    except:\n",
    "        print('Failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = comp_gdf.iloc[200:].apply(lambda x: tidal_sync(x), axis=1)\n",
    "# test[~test.ID.isna()].to_file('tide_sync_test_3.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U --user --extra-index-url=\"https://packages.dea.ga.gov.au/\" odc-algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='Intertidal_elevation', env='c3-samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(env='c3-samples')\n",
    "from dea_datahandling import load_ard\n",
    "\n",
    "try:\n",
    "    dc = datacube.Datacube(app='Intertidal_elevation', env='c3-samples')\n",
    "except:\n",
    "    dc = datacube.Datacube(app='Intertidal_elevation')\n",
    "\n",
    "lat, lon = -20.58, 117.87\n",
    "\n",
    "# Load available data from all three Landsat satellites\n",
    "ds = load_ard(dc=dc,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "              x=(lon - 0.01, lon + 0.01),\n",
    "              y=(lat - 0.01, lat + 0.01),\n",
    "              time=('1988', '2018'),\n",
    "              measurements=['nbart_green'],\n",
    "              output_crs='EPSG:3577',\n",
    "              resolution=(-30, 30),\n",
    "              group_by='solar_day',\n",
    "              dask_chunks={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(env='c3-samples')\n",
    "\n",
    "from dea_datahandling import load_ard\n",
    "from dea_coastaltools import tidal_tag\n",
    "from dea_coastaltools import tidal_stats\n",
    "\n",
    "\n",
    "#     lon, lat = row.geometry.centroid.coords.xy\n",
    "lat, lon = -15.99, 137.21\n",
    "\n",
    "# Load available data from all three Landsat satellites\n",
    "ds = load_ard(dc=dc,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "              x=(lon - 0.01, lon + 0.01),\n",
    "              y=(lat - 0.01, lat + 0.01),\n",
    "              time=('1988', '2018'),\n",
    "              measurements=['nbart_green'],\n",
    "              output_crs='EPSG:3577',\n",
    "              gqa_iterative_mean_xy=[0, 1],\n",
    "              cloud_cover=[0, 80],\n",
    "              resolution=(-30, 30),\n",
    "              group_by='solar_day',\n",
    "              dask_chunks={})\n",
    "\n",
    "\n",
    "stats, observed_tides, all_tides = tidal_stats(ds=ds, return_tides=True, modelled_freq='30T')\n",
    "\n",
    "# print(annual_ht_stats)\n",
    "# return(row.append(annual_ht_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tides.quantile([0.4, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, = dc.find_datasets(product='ga_ls5t_ard_3', limit=1)\n",
    "dir(ds.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_diff = (all_tides.max() - all_tides.min()) * 0.15\n",
    "tide_min = 0 - tide_diff\n",
    "tide_max = 0 + tide_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_tides[observed_tides.tide_height.between(tide_min.item(), tide_max.item())].resample('1Y').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tides.loc[slice('2018-06-04', '2018-07-05')].plot(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tides.loc[slice('2018-12-01', '2018-12-02')].plot(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_buffer = min(((observed_tides.tide_height.max() - observed_tides.tide_height.min()) * 0.15), 1.0)\n",
    "tide_cutoff_min = 0.5-tide_buffer\n",
    "tide_cutoff_max = 0.5+tide_buffer\n",
    "\n",
    "\n",
    "observed_tides.plot()\n",
    "plt.axhline(tide_cutoff_min, color='red')\n",
    "plt.axhline(tide_cutoff_max, color='red')\n",
    "\n",
    "yearly_counts = observed_tides[(observed_tides.tide_height > tide_cutoff_min) & \n",
    "                               (observed_tides.tide_height < tide_cutoff_max)].resample('Y').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "((ds.tide_height.max() - ds.tide_height.min()) * 0.25).clip(0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds.tide_height.sel(time = ((ds.tide_height >= ds.tide_height.quantile(q=0.6)) & \n",
    "                                  (ds.tide_height <= ds.tide_height.quantile(q=0.9))))\n",
    "test.plot(size=8)\n",
    "test.resample(time='Y').mean().plot(linewidth=5, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.resample(time='Y').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_points_gdf.iloc[3057, 1:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = gpd.read_file('tide_sync_test_1.geojson')\n",
    "two = gpd.read_file('tide_sync_test_3.geojson')\n",
    "three = gpd.read_file('tide_sync_test_3.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.concat([one, two, three]).to_file('tide_sync_test.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[~test.ID.isna()].plot()\n",
    "\n",
    "# .to_file('tide_sync_test.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle            \n",
    "            \n",
    "with open(r\"someobject.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`sentinel 2`, :index:`dea_plotting`, :index:`rgb`, :index:`virtual products`, :index:`NDVI`, :index:`tasseled cap`, :index:`cloud masking`, :index:`dask`, :index:`image compositing`, :index:`statistics`, :index:`pixel quality`, :index:`combining data`, :index:`native load`, :index:`reprojecting`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
