{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHTS stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         108G   29G   80G  27% /\n",
      "tmpfs            68M     0   68M   0% /dev\n",
      "tmpfs            67G     0   67G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  108G   29G   80G  27% /notebooks\n",
      "/dev/nvme4n1     11G  9.2G  1.3G  89% /home/jovyan\n",
      "shm              68M     0   68M   0% /dev/shm\n",
      "tmpfs            67G     0   67G   0% /proc/acpi\n",
      "tmpfs            67G     0   67G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "def change_regress(row, x_vals, x_labels, std_dev=3):\n",
    "    \n",
    "    # Extract x (time) and y (distance) values\n",
    "    x = x_vals\n",
    "    y = row.values[1:].astype(np.float)\n",
    "    \n",
    "    # Drop NAN rows\n",
    "    xy_df = np.vstack([x, y]).T\n",
    "    is_valid = ~np.isnan(xy_df).any(axis=1)\n",
    "    xy_df = xy_df[is_valid]\n",
    "    valid_labels = x_labels[is_valid]\n",
    "    \n",
    "    # Remove outliers\n",
    "    outlier_bool = (np.abs(stats.zscore(xy_df)) < float(std_dev)).all(axis=1)\n",
    "    xy_df = xy_df[outlier_bool]\n",
    "        \n",
    "    # Compute linear regression\n",
    "    lin_reg = stats.linregress(x=xy_df[:,0], \n",
    "                               y=xy_df[:,1])\n",
    "    \n",
    "    # Return slope, p-values and list of outlier years excluded from regression   \n",
    "    return pd.Series({'slope': np.round(lin_reg.slope, 2), \n",
    "                      'pvalue': np.round(lin_reg.pvalue, 3),\n",
    "                      'outliers': str(valid_labels[~outlier_bool]).replace('[', '').replace(']', '')})\n",
    "\n",
    "\n",
    "# This will speed up loading data\n",
    "import datacube.utils.rio\n",
    "datacube.utils.rio.configure_s3_access(aws_unsigned=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in contours\n",
    "study_area = 'yamba'\n",
    "water_index = 'mndwi'\n",
    "index_threshold = '0.00'\n",
    "\n",
    "# Get array of water index values for baseline time period \n",
    "baseline_array = xr.open_rasterio(filename=f'output_data/{study_area}/{water_index}_1990.tif').squeeze(dim='band')\n",
    "\n",
    "# Import contours and project to local CRS\n",
    "contours_gdf = gpd.read_file(f'output_data/{study_area}/contours_{water_index}_{index_threshold}.geojson').to_crs(baseline_array.crs).set_index('year')\n",
    "\n",
    "# Set annual shoreline to use as a baseline\n",
    "baseline_year = '1990'\n",
    "baseline_contour = contours_gdf.loc[[baseline_year]].geometry\n",
    "\n",
    "# Generate points along line and convert to geopandas.GeoDataFrame\n",
    "points_line = [baseline_contour.iloc[0].interpolate(i) \n",
    "               for i in range(0, int(baseline_contour.length), 30)]\n",
    "points_gdf = gpd.GeoDataFrame(geometry=points_line, crs=baseline_array.crs)\n",
    "\n",
    "# Copy geometry to baseline point\n",
    "points_gdf['p_baseline'] = points_gdf.geometry\n",
    "baseline_x_vals = points_gdf.geometry.x\n",
    "baseline_y_vals = points_gdf.geometry.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n"
     ]
    }
   ],
   "source": [
    "# Iterate through all comparison years in contour gdf\n",
    "for comp_year in contours_gdf.index.unique().values:\n",
    "\n",
    "    print(comp_year)\n",
    "\n",
    "    # Set comparison contour\n",
    "    comp_contour = contours_gdf.loc[[comp_year]].geometry.iloc[0]\n",
    "\n",
    "    # Find nearest point on comparison contour\n",
    "    points_gdf[f'p_{comp_year}'] = points_gdf.apply(lambda x: \n",
    "                                                    nearest_points(x.p_baseline, comp_contour)[1], axis=1)\n",
    "\n",
    "    # Compute distance between baseline and comparison year points\n",
    "    points_gdf[f'{comp_year}'] = points_gdf.apply(lambda x: \n",
    "                                                  x.geometry.distance(x[f'p_{comp_year}']), axis=1)\n",
    "\n",
    "    # Extract comparison array\n",
    "    comp_array = xr.open_rasterio(filename=f'output_data/{study_area}/{water_index}_{comp_year}.tif').squeeze(dim='band')\n",
    "\n",
    "    # Convert baseline and comparison year points to geoseries to allow easy access to x and y coords\n",
    "    comp_x_vals = gpd.GeoSeries(points_gdf[f'p_{comp_year}']).x\n",
    "    comp_y_vals = gpd.GeoSeries(points_gdf[f'p_{comp_year}']).y\n",
    "\n",
    "    # Sample NDWI values from arrays based on baseline and comparison points\n",
    "    baseline_x_vals = xr.DataArray(baseline_x_vals, dims='z')\n",
    "    baseline_y_vals = xr.DataArray(baseline_y_vals, dims='z')\n",
    "    comp_x_vals = xr.DataArray(comp_x_vals, dims='z')\n",
    "    comp_y_vals = xr.DataArray(comp_y_vals, dims='z')   \n",
    "    points_gdf['index_comp_p1'] = comp_array.interp(x=baseline_x_vals, y=baseline_y_vals)\n",
    "    points_gdf['index_baseline_p2'] = baseline_array.interp(x=comp_x_vals, y=comp_y_vals)\n",
    "\n",
    "    # Compute directionality of change (negative = erosion, positive = accretion)    \n",
    "    points_gdf['loss_gain'] = (points_gdf.index_baseline_p2 > points_gdf.index_comp_p1).astype(int).replace(to_replace=0, value=-1)\n",
    "    points_gdf[f'{comp_year}'] = points_gdf[f'{comp_year}'] * points_gdf.loss_gain\n",
    "\n",
    "# Keep required columns\n",
    "points_gdf = points_gdf[['geometry'] + contours_gdf.index.unique().values.tolist()]\n",
    "points_gdf = points_gdf.round(2)\n",
    "\n",
    "# # Identify dates for regression\n",
    "x_years = np.array([int(i[:4]) for i in points_gdf.columns[1:]])\n",
    "\n",
    "# # Identify SOI values for regression\n",
    "# soi_df = pd.read_csv('/g/data/r78/rt1527/dea-notebooks/Waterline_extraction/raw_data/SOI_EastAnglia.txt', \n",
    "#                      sep='\\t', skiprows=1, usecols=['year', 'annual average'], index_col='year')\n",
    "# soi_df = soi_df.rename({'annual average': 'annual_SOI'}, axis=1)\n",
    "# x_soi = soi_df.loc[x_years].annual_SOI.values\n",
    "\n",
    "# Identify SOI values for regression\n",
    "climate_df = pd.read_csv('input_data/climate_indices.csv', index_col='year')\n",
    "\n",
    "x_soi = climate_df.loc[x_years].SOI_NOAA.values\n",
    "x_iod = climate_df.loc[x_years].IOD.values\n",
    "\n",
    "# # Identify La Nina / El Nino years\n",
    "# lan_eln = np.array(['ElN', 'LaN', 'LaN', 'na', 'ElN', 'ElN', 'ElN', 'ElN', 'na', 'na', 'ElN', \n",
    "#                     'LaN', 'LaN', 'LaN', 'na', 'ElN', 'na', 'na', 'na', 'ElN', 'LaN', 'LaN', \n",
    "#                     'ElN', 'LaN', 'LaN', 'na', 'na', 'na', 'ElN', 'na', 'na', 'na'])\n",
    "# lan_eln_df = pd.DataFrame({'year': range(1987, 2019), 'lan_eln': lan_eln}).set_index('year')\n",
    "# x_lan_eln = lan_eln_df.loc[x_years].lan_eln.values\n",
    "\n",
    "# # Get custom x values\n",
    "# x_neg = np.where(x_soi >= 0, x_soi, np.nan)\n",
    "# x_pos = np.where(x_soi <= 0, x_soi, np.nan)\n",
    "# x_lan = np.where(x_lan_eln == 'LaN', x_soi, np.nan)\n",
    "# x_eln = np.where(x_lan_eln == 'ElN', x_soi, np.nan)\n",
    "\n",
    "# # Compute change rates\n",
    "rate_out = points_gdf.apply(lambda x: change_regress(x, x_vals = x_years, x_labels = x_years, std_dev=3), axis=1)\n",
    "soi_out = points_gdf.apply(lambda x: change_regress(x, x_vals = x_soi, x_labels = x_years, std_dev=3), axis=1)\n",
    "iod_out = points_gdf.apply(lambda x: change_regress(x, x_vals = x_iod, x_labels = x_years, std_dev=3), axis=1)\n",
    "# pos_out = points_gdf.apply(lambda x: change_regress(x, x_vals = x_pos, x_labels = x_years, std_dev=3), axis=1)\n",
    "# eln_out = points_gdf.apply(lambda x: change_regress(x, x_vals = x_eln, x_labels = x_years, std_dev=3), axis=1)\n",
    "# lan_out = points_gdf.apply(lambda x: change_regress(x, x_vals = x_lan, x_labels = x_years, std_dev=3), axis=1)\n",
    "points_gdf[['mov_rate', 'mov_sig', 'mov_outl']] = rate_out\n",
    "points_gdf[['soi_rate', 'soi_sig', 'soi_outl']] = soi_out\n",
    "points_gdf[['iod_rate', 'iod_sig', 'iod_outl']] = iod_out\n",
    "# points_gdf[['pos_rate', 'pos_sig', 'pos_outl']] = pos_out\n",
    "# points_gdf[['eln_rate', 'eln_sig', 'eln_outl']] = eln_out\n",
    "# points_gdf[['lan_rate', 'lan_sig', 'lan_outl']] = lan_out\n",
    "\n",
    "# # Set insignificant rates to nan\n",
    "points_gdf.loc[points_gdf.mov_sig > 0.05, 'mov_rate'] = np.nan\n",
    "points_gdf.loc[points_gdf.soi_sig > 0.05, 'soi_rate'] = np.nan\n",
    "points_gdf.loc[points_gdf.iod_sig > 0.05, 'iod_rate'] = np.nan\n",
    "# points_gdf.loc[points_gdf.pos_sig > 0.05, 'pos_rate'] = np.nan\n",
    "# points_gdf.loc[points_gdf.eln_sig > 0.05, 'eln_rate'] = np.nan\n",
    "# points_gdf.loc[points_gdf.lan_sig > 0.05, 'lan_rate'] = np.nan\n",
    "\n",
    "# # Set CRS\n",
    "points_gdf.crs = baseline_array.crs\n",
    "\n",
    "# # Sort by descending absolute value and export\n",
    "# points_gdf.reindex(points_gdf.mov_rate.abs().sort_values().index).to_file(baseline_points_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_iod.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f'output_data/{study_area}/{study_area}_stats_{water_index}_{index_threshold}.geojson'\n",
    "(points_gdf.reindex(points_gdf.mov_rate.abs().sort_values().index)\n",
    " .to_file(output_filename, driver='GeoJSON'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`sentinel 2`, :index:`dea_plotting`, :index:`rgb`, :index:`virtual products`, :index:`NDVI`, :index:`tasseled cap`, :index:`cloud masking`, :index:`dask`, :index:`image compositing`, :index:`statistics`, :index:`pixel quality`, :index:`combining data`, :index:`native load`, :index:`reprojecting`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
