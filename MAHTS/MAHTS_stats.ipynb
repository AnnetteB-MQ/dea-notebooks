{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHTS stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# !pip install --user ruptures\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import ruptures as rpt\n",
    "from scipy import stats\n",
    "from affine import Affine\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import nearest_points\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import array_bounds\n",
    "from skimage.morphology import disk\n",
    "from skimage.morphology import square\n",
    "from skimage.morphology import binary_opening\n",
    "from skimage.morphology import binary_dilation\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_spatialtools import largest_region\n",
    "from dea_spatialtools import subpixel_contours\n",
    "from dea_spatialtools import largest_region\n",
    "from dea_plotting import map_shapefile\n",
    "\n",
    "def change_regress(row, \n",
    "                   x_vals, \n",
    "                   x_labels, \n",
    "                   std_dev=3, \n",
    "                   detrend_params=None,\n",
    "                   slope_var='slope', \n",
    "                   interc_var='intercept',\n",
    "                   pvalue_var='pvalue', \n",
    "                   outliers_var='outliers'):\n",
    "    \n",
    "    # Extract x (time) and y (distance) values\n",
    "    x = x_vals\n",
    "    y = row.values.astype(np.float)\n",
    "    \n",
    "    # Drop NAN rows\n",
    "    xy_df = np.vstack([x, y]).T\n",
    "    is_valid = ~np.isnan(xy_df).any(axis=1)\n",
    "    xy_df = xy_df[is_valid]\n",
    "    valid_labels = x_labels[is_valid]\n",
    "    \n",
    "    # If detrending parameters are provided, apply these to the data to\n",
    "    # remove the trend prior to running the regression\n",
    "    if detrend_params:\n",
    "        xy_df[:,1] = xy_df[:,1]-(detrend_params[0]*xy_df[:,0]+detrend_params[1])    \n",
    "    \n",
    "    # Remove outliers\n",
    "    outlier_bool = (np.abs(stats.zscore(xy_df)) < float(std_dev)).all(axis=1)\n",
    "    xy_df = xy_df[outlier_bool]\n",
    "        \n",
    "    # Compute linear regression\n",
    "    lin_reg = stats.linregress(x=xy_df[:,0], \n",
    "                               y=xy_df[:,1])  \n",
    "       \n",
    "    # Return slope, p-values and list of outlier years excluded from regression   \n",
    "    return pd.Series({slope_var: np.round(lin_reg.slope, 3), \n",
    "                      interc_var: np.round(lin_reg.intercept, 3),\n",
    "                      pvalue_var: np.round(lin_reg.pvalue, 3),\n",
    "                      outliers_var: ' '.join(map(str, valid_labels[~outlier_bool]))})\n",
    "\n",
    "\n",
    "def breakpoints(x, labels, model='rbf', pen=10, min_size=2, jump=1):\n",
    "    '''\n",
    "    Takes an array of erosion values, and returns a list of \n",
    "    breakpoint years\n",
    "    '''\n",
    "    signal = x.values\n",
    "    algo = rpt.Pelt(model=model, min_size=min_size, jump=jump).fit(signal)\n",
    "    result = algo.predict(pen=pen)\n",
    "    if len(result) > 1:\n",
    "        return [labels[i] for i in result[0:-1]][0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def mask_ocean(bool_array, connectivity=1):\n",
    "    '''\n",
    "    Identifies ocean by selecting the largest connected area of water\n",
    "    pixels, then dilating this region by 1 pixel to include mixed pixels\n",
    "    '''\n",
    "    \n",
    "    ocean_mask = largest_region(bool_array, connectivity=connectivity)\n",
    "    ocean_mask = binary_dilation(ocean_mask, selem=square(3))\n",
    "\n",
    "    return ocean_mask\n",
    "\n",
    "\n",
    "def rocky_shores_buffer(smartline, buffer=50):\n",
    "\n",
    "    to_keep = (\n",
    "               'Bedrock breakdown debris (cobbles/boulders)',\n",
    "               'Boulder (rock) beach',\n",
    "               'Cliff (>5m) (undiff)',\n",
    "               'Colluvium (talus) undiff',\n",
    "               'Flat boulder deposit (rock) undiff',\n",
    "               'Hard bedrock shore',\n",
    "               'Hard bedrock shore inferred',\n",
    "               'Hard rock cliff (>5m)',\n",
    "               'Hard rocky shore platform',\n",
    "               'Rocky shore (undiff)',\n",
    "               'Rocky shore platform (undiff)',\n",
    "               'Sloping hard rock shore',\n",
    "               'Sloping rocky shore (undiff)',\n",
    "               'Soft `bedrockÂ¿ cliff (>5m)',\n",
    "               'Steep boulder talus',\n",
    "               'Hard rocky shore platform'\n",
    "    )\n",
    "\n",
    "    # Extract rocky vs non-rocky\n",
    "    rocky_gdf = smartline_gdf[smartline_gdf.INTERTD1_V.isin(to_keep)].copy()\n",
    "    nonrocky_gdf = smartline_gdf[~smartline_gdf.INTERTD1_V.isin(to_keep)].copy()\n",
    "    \n",
    "    # Buffer both features\n",
    "    rocky_gdf['geometry'] = rocky_gdf.buffer(buffer)\n",
    "    nonrocky_gdf['geometry'] = nonrocky_gdf.buffer(buffer)\n",
    "    \n",
    "    return gpd.overlay(rocky_gdf, nonrocky_gdf, how='difference')\n",
    "\n",
    "\n",
    "# # This will speed up loading data\n",
    "# import datacube.utils.rio\n",
    "# datacube.utils.rio.configure_s3_access(aws_unsigned=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:         (x: 2203, y: 2206, year: 31)\n",
       "Coordinates:\n",
       "  * y               (y) float64 -2.2e+06 -2.2e+06 ... -2.266e+06 -2.266e+06\n",
       "  * x               (x) float64 7.516e+05 7.516e+05 ... 8.176e+05 8.176e+05\n",
       "  * year            (year) int64 1988 1989 1990 1991 ... 2015 2016 2017 2018\n",
       "Data variables:\n",
       "    mndwi           (year, y, x) float32 0.7960993 0.80552214 ... -0.5042683\n",
       "    gapfill_index   (year, y, x) float32 0.8409425 0.8057803 ... -0.51529694\n",
       "    gapfill_tide_m  (year, y, x) float32 -0.25103498 -0.25098908 ... -0.701527\n",
       "    gapfill_count   (year, y, x) int16 31 30 30 31 31 30 ... 46 46 46 46 47 46\n",
       "    stdev           (year, y, x) float32 0.13297957 0.13826987 ... 0.015576812\n",
       "    tide_m          (year, y, x) float32 -0.47693783 -0.4768318 ... -0.5407289\n",
       "    count           (year, y, x) int16 13 13 13 13 13 13 ... 16 16 16 16 18 17\n",
       "Attributes:\n",
       "    crs:        +init=epsg:32650\n",
       "    transform:  | 30.00, 0.00, 751575.00|\\n| 0.00,-30.00,-2200215.00|\\n| 0.00...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:         (x: 2203, y: 2206, year: 31)\n",
       "Coordinates:\n",
       "  * y               (y) float64 -2.2e+06 -2.2e+06 ... -2.266e+06 -2.266e+06\n",
       "  * x               (x) float64 7.516e+05 7.516e+05 ... 8.176e+05 8.176e+05\n",
       "  * year            (year) int64 1988 1989 1990 1991 ... 2015 2016 2017 2018\n",
       "Data variables:\n",
       "    mndwi           (year, y, x) float32 0.7960993 0.80552214 ... -0.5042683\n",
       "    gapfill_index   (year, y, x) float32 0.8409425 0.8057803 ... -0.51529694\n",
       "    gapfill_tide_m  (year, y, x) float32 -0.25103498 -0.25098908 ... -0.701527\n",
       "    gapfill_count   (year, y, x) int16 31 30 30 31 31 30 ... 46 46 46 46 47 46\n",
       "    stdev           (year, y, x) float32 0.13297957 0.13826987 ... 0.015576812\n",
       "    tide_m          (year, y, x) float32 -0.47693783 -0.4768318 ... -0.5407289\n",
       "    count           (year, y, x) int16 13 13 13 13 13 13 ... 16 16 16 16 18 17\n",
       "Attributes:\n",
       "    crs:        +init=epsg:32650\n",
       "    transform:  | 30.00, 0.00, 751575.00|\\n| 0.00,-30.00,-2200215.00|\\n| 0.00..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in contours\n",
    "study_area = 1677\n",
    "water_index = 'mndwi'\n",
    "index_threshold = 0.00\n",
    "\n",
    "# Create output folder\n",
    "output_dir = f'output_data/{study_area}/vectors/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get file paths\n",
    "gapfill_index_files = sorted(glob.glob(f'output_data/{study_area}/gapfill_{water_index}_*.tif'))\n",
    "gapfill_tide_files = sorted(glob.glob(f'output_data/{study_area}/gapfill_tide_m_*.tif'))\n",
    "gapfill_count_files = sorted(glob.glob(f'output_data/{study_area}/gapfill_count_*.tif'))\n",
    "index_files = sorted(glob.glob(f'output_data/{study_area}/{water_index}_*.tif'))[1:len(gapfill_index_files)+1]\n",
    "stdev_files = sorted(glob.glob(f'output_data/{study_area}/stdev_*.tif'))[1:len(gapfill_index_files)+1]\n",
    "tidem_files = sorted(glob.glob(f'output_data/{study_area}/tide_m_*.tif'))[1:len(gapfill_index_files)+1]\n",
    "count_files = sorted(glob.glob(f'output_data/{study_area}/count_*.tif'))[1:len(gapfill_index_files)+1]\n",
    "\n",
    "# Create variable used for time axis\n",
    "time_var = xr.Variable('year', [int(i[-8:-4]) for i in index_files])\n",
    "\n",
    "# Import data\n",
    "index_da = xr.concat([xr.open_rasterio(i) for i in index_files], dim=time_var)\n",
    "gapfill_index_da = xr.concat([xr.open_rasterio(i) for i in gapfill_index_files], dim=time_var)\n",
    "gapfill_tide_da = xr.concat([xr.open_rasterio(i) for i in gapfill_tide_files], dim=time_var)\n",
    "gapfill_count_da = xr.concat([xr.open_rasterio(i) for i in gapfill_count_files], dim=time_var)\n",
    "stdev_da = xr.concat([xr.open_rasterio(i) for i in stdev_files], dim=time_var)\n",
    "tidem_da = xr.concat([xr.open_rasterio(i) for i in tidem_files], dim=time_var)\n",
    "count_da = xr.concat([xr.open_rasterio(i) for i in count_files], dim=time_var)\n",
    "\n",
    "# Assign names to allow merge\n",
    "index_da.name = water_index\n",
    "gapfill_index_da.name = 'gapfill_index'\n",
    "gapfill_tide_da.name = 'gapfill_tide_m'\n",
    "gapfill_count_da.name = 'gapfill_count'\n",
    "stdev_da.name = 'stdev'\n",
    "tidem_da.name = 'tide_m'\n",
    "count_da.name = 'count'\n",
    "\n",
    "# Combine into a single dataset and set CRS\n",
    "yearly_ds = xr.merge([index_da, gapfill_index_da, gapfill_tide_da, \n",
    "                      gapfill_count_da, stdev_da, tidem_da, count_da]).squeeze('band', drop=True)\n",
    "yearly_ds.attrs['crs'] = index_da.crs\n",
    "yearly_ds.attrs['transform'] = Affine(*index_da.transform)\n",
    "\n",
    "# Print\n",
    "yearly_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bounding box to load data for\n",
    "bbox = gpd.GeoSeries(box(*array_bounds(height=yearly_ds.sizes['y'], \n",
    "                                       width=yearly_ds.sizes['x'], \n",
    "                                       transform=yearly_ds.transform)), \n",
    "                     crs=yearly_ds.crs)\n",
    "\n",
    "# Study area polygon\n",
    "comp_gdf = (gpd.read_file('input_data/50km_albers_grid.shp', bbox=bbox)\n",
    "            .set_index('id')\n",
    "            .to_crs(str(yearly_ds.crs)))\n",
    "\n",
    "# Estaury mask\n",
    "estuary_gdf = (gpd.read_file('input_data/estuary_mask.shp', bbox=bbox)\n",
    "               .to_crs(yearly_ds.crs))\n",
    "\n",
    "# Rocky shore mask\n",
    "smartline_gdf = (gpd.read_file('input_data/Smartline.gdb', bbox=bbox)\n",
    "                 .to_crs(yearly_ds.crs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract shoreline contours\n",
    "\n",
    "### Extract ocean-masked contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract study area poly\n",
    "study_area_poly = comp_gdf.loc[study_area]\n",
    "\n",
    "# Remove low obs and high variance pixels and replace with 3-year gapfill\n",
    "gapfill_mask = (yearly_ds['count'] > 5)   #& (yearly_ds['stdev'] < 0.5)\n",
    "yearly_ds[water_index] = yearly_ds[water_index].where(gapfill_mask, other=yearly_ds.gapfill_index)\n",
    "yearly_ds['tide_m'] = yearly_ds['tide_m'].where(gapfill_mask, other=yearly_ds.gapfill_tide_m)\n",
    "yearly_ds['count'] = yearly_ds['count'].where(gapfill_mask, other=yearly_ds.gapfill_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply water index threshold\n",
    "thresholded_ds = (yearly_ds[water_index] > index_threshold)\n",
    "thresholded_ds = thresholded_ds.where(~yearly_ds[water_index].isnull())\n",
    "\n",
    "# Rasterize estuary polygons into a numpy mask. The try-except catches cases\n",
    "# where no estuary polygons exist in the study area\n",
    "try:\n",
    "    estuary_mask = rasterize(shapes=estuary_gdf['geometry'],\n",
    "                             out_shape=yearly_ds[water_index].shape[1:],\n",
    "                             transform=yearly_ds.transform,\n",
    "                             all_touched=True).astype(bool)\n",
    "except:\n",
    "    estuary_mask = np.full(yearly_ds[water_index].shape[1:], False, dtype=bool)\n",
    "\n",
    "# Drop empty timesteps and apply estuary mask\n",
    "thresholded_ds = (thresholded_ds\n",
    "                  .sel(year=thresholded_ds.sum(dim=['x', 'y']) > 0)\n",
    "                  .where(~estuary_mask, 0))\n",
    "\n",
    "# Identify ocean by identifying the largest connected area of water pixels\n",
    "# as water in at least 90% of the entire stack of thresholded data\n",
    "all_time_median = (thresholded_ds.mean(dim='year') > 0.9)\n",
    "full_sea_mask = mask_ocean(binary_opening(all_time_median, disk(3)))\n",
    "\n",
    "# Generate all time 750 m buffer from ocean-land boundary\n",
    "buffer_ocean = binary_dilation(full_sea_mask, disk(25))\n",
    "buffer_land = binary_dilation(~full_sea_mask, disk(25))\n",
    "coastal_buffer = buffer_ocean & buffer_land\n",
    "\n",
    "# # Generate sea mask for each timestep\n",
    "yearly_sea_mask = thresholded_ds.groupby('year').apply(mask_ocean)\n",
    "\n",
    "# Keep only pixels that are within 750 m of the ocean in the\n",
    "# full stack, and directly connected to ocean in each yearly timestep\n",
    "masked_ds = yearly_ds[water_index].where(yearly_sea_mask & coastal_buffer)\n",
    "\n",
    "# Restrict to study area polygon\n",
    "# masked_ds = masked_ds.where(study_area_mask)\n",
    "\n",
    "# Set CRS and trasnform from input data\n",
    "masked_ds.attrs['crs'] = yearly_ds.crs[6:]\n",
    "masked_ds.attrs['transform'] = yearly_ds.transform\n",
    "\n",
    "# Extract contours\n",
    "contours_gdf = subpixel_contours(da=masked_ds,\n",
    "                                 z_values=index_threshold,\n",
    "                                 min_vertices=10,\n",
    "                                 dim='year')\n",
    "\n",
    "# Plot\n",
    "# contours_gdf.plot(column='year', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;count&#x27; (y: 2206, x: 2203)&gt;\n",
       "array([[6, 6, 6, ..., 6, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6],\n",
       "       ...,\n",
       "       [6, 6, 6, ..., 8, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6]], dtype=int16)\n",
       "Coordinates:\n",
       "  * y        (y) float64 -2.2e+06 -2.2e+06 -2.2e+06 ... -2.266e+06 -2.266e+06\n",
       "  * x        (x) float64 7.516e+05 7.516e+05 7.516e+05 ... 8.176e+05 8.176e+05\n",
       "Attributes:\n",
       "    transform:      (30.0, 0.0, 751575.0, 0.0, -30.0, -2200215.0)\n",
       "    crs:            +init=epsg:32650\n",
       "    res:            (30.0, 30.0)\n",
       "    is_tiled:       1\n",
       "    nodatavals:     (-999.0,)\n",
       "    scales:         (1.0,)\n",
       "    offsets:        (0.0,)\n",
       "    AREA_OR_POINT:  Area</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'count' (y: 2206, x: 2203)>\n",
       "array([[6, 6, 6, ..., 6, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6],\n",
       "       ...,\n",
       "       [6, 6, 6, ..., 8, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6],\n",
       "       [6, 6, 6, ..., 6, 6, 6]], dtype=int16)\n",
       "Coordinates:\n",
       "  * y        (y) float64 -2.2e+06 -2.2e+06 -2.2e+06 ... -2.266e+06 -2.266e+06\n",
       "  * x        (x) float64 7.516e+05 7.516e+05 7.516e+05 ... 8.176e+05 8.176e+05\n",
       "Attributes:\n",
       "    transform:      (30.0, 0.0, 751575.0, 0.0, -30.0, -2200215.0)\n",
       "    crs:            +init=epsg:32650\n",
       "    res:            (30.0, 30.0)\n",
       "    is_tiled:       1\n",
       "    nodatavals:     (-999.0,)\n",
       "    scales:         (1.0,)\n",
       "    offsets:        (0.0,)\n",
       "    AREA_OR_POINT:  Area"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yearly_ds['count'].min(dim='year', keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.helpers import write_geotiff\n",
    "write_geotiff(filename=f'test_count.tif', \n",
    "              dataset=yearly_ds[['count']].min(dim='year', keep_attrs=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartline_gdf = gpd.read_file('input_data/Smartline.gdb', \n",
    "                                           bbox=bbox).to_crs(yearly_ds.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartline_gdf = gpd.GeoDataFrame.from_file('input_data/Smartline.gdb', \n",
    "                                           bbox=bbox).to_crs(yearly_ds.crs)\n",
    "\n",
    "\n",
    "\n",
    "rocky_shore_buffer = rocky_shores_buffer(smartline=smartline_gdf, buffer=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dea_spatialtools import xr_rasterize\n",
    "# xr_rasterize(gdf=rocky_shores_buffer(smartline=smartline_gdf, buffer=50),\n",
    "#              da=yearly_ds[water_index].isel(year=0), \n",
    "#              crs=yearly_ds.crs, \n",
    "#              transform=yearly_ds.transform).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics\n",
    "### Measure distances from baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get array of water index values for baseline time period \n",
    "baseline_year = '2018'\n",
    "baseline_array = yearly_ds[water_index].sel(year=int(baseline_year))\n",
    "\n",
    "# Import contours and project to local CRS\n",
    "# contours_gdf = contours_clean_gdf\n",
    "contours_index_gdf = contours_gdf.set_index('year')\n",
    "\n",
    "# Set annual shoreline to use as a baseline\n",
    "baseline_contour = contours_index_gdf.loc[[baseline_year]].geometry\n",
    "\n",
    "# Generate points along line and convert to geopandas.GeoDataFrame\n",
    "points_line = [baseline_contour.iloc[0].interpolate(i) \n",
    "               for i in range(0, int(baseline_contour.length), 30)]\n",
    "points_gdf = gpd.GeoDataFrame(geometry=points_line, crs=baseline_array.crs)\n",
    "\n",
    "# Make a copy of the GeoDataFrame to hold tidal data\n",
    "tide_points_gdf = points_gdf.copy()\n",
    "# gapfill_points_gdf = points_gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy geometry to baseline point\n",
    "points_gdf['p_baseline'] = points_gdf.geometry\n",
    "baseline_x_vals = points_gdf.geometry.x\n",
    "baseline_y_vals = points_gdf.geometry.y\n",
    "\n",
    "# Iterate through all comparison years in contour gdf\n",
    "for comp_year in contours_index_gdf.index.unique().values[0:32]:\n",
    "\n",
    "    print(comp_year)\n",
    "\n",
    "    # Set comparison contour\n",
    "    comp_contour = contours_index_gdf.loc[[comp_year]].geometry.iloc[0]\n",
    "\n",
    "    # Find nearest point on comparison contour\n",
    "    points_gdf[f'p_{comp_year}'] = points_gdf.apply(lambda x: \n",
    "                                                    nearest_points(x.p_baseline, comp_contour)[1], axis=1)\n",
    "\n",
    "    # Compute distance between baseline and comparison year points\n",
    "    points_gdf[f'{comp_year}'] = points_gdf.apply(lambda x: \n",
    "                                                  x.geometry.distance(x[f'p_{comp_year}']), axis=1)\n",
    "\n",
    "    # Extract comparison array\n",
    "    comp_array = yearly_ds[water_index].sel(year=int(comp_year))\n",
    "\n",
    "    # Convert baseline and comparison year points to geoseries to allow easy access to x and y coords\n",
    "    comp_x_vals = gpd.GeoSeries(points_gdf[f'p_{comp_year}']).x\n",
    "    comp_y_vals = gpd.GeoSeries(points_gdf[f'p_{comp_year}']).y\n",
    "\n",
    "    # Sample NDWI values from arrays based on baseline and comparison points\n",
    "    baseline_x_vals = xr.DataArray(baseline_x_vals, dims='z')\n",
    "    baseline_y_vals = xr.DataArray(baseline_y_vals, dims='z')\n",
    "    comp_x_vals = xr.DataArray(comp_x_vals, dims='z')\n",
    "    comp_y_vals = xr.DataArray(comp_y_vals, dims='z')   \n",
    "    points_gdf['index_comp_p1'] = comp_array.interp(x=baseline_x_vals, y=baseline_y_vals)\n",
    "    points_gdf['index_baseline_p2'] = baseline_array.interp(x=comp_x_vals, y=comp_y_vals)\n",
    "\n",
    "    # Compute directionality of change (negative = erosion, positive = accretion)    \n",
    "    points_gdf['loss_gain'] = np.where(points_gdf.index_baseline_p2 > \n",
    "                                       points_gdf.index_comp_p1, 1, -1)\n",
    "    points_gdf[f'{comp_year}'] = points_gdf[f'{comp_year}'] * points_gdf.loss_gain\n",
    "    \n",
    "    # Add tide data\n",
    "    tide_array = yearly_ds['tide_m'].sel(year=int(comp_year))\n",
    "    tide_points_gdf[f'{comp_year}'] = tide_array.interp(x=baseline_x_vals, y=baseline_y_vals)\n",
    "    \n",
    "#     # Add gapfill data\n",
    "#     gapfill_array = gapfill_mask.sel(year=int(comp_year))\n",
    "#     gapfill_points_gdf[f'{comp_year}'] = gapfill_array.astype(int).interp(x=comp_x_vals, \n",
    "#                                                                           y=comp_y_vals, \n",
    "#                                                                           method='nearest')\n",
    "    \n",
    "# Keep required columns\n",
    "points_gdf = points_gdf[['geometry'] + \n",
    "                        contours_index_gdf.index.unique().values.tolist()]\n",
    "points_gdf = points_gdf.round(2)\n",
    "\n",
    "# Zero values to 1988\n",
    "points_gdf.iloc[:,1:] = points_gdf.iloc[:,1:].subtract(points_gdf['1988'], axis=0)\n",
    "\n",
    "# Identify dates for regression\n",
    "x_years = np.array([int(i[:4]) for i in points_gdf.columns[1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify SOI values for regression\n",
    "climate_df = pd.read_csv('input_data/climate_indices.csv', index_col='year')\n",
    "climate_df = climate_df.loc[x_years,:]\n",
    "\n",
    "# Compute change rates\n",
    "rate_out = points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=x, \n",
    "                                                     x_vals = x_years, \n",
    "                                                     x_labels = x_years, \n",
    "                                                     std_dev=2), axis=1)\n",
    "points_gdf[['rate_time', 'incpt_time', 'sig_time', 'outl_time']] = rate_out\n",
    "\n",
    "\n",
    "# Compute tide flag\n",
    "# tide_out = tide_points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=x, \n",
    "#                                                x_vals = x_years, \n",
    "#                                                x_labels = x_years, \n",
    "#                                                std_dev=3), axis=1)\n",
    "tide_out = tide_points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=points_gdf[x_years.astype(str)].iloc[x.name], \n",
    "                                               x_vals=x, \n",
    "                                               x_labels=x_years, \n",
    "                                               std_dev=2), axis=1)\n",
    "points_gdf[['rate_tide', 'incpt_tide', 'sig_tide', 'outl_tide']] = tide_out \n",
    "\n",
    "\n",
    "# Compute stats for each index\n",
    "for ci in climate_df:\n",
    "    \n",
    "    print(ci)\n",
    "    \n",
    "    # Compute stats for each row\n",
    "    ci_out = points_gdf[x_years.astype(str)].apply(lambda x: change_regress(row=x,\n",
    "                                                       x_vals = climate_df[ci].values, \n",
    "                                                       x_labels = x_years, \n",
    "#                                                        detrend_params=[x.rate_time, x.incpt_time],\n",
    "                                                       std_dev=3), axis=1)\n",
    "    \n",
    "    # Add data as columns  \n",
    "    points_gdf[[f'rate_{ci}', f'incpt_{ci}', f'sig_{ci}', f'outl_{ci}']] = ci_out\n",
    "\n",
    "\n",
    "# # Add breakpoints\n",
    "# print('Identifying breakpoints')\n",
    "# points_gdf['breakpoint'] = points_gdf.apply(lambda x: breakpoints(x=x[x_years.astype(str)], \n",
    "#                                                                   labels=x_years, \n",
    "#                                                                   pen=10), axis=1)\n",
    "\n",
    "# Set CRS\n",
    "points_gdf.crs = baseline_array.crs\n",
    "\n",
    "# Custom sorting\n",
    "points_towrite = points_gdf.loc[:, [\n",
    "    'rate_time', 'rate_SOI', 'rate_IOD', 'rate_SAM', 'rate_IPO', 'rate_PDO', 'rate_tide',\n",
    "    'sig_time', 'sig_SOI', 'sig_IOD', 'sig_SAM', 'sig_IPO', 'sig_PDO', 'sig_tide',\n",
    "    'outl_time', 'outl_SOI', 'outl_IOD', 'outl_SAM', 'outl_IPO', 'outl_PDO', 'outl_tide',\n",
    "#     'breakpoint', \n",
    "    *x_years.astype(str).tolist(), 'geometry'\n",
    "]]\n",
    "\n",
    "# Export\n",
    "stats_path = f'output_data/{study_area}/vectors/{study_area}_stats_{water_index}_{index_threshold}'\n",
    "points_towrite.to_file(f'{stats_path}.geojson', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapefile package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = 'msl'\n",
    "\n",
    "# Clip points to extent of polygon\n",
    "points_gdf = points_gdf[points_gdf.intersects(study_area_poly['geometry'])]\n",
    "points_gdf.to_file(f'{stats_path}.geojson', driver='GeoJSON')\n",
    "\n",
    "# Overwrite contours after clipping to study area\n",
    "contours_gdf['geometry'] = contours_gdf.intersection(study_area_poly['geometry'])\n",
    "contours_gdf.to_file(f'{contour_path}.geojson', driver='GeoJSON')\n",
    "\n",
    "# Export as shapefile\n",
    "contours_gdf.to_file(f'{contour_path}.shp')\n",
    "points_towrite.to_file(f'{stats_path}.shp')\n",
    "\n",
    "shutil.make_archive(base_name=f'output_data/outputs_{study_area}_{suffix}', \n",
    "                    format='zip', \n",
    "                    root_dir=f'output_data/{study_area}/vectors/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_towrite[points_towrite.intersects(study_area_poly['geometry'])].to_file(f'{stats_path}2.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Time history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = points_gdf[x_years.astype(str)].iloc[[5129]] \n",
    "\n",
    "test.apply(lambda x: change_regress(row=x,\n",
    "                                    x_vals = x_years, \n",
    "                                    x_labels = x_years, \n",
    "                                    std_dev=2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_val = 5129\n",
    "plot_df = pd.DataFrame({\n",
    "    'mov': points_gdf.loc[id_val, contours_gdf.year.to_list()].values.astype(float),\n",
    "    'time': x_years,\n",
    "    'soi': climate_df['SOI'].values,\n",
    "    'tide': tide_points_gdf.mean(axis=0),\n",
    "})\n",
    "\n",
    "plot_df.plot.scatter(x='time',\n",
    "                     y='mov',\n",
    "                     c='soi',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")\n",
    "\n",
    "plot_df.plot.scatter(x='soi',\n",
    "                     y='mov',\n",
    "                     c='soi',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")\n",
    "\n",
    "plot_df.plot.scatter(x='tide',\n",
    "                     y='mov',\n",
    "                     c='tide',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.plot.scatter(x='time',\n",
    "                     y='tide',\n",
    "                     c='tide',\n",
    "                     cmap='RdYlBu',\n",
    "                     s=50,\n",
    "                     edgecolors=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test breakpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_val = 6015\n",
    "# signal = points_gdf.loc[id_val,contours_gdf.index.to_list()].values\n",
    "\n",
    "# # detection\n",
    "# algo = rpt.Pelt(model=\"rbf\", min_size=2, jump=1).fit(signal)\n",
    "# result = algo.predict(pen=8)\n",
    "# print(contours_gdf.index.to_list()[result[0]])\n",
    "# rpt.display(signal, [32], result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = climate_df.corr()\n",
    "# corr.style.background_gradient(cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "\n",
    "# Use the tidal model to compute tide heights for each observation:\n",
    "obs_datetimes = [datetime.datetime(1986, 8, 23, 1, 36, 23),\n",
    "                 datetime.datetime(1987, 5, 29, 1, 44, 59)]\n",
    "obs_timepoints = [TimePoint(115.35, -20.86, dt) \n",
    "                  for dt in obs_datetimes]\n",
    "obs_predictedtides = predict_tide(obs_timepoints)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "comp_gdf = gpd.read_file('input_data/item_polygons.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(env='c3-samples')\n",
    "\n",
    "from dea_datahandling import load_ard\n",
    "from dea_coastaltools import tidal_tag\n",
    "\n",
    "\n",
    "def tidal_sync(row):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "    #     lon, lat = row.geometry.centroid.coords.xy\n",
    "        lon = row.lon\n",
    "        lat = row.lat\n",
    "\n",
    "        # Load available data from all three Landsat satellites\n",
    "        ds = load_ard(dc=dc,\n",
    "                      products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "                      x=(lon - 0.01, lon + 0.01),\n",
    "                      y=(lat - 0.01, lat + 0.01),\n",
    "                      time=('1988', '2018'),\n",
    "                      measurements=['nbart_green'],\n",
    "                      output_crs='EPSG:3577',\n",
    "                      gqa_iterative_mean_xy=[0, 1],\n",
    "                      cloud_cover=[0, 80],\n",
    "                      resolution=(-30, 30),\n",
    "                      group_by='solar_day',\n",
    "                      dask_chunks={})\n",
    "\n",
    "        ds = tidal_tag(ds=ds)\n",
    "\n",
    "\n",
    "        annual_ht_mean = ds.tide_height.sel(time = ds.tide_height > ds.tide_height.median()).resample(time='Y').mean()\n",
    "        annual_ht_stats = change_regress(row=annual_ht_mean,\n",
    "                       x_vals=annual_ht_mean.time.dt.year,\n",
    "                       x_labels=annual_ht_mean.time.dt.year,\n",
    "                       std_dev=5,\n",
    "                       detrend_params=None,\n",
    "                       slope_var='slope',\n",
    "                       interc_var='intercept',\n",
    "                       pvalue_var='pvalue',\n",
    "                       outliers_var='outliers')\n",
    "\n",
    "        print(annual_ht_stats)\n",
    "        return(row.append(annual_ht_stats))\n",
    "    \n",
    "    except:\n",
    "        print('Failed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = comp_gdf.iloc[200:].apply(lambda x: tidal_sync(x), axis=1)\n",
    "# test[~test.ID.isna()].to_file('tide_sync_test_3.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U --user --extra-index-url=\"https://packages.dea.ga.gov.au/\" odc-algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='Intertidal_elevation', env='c3-samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(env='c3-samples')\n",
    "from dea_datahandling import load_ard\n",
    "\n",
    "try:\n",
    "    dc = datacube.Datacube(app='Intertidal_elevation', env='c3-samples')\n",
    "except:\n",
    "    dc = datacube.Datacube(app='Intertidal_elevation')\n",
    "\n",
    "lat, lon = -20.58, 117.87\n",
    "\n",
    "# Load available data from all three Landsat satellites\n",
    "ds = load_ard(dc=dc,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "              x=(lon - 0.01, lon + 0.01),\n",
    "              y=(lat - 0.01, lat + 0.01),\n",
    "              time=('1988', '2018'),\n",
    "              measurements=['nbart_green'],\n",
    "              output_crs='EPSG:3577',\n",
    "              resolution=(-30, 30),\n",
    "              group_by='solar_day',\n",
    "              dask_chunks={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "dc = datacube.Datacube(env='c3-samples')\n",
    "\n",
    "from dea_datahandling import load_ard\n",
    "from dea_coastaltools import tidal_tag\n",
    "from dea_coastaltools import tidal_stats\n",
    "\n",
    "\n",
    "#     lon, lat = row.geometry.centroid.coords.xy\n",
    "lat, lon = -15.99, 137.21\n",
    "\n",
    "# Load available data from all three Landsat satellites\n",
    "ds = load_ard(dc=dc,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "              x=(lon - 0.01, lon + 0.01),\n",
    "              y=(lat - 0.01, lat + 0.01),\n",
    "              time=('1988', '2018'),\n",
    "              measurements=['nbart_green'],\n",
    "              output_crs='EPSG:3577',\n",
    "              gqa_iterative_mean_xy=[0, 1],\n",
    "              cloud_cover=[0, 80],\n",
    "              resolution=(-30, 30),\n",
    "              group_by='solar_day',\n",
    "              dask_chunks={})\n",
    "\n",
    "\n",
    "stats, observed_tides, all_tides = tidal_stats(ds=ds, return_tides=True, modelled_freq='30T')\n",
    "\n",
    "# print(annual_ht_stats)\n",
    "# return(row.append(annual_ht_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tides.quantile([0.4, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, = dc.find_datasets(product='ga_ls5t_ard_3', limit=1)\n",
    "dir(ds.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_diff = (all_tides.max() - all_tides.min()) * 0.15\n",
    "tide_min = 0 - tide_diff\n",
    "tide_max = 0 + tide_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_tides[observed_tides.tide_height.between(tide_min.item(), tide_max.item())].resample('1Y').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tides.loc[slice('2018-06-04', '2018-07-05')].plot(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tides.loc[slice('2018-12-01', '2018-12-02')].plot(figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_buffer = min(((observed_tides.tide_height.max() - observed_tides.tide_height.min()) * 0.15), 1.0)\n",
    "tide_cutoff_min = 0.5-tide_buffer\n",
    "tide_cutoff_max = 0.5+tide_buffer\n",
    "\n",
    "\n",
    "observed_tides.plot()\n",
    "plt.axhline(tide_cutoff_min, color='red')\n",
    "plt.axhline(tide_cutoff_max, color='red')\n",
    "\n",
    "yearly_counts = observed_tides[(observed_tides.tide_height > tide_cutoff_min) & \n",
    "                               (observed_tides.tide_height < tide_cutoff_max)].resample('Y').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_buffer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "((ds.tide_height.max() - ds.tide_height.min()) * 0.25).clip(0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_counts.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds.tide_height.sel(time = ((ds.tide_height >= ds.tide_height.quantile(q=0.6)) & \n",
    "                                  (ds.tide_height <= ds.tide_height.quantile(q=0.9))))\n",
    "test.plot(size=8)\n",
    "test.resample(time='Y').mean().plot(linewidth=5, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.resample(time='Y').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_points_gdf.iloc[3057, 1:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = gpd.read_file('tide_sync_test_1.geojson')\n",
    "two = gpd.read_file('tide_sync_test_3.geojson')\n",
    "three = gpd.read_file('tide_sync_test_3.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.concat([one, two, three]).to_file('tide_sync_test.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[~test.ID.isna()].plot()\n",
    "\n",
    "# .to_file('tide_sync_test.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle            \n",
    "            \n",
    "with open(r\"someobject.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(test, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`sentinel 2`, :index:`dea_plotting`, :index:`rgb`, :index:`virtual products`, :index:`NDVI`, :index:`tasseled cap`, :index:`cloud masking`, :index:`dask`, :index:`image compositing`, :index:`statistics`, :index:`pixel quality`, :index:`combining data`, :index:`native load`, :index:`reprojecting`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
