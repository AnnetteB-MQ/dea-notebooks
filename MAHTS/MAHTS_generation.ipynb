{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHTS generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Take full time series of NDWI in dask\n",
    "* Compute 2D tides for every timestep\n",
    "* Compute median of tides from entire tide timeseries\n",
    "* For each year in dask NDWI timeseries:\n",
    "    * Mask pixels where tide > overall median\n",
    "    * `.compute()` and take median \n",
    "    \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import otps\n",
    "import datacube\n",
    "import shapely.wkt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils.geometry import Geometry, CRS\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.virtual import catalog_from_file\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_plotting import rgb\n",
    "from dea_plotting import display_map\n",
    "from dea_spatialtools import interpolate_2d\n",
    "from dea_datahandling import load_ard\n",
    "from dea_bandindices import calculate_indices\n",
    "from dea_datahandling import mostcommon_crs\n",
    "\n",
    "\n",
    "def get_geopoly(index, gdf):\n",
    "    '''\n",
    "    Selects a row from a geopandas.GeoDataFrame, and converts this\n",
    "    into a geopolygon feature as an input to dc.load\n",
    "    '''\n",
    "    return Geometry(geo=gdf.loc[index].geometry.__geo_interface__, \n",
    "                    crs=CRS(gdf.crs['init']))\n",
    "\n",
    "\n",
    "def interpolate_tide(timestep_ds, tidepoints_gdf, method='rbf', factor=20):    \n",
    "    '''\n",
    "    Extract a subset of tide modelling point data for a given time-step,\n",
    "    then interpolate these tides into the extent of the xarray dataset.\n",
    "    '''    \n",
    "  \n",
    "    # Extract subset of observations based on timestamp of imagery\n",
    "    time_string = str(timestep_ds.time.values)[0:19].replace('T', ' ')\n",
    "    tidepoints_subset = tidepoints_gdf.loc[time_string]\n",
    "    print(time_string, end='\\r')\n",
    "    \n",
    "    # Get lists of x, y and z (tide height) data to interpolate\n",
    "    x_coords = tidepoints_subset.geometry.x,\n",
    "    y_coords = tidepoints_subset.geometry.y,\n",
    "    z_coords = tidepoints_subset.tide_m\n",
    "    \n",
    "    # Interpolate tides into the extent of the satellite timestep\n",
    "    out_tide = interpolate_2d(ds=timestep_ds,\n",
    "                              x_coords=x_coords,\n",
    "                              y_coords=y_coords,\n",
    "                              z_coords=z_coords,\n",
    "                              method=method,\n",
    "                              factor=factor)\n",
    "    \n",
    "    # Return data as a Float32 to conserve memory\n",
    "    return out_tide.astype(np.float32)\n",
    "\n",
    "\n",
    "def load_tidal_subset(year_ds, tide_cutoff_min, tide_cutoff_max):\n",
    "    \n",
    "    # Print status\n",
    "    year = year_ds.time[0].dt.year.item()\n",
    "    print(f'Processing {year}')\n",
    "    \n",
    "    # Determine what pixels were acquired in selected tide range, and \n",
    "    # drop time-steps without any relevant pixels to reduce data to load\n",
    "    tide_bool = ((year_ds.tide_m >= tide_cutoff_min) & \n",
    "                 (year_ds.tide_m <= tide_cutoff_max))\n",
    "    year_ds = year_ds.sel(time=tide_bool.sum(dim=['x', 'y']) > 0)\n",
    "    \n",
    "    # Apply mask, and load in corresponding high tide data\n",
    "    year_ds = year_ds.where(tide_bool)\n",
    "    return year_ds.compute()\n",
    "\n",
    "    \n",
    "def tidal_composite(year_ds, \n",
    "                    label, \n",
    "                    label_dim, \n",
    "                    output_dir, \n",
    "                    output_prefix='',\n",
    "                    export_geotiff=False):\n",
    "    '''\n",
    "    Loads data for a time period into memory, masks to keep only\n",
    "    pixels observed at > median tide, takes median and counts of valid \n",
    "    water index results, and optionally writes each water index, tide \n",
    "    height and valid pixel counts for the time period to file\n",
    "    '''\n",
    "        \n",
    "    # Compute median water indices and counts of valid pixels\n",
    "    median_ds = year_ds.median(dim='time', keep_attrs=True)\n",
    "    median_ds['count'] = (year_ds.mndwi\n",
    "                          .count(dim='time', keep_attrs=True)\n",
    "                          .astype('int16'))\n",
    "    median_ds['stdev'] = year_ds.mndwi.std(dim='time', keep_attrs=True)\n",
    "    \n",
    "    # Write each variable to file  \n",
    "    if export_geotiff:\n",
    "        for i in median_ds:\n",
    "            try:\n",
    "                \n",
    "                # Write using float nodata type\n",
    "                geotiff_profile = {'blockxsize': 1024, \n",
    "                                       'blockysize': 1024, \n",
    "                                       'compress': 'deflate', \n",
    "                                       'zlevel': 5,\n",
    "                                       'nodata': np.nan}\n",
    "                \n",
    "                write_geotiff(filename=f'{output_dir}/{output_prefix}{i}_{str(label)}.tif', \n",
    "                              dataset=median_ds[[i]],\n",
    "                              profile_override=geotiff_profile)\n",
    "            except:\n",
    "                \n",
    "                # Update nodata value for int data type\n",
    "                geotiff_profile.update(nodata=-999)\n",
    "                write_geotiff(filename=f'{output_dir}/{output_prefix}{i}_{str(label)}.tif', \n",
    "                              dataset=median_ds[[i]],\n",
    "                              profile_override=geotiff_profile)\n",
    "            \n",
    "    # Set coordinate and dim\n",
    "    median_ds = (median_ds\n",
    "                 .assign_coords(**{label_dim: label})\n",
    "                 .expand_dims(label_dim)) \n",
    "        \n",
    "    return median_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datacube.utils.rio import configure_s3_access\n",
    "# from datacube.utils.dask import start_local_dask\n",
    "# import os\n",
    "# import dask\n",
    "# from dask.utils import parse_bytes\n",
    "\n",
    "# # configure dashboard link to go over proxy\n",
    "# dask.config.set({\"distributed.dashboard.link\":\n",
    "#                  os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"});\n",
    "\n",
    "# # Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "# mem_limit = int(os.environ.get('MEM_LIMIT', '0'))\n",
    "# cpu_limit = float(os.environ.get('CPU_LIMIT', '0'))\n",
    "# cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "# mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# # leave 4Gb for notebook itself\n",
    "# mem_limit -= parse_bytes('4Gb')\n",
    "\n",
    "# # close previous client if any, so that one can re-run this cell without issues\n",
    "# client = locals().get('client', None)\n",
    "# if client is not None:\n",
    "#     client.close()\n",
    "#     del client\n",
    "    \n",
    "# client = start_local_dask(n_workers=1,\n",
    "#                           threads_per_worker=cpu_limit, \n",
    "#                           memory_limit=mem_limit)\n",
    "# display(client)\n",
    "\n",
    "# # Configure GDAL for s3 access \n",
    "# configure_s3_access(aws_unsigned=True,  # works only when reading public resources\n",
    "#                     client=client);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='MAHTS_testing', env='c3-samples')\n",
    "# dc = datacube.Datacube(app='MAHTS_testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load virtual products catalogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog_from_file('MAHTS_virtual_products.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tidal points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = gpd.read_file('input_data/tide_points_coastal.geojson')\n",
    "# comp_gdf = gpd.read_file('input_data/Secondary_compartments.shp').set_index('ID_Seconda')\n",
    "comp_gdf = gpd.read_file('/g/data/r78/rt1527/shapefiles/50km_albers_grid/50km_albers_grid.shp').to_crs(epsg=4326).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area = 1677\n",
    "query = {'geopolygon': get_geopoly(study_area, comp_gdf),\n",
    "         'time': ('2018', '2018'),\n",
    "         'cloud_cover': [0, 10]}\n",
    "\n",
    "# Preview study area\n",
    "display_map(x=(query['geopolygon'].envelope.left, \n",
    "               query['geopolygon'].envelope.right), \n",
    "            y=(query['geopolygon'].envelope.top, \n",
    "               query['geopolygon'].envelope.bottom))\n",
    "\n",
    "# # Preview study area\n",
    "# display_map(x=query['x'], y=query['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load virtual product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product = catalog['ls_nbart_indices']\n",
    "# ds = product.load(dc, **query)\n",
    "# ds\n",
    "\n",
    "# Load virtual product\n",
    "crs = mostcommon_crs(dc=dc, product='ga_ls8c_ard_3', query=query)\n",
    "\n",
    "ds = load_ard(dc=dc, \n",
    "              measurements=['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2'], \n",
    "              min_gooddata=0.0,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "              output_crs=crs,\n",
    "              resampling={'*': 'average', 'fmask': 'nearest', 'oa_fmask': 'nearest'},\n",
    "              resolution=(-30, 30),  \n",
    "              gqa_iterative_mean_xy=[0, 1],\n",
    "              align=(15, 15),\n",
    "              group_by='solar_day',\n",
    "              dask_chunks={'time': 1},\n",
    "              mask_contiguity=False,\n",
    "              mask_pixel_quality=False,\n",
    "              **query)\n",
    "\n",
    "ds = (calculate_indices(ds, index=['MNDWI'], \n",
    "                        collection='ga_ls_3', \n",
    "                        drop=True)\n",
    "      .rename({'MNDWI': 'mndwi'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.api.query import query_group_by\n",
    "from datacube.model.utils import xr_apply\n",
    "dc = datacube.Datacube(app='MAHTS_testing', env='c3-samples')\n",
    "\n",
    "query = dict(product='ga_ls8c_ard_3', \n",
    "             lat=(-35, -36), \n",
    "             lon=(148, 149), \n",
    "             time='2018', \n",
    "             group_by='solar_day'\n",
    "            )\n",
    "\n",
    "gb = query_group_by(**query)\n",
    "datasets = dc.find_datasets(**query)\n",
    "dataset_array = dc.group_datasets(datasets, gb)\n",
    "cloud_cover = xr_apply(dataset_array, lambda t, dd: np.mean([d.metadata.cloud_cover for d in dd]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.api.query import _convert_to_solar_time\n",
    "\n",
    "# cloud_cover.groupby(cloud_cover.time.dt.round(freq='D')).mean(dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(datasets[0].metadata.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_convert_to_solar_time(d.center_time, longitude=) for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.load(product='ga_ls8c_ard_3', \n",
    "        lat=(-35, -36), \n",
    "        lon=(148, 149), \n",
    "        output_crs='epsg:32653',\n",
    "        resolution=(-30, 30),\n",
    "        time='2018',\n",
    "        group_by='solar_day',\n",
    "        dask_chunks={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = dc.load(product='ga_ls7e_ard_3',\n",
    "             measurements=['fmask'],\n",
    "             x=(119.50, 119.57),\n",
    "             y=(-20.08, -20.03),\n",
    "             time=('2018', '2018'),\n",
    "             output_crs='epsg:32653',\n",
    "             resolution=(-30, 30),\n",
    "             cloud_cover= [0, 20],\n",
    "             group_by='solar_day')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_ard(dc=dc, \n",
    "              measurements=['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2'], \n",
    "              min_gooddata=0.0,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "              output_crs=crs,\n",
    "              resampling={'*': 'average', 'fmask': 'nearest', 'oa_fmask': 'nearest'},\n",
    "              resolution=(-30, 30),  \n",
    "              gqa_iterative_mean_xy=[0, 1],\n",
    "              align=(15, 15),\n",
    "              group_by='solar_day',\n",
    "              dask_chunks={'time': 1},\n",
    "              mask_contiguity=False,              \n",
    "#               \n",
    "              x=(119.50, 119.57),\n",
    "              y=(-20.08, -20.03),\n",
    "              time='2018',\n",
    "              cloud_cover=[0, 20])\n",
    "\n",
    "ds = (calculate_indices(ds, index=['MNDWI'], \n",
    "                        collection='ga_ls_3', \n",
    "                        drop=True)\n",
    "      .rename({'MNDWI': 'mndwi'})).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.mndwi.median(dim='time') > 0).plot(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.mndwi.median(dim='time') > 0).plot(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.fmask == 2).sum(dim='time').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape\n",
    "subset_gdf = points_gdf[points_gdf.geometry.intersects(shape(ds.geobox.geographic_extent.json))]\n",
    "# subset_gdf = subset_gdf.iloc[::2, :]\n",
    "\n",
    "# Extract lon, lat from tides, and time from satellite data\n",
    "x_vals = subset_gdf.geometry.centroid.x\n",
    "y_vals = subset_gdf.geometry.centroid.y\n",
    "observed_datetimes = ds.time.data.astype('M8[s]').astype('O').tolist()\n",
    "\n",
    "# Create list of lat/lon/time scenarios to model\n",
    "observed_timepoints = [otps.TimePoint(lon, lat, date) \n",
    "                       for date in observed_datetimes\n",
    "                       for lon, lat in zip(x_vals, y_vals)]\n",
    "\n",
    "# Model tides for each scenario\n",
    "observed_predictedtides = otps.predict_tide(observed_timepoints)\n",
    "\n",
    "# Output results into pandas.DataFrame\n",
    "tidepoints_df = pd.DataFrame([(i.timepoint.timestamp, \n",
    "                               i.timepoint.lon, \n",
    "                               i.timepoint.lat, \n",
    "                               i.tide_m) for i in observed_predictedtides], \n",
    "                             columns=['time', 'lon', 'lat', 'tide_m']) \n",
    "\n",
    "# Convert data to spatial geopandas.GeoDataFrame\n",
    "tidepoints_gdf = gpd.GeoDataFrame(data={'time': tidepoints_df.time, \n",
    "                                        'tide_m': tidepoints_df.tide_m}, \n",
    "                                  geometry=gpd.points_from_xy(tidepoints_df.lon, \n",
    "                                                              tidepoints_df.lat), \n",
    "                                  crs={'init': 'EPSG:4326'})\n",
    "\n",
    "# Reproject to satellite data CRS\n",
    "tidepoints_gdf = tidepoints_gdf.to_crs(epsg=ds.crs.epsg)\n",
    "\n",
    "# Fix time and set to index\n",
    "tidepoints_gdf['time'] = pd.to_datetime(tidepoints_gdf['time'], utc=True)\n",
    "tidepoints_gdf = tidepoints_gdf.set_index('time')\n",
    "\n",
    "# Plot a sample timestep\n",
    "ds.isel(time=0).mndwi.plot()\n",
    "tidepoints_gdf.plot(ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate tides into each satellite timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "pool = multiprocessing.Pool(multiprocessing.cpu_count() - 1)\n",
    "print(f'Parallelising {multiprocessing.cpu_count() -1} processes')\n",
    "tide_list = pool.map(partial(interpolate_tide, \n",
    "                             tidepoints_gdf=tidepoints_gdf, \n",
    "                             factor=50), \n",
    "                     iterable=[group for (i, group) in ds.groupby('time')])\n",
    "\n",
    "# Interpolate tides for each timestep into the spatial extent of the data\n",
    "ds['tide_m'] = xr.concat(tide_list, dim=ds.time)\n",
    "\n",
    "# Determine tide cutoff\n",
    "tide_cutoff_buff = ((ds['tide_m'].max(dim='time') - \n",
    "                     ds['tide_m'].min(dim='time')) * 0.25)  #.clip(0.0, 1.0)\n",
    "tide_cutoff_min = 0.0 - tide_cutoff_buff\n",
    "tide_cutoff_max = 0.0 + tide_cutoff_buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "ds_i = ds['tide_m'].isel(time=10)\n",
    "ds_i.plot.imshow(robust=True, \n",
    "                 cmap='viridis', \n",
    "                 size=12, \n",
    "                 vmin=ds_i.min().item(), \n",
    "                 vmax=ds_i.max().item())\n",
    "tidepoints_gdf.loc[str(ds_i.time.values)[0:10]].plot(ax=plt.gca(), \n",
    "                                                     column='tide_m', \n",
    "                                                     cmap='viridis', \n",
    "                                                     markersize=100,\n",
    "                                                     edgecolor='black',\n",
    "                                                     vmin=ds_i.min().item(), \n",
    "                                                     vmax=ds_i.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate yearly composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If output folder doesn't exist, create it\n",
    "output_dir = f'output_data/{study_area}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create empty vars containing un-composited data from the previous,\n",
    "# current and future year. This is progressively updated to ensure that\n",
    "# no more than 3 years of data are loaded into memory at any one time\n",
    "previous_ds = None\n",
    "current_ds = None\n",
    "future_ds = None\n",
    "\n",
    "# Iterate through each year in the dataset, starting at one year before\n",
    "for year in np.unique(ds.time.dt.year) - 1:\n",
    "\n",
    "    # Load data for the subsequent year\n",
    "    future_ds = load_tidal_subset(ds.sel(time=str(year + 1)), \n",
    "                                  tide_cutoff_min=tide_cutoff_min,\n",
    "                                  tide_cutoff_max=tide_cutoff_max)\n",
    "\n",
    "    # If the current year var contains data, combine these observations\n",
    "    # into median annual high tide composites and export GeoTIFFs\n",
    "    if current_ds:\n",
    "\n",
    "        # Generate composite\n",
    "        tidal_composite(current_ds, \n",
    "                        label=year,\n",
    "                        label_dim='year',\n",
    "                        output_dir=output_dir, \n",
    "                        export_geotiff=True)        \n",
    "\n",
    "    # If ALL of the previous, current and future year vars contain data,\n",
    "    # combine these three years of observations into a single median \n",
    "    # 3-year gapfill composite\n",
    "    if previous_ds and current_ds and future_ds:\n",
    "\n",
    "        # Concatenate the three years into one xarray.Dataset\n",
    "        gapfill_ds = xr.concat([previous_ds, current_ds, future_ds], \n",
    "                               dim='time')\n",
    "\n",
    "        # Generate composite\n",
    "        tidal_composite(gapfill_ds,\n",
    "                        label=year,\n",
    "                        label_dim='year',\n",
    "                        output_dir=output_dir, \n",
    "                        output_prefix='gapfill_',\n",
    "                        export_geotiff=True)        \n",
    "\n",
    "    # Shift all loaded data back so that we can re-use it in the next\n",
    "    # iteration and not have to load the same data multiple times\n",
    "    previous_ds = current_ds\n",
    "    current_ds = future_ds\n",
    "    future_ds = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidal_composite(current_ds, \n",
    "                        label=year,\n",
    "                        label_dim='year',\n",
    "                        output_dir=output_dir, \n",
    "                        export_geotiff=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '_get_index_and_items',\n",
    "#  '_group',\n",
    "#  '_group_dim',\n",
    "#  '_group_indices',\n",
    "#  '_groups',\n",
    "\n",
    "# dir(ds.groupby('time.year'))\n",
    "# i, year = 0, np.unique(ds.time.dt.year)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_ds = []\n",
    "# current_ds = []\n",
    "# future_ds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# yearly_ds['count'].sum(dim='year').plot(size=10, robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly_ds.geobox.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
