{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHTS generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "* Take full time series of NDWI in dask\n",
    "* Compute 2D tides for every timestep\n",
    "* Compute median of tides from entire tide timeseries\n",
    "* For each year in dask NDWI timeseries:\n",
    "    * Mask pixels where tide > overall median\n",
    "    * `.compute()` and take median \n",
    "    \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import otps\n",
    "import datacube\n",
    "import shapely.wkt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "# from skimage import measure\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.virtual import catalog_from_file\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from dea_plotting import rgb\n",
    "from dea_plotting import display_map\n",
    "from dea_spatialtools import interpolate_2d\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_tide(timestep_ds, tidepoints_gdf, method='rbf', factor=20):    \n",
    "    '''\n",
    "    Extract a subset of tide modelling point data for a given time-step,\n",
    "    then interpolate these tides into the extent of the xarray dataset.\n",
    "    '''    \n",
    "  \n",
    "    # Extract subset of observations based on timestamp of imagery\n",
    "    time_string = str(timestep_ds.time.values)[0:19].replace('T', ' ')\n",
    "    tidepoints_subset = tidepoints_gdf.loc[time_string]\n",
    "    print(time_string, end='\\r')\n",
    "    \n",
    "    # Get lists of x, y and z (tide height) data to interpolate\n",
    "    x_coords = tidepoints_subset.geometry.x,\n",
    "    y_coords = tidepoints_subset.geometry.y,\n",
    "    z_coords = tidepoints_subset.tide_m\n",
    "    \n",
    "    # Interpolate tides into the extent of the satellite timestep\n",
    "    out_tide = interpolate_2d(ds=timestep_ds,\n",
    "                              x_coords=x_coords,\n",
    "                              y_coords=y_coords,\n",
    "                              z_coords=z_coords,\n",
    "                              method=method,\n",
    "                              factor=factor)\n",
    "    \n",
    "    # Return data as a Float32 to conserve memory\n",
    "    return out_tide.astype(np.float32)\n",
    "\n",
    "\n",
    "def tidal_composite(year_ds, tide_cutoff, output_dir, export_geotiff=False):\n",
    "    '''\n",
    "    Loads data for a time period into memory, masks to keep only\n",
    "    pixels observed at > median tide, takes median and counts of valid \n",
    "    water index results, and optionally writes each water index, tide \n",
    "    height and valid pixel counts for the time period to file\n",
    "    '''\n",
    "    \n",
    "    # Print status\n",
    "    year = year_ds.time[0].dt.year.item()\n",
    "    print(f'Processing {year}')\n",
    "    \n",
    "    # Load in data for year and mask to keep above median tide pixels\n",
    "    year_ds = year_ds.compute()\n",
    "    year_ds = year_ds.where(year_ds.tide_m >= tide_cutoff)\n",
    "    \n",
    "    # Compute median water indices and counts of valid pixels\n",
    "    median_ds = year_ds.median(dim='time', keep_attrs=True)\n",
    "    median_ds['count'] = (year_ds.mndwi\n",
    "                          .count(dim='time', keep_attrs=True)\n",
    "                          .astype('int16'))\n",
    "    median_ds['stdev'] = year_ds.mndwi.std(dim='time', keep_attrs=True)\n",
    "    \n",
    "    # Write each variable to file\n",
    "    if export_geotiff:\n",
    "        for i in median_ds:\n",
    "            write_geotiff(filename=f'{output_dir}/{i}_{str(year)}.tif', \n",
    "                          dataset=median_ds[[i]])\n",
    "        \n",
    "    return median_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datacube.utils.rio import configure_s3_access\n",
    "# from datacube.utils.dask import start_local_dask\n",
    "# import os\n",
    "# import dask\n",
    "# from dask.utils import parse_bytes\n",
    "\n",
    "# # configure dashboard link to go over proxy\n",
    "# dask.config.set({\"distributed.dashboard.link\":\n",
    "#                  os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"});\n",
    "\n",
    "# # Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "# mem_limit = int(os.environ.get('MEM_LIMIT', '0'))\n",
    "# cpu_limit = float(os.environ.get('CPU_LIMIT', '0'))\n",
    "# cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "# mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# # leave 4Gb for notebook itself\n",
    "# mem_limit -= parse_bytes('4Gb')\n",
    "\n",
    "# # close previous client if any, so that one can re-run this cell without issues\n",
    "# client = locals().get('client', None)\n",
    "# if client is not None:\n",
    "#     client.close()\n",
    "#     del client\n",
    "    \n",
    "# client = start_local_dask(n_workers=1,\n",
    "#                           threads_per_worker=cpu_limit, \n",
    "#                           memory_limit=mem_limit)\n",
    "# display(client)\n",
    "\n",
    "# # Configure GDAL for s3 access \n",
    "# configure_s3_access(aws_unsigned=True,  # works only when reading public resources\n",
    "#                     client=client);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='MAHTS_testing', env='c3-samples')\n",
    "# dc = datacube.Datacube(app='MAHTS_testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load virtual products catalogue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog_from_file('MAHTS_virtual_products.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tidal points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gdf = gpd.read_file('input_data/tide_points_coastal.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'stradbroke'\n",
    "# query = {'x': (153.42, 153.61),\n",
    "#          'y': (-27.35, -28.4919),\n",
    "#          'time': ('1987', '2018'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80],\n",
    "#         }\n",
    "\n",
    "\n",
    "# name = 'goldcoast_test'\n",
    "# query = {'x': (153.42, 153.61),\n",
    "#          'y': (-28.0235, -28.4281),\n",
    "#          'time': ('1987', '2018'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80],\n",
    "#         }\n",
    "\n",
    "\n",
    "# # Gold Coast\n",
    "# name = 'brisbane'\n",
    "# query = {'x': (153.00, 153.50),\n",
    "#          'y': (-26.6499,  -27.6495),\n",
    "#          'time': ('1987', '2018'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80],\n",
    "#         }\n",
    "\n",
    "\n",
    "# name = 'fraser'\n",
    "# query = {'x': (152.9819, 153.20),\n",
    "#          'y': (-25.60,  -26.67),\n",
    "#          'time': ('1987', '2018'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80],\n",
    "#         }\n",
    "\n",
    "\n",
    "# name = 'yamba'\n",
    "# query = {'x': (153.29,  153.4735  ),\n",
    "#          'y': (-29.0994,  -29.7381),\n",
    "#          'time': ('1987', '2018'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80],\n",
    "#         }\n",
    "\n",
    "    \n",
    "name = 'byron'\n",
    "query = {'x': (153.42, 153.645),\n",
    "         'y': (-28.46, -29.17),\n",
    "         'time': ('1987', '2018'),\n",
    "         'dask_chunks': {'x': 1000, 'y': 1000},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "\n",
    "# # Moreton Bay\n",
    "# name = 'moretonbay'\n",
    "# query = {'x': (153.00, 153.25),\n",
    "#          'y': (-26.81, -27.44),\n",
    "#          'time': ('2008', '2008'),\n",
    "#          'cloud_cover': [0, 80]}\n",
    "\n",
    "# # Moreton Bay seam test\n",
    "# name = 'moretonbay'\n",
    "# query = {'x': (153.10, 153.18),\n",
    "#          'y': (-26.70, -26.83),\n",
    "#          'time': ('2000', '2010'),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80]}\n",
    "\n",
    "# # Moreton Bay small\n",
    "# name = 'moretonbay'\n",
    "# query = {'x': (153.16, 153.2840),\n",
    "#          'y': (-27.315, -27.4416),\n",
    "#          'time': ('2000', '2010'),\n",
    "# #          'output_crs': 'EPSG:32756',\n",
    "# #          'resolution': (-30, 30),\n",
    "#          'dask_chunks': {'time': 1},\n",
    "#          'cloud_cover': [0, 80]}\n",
    "\n",
    "name = 'perthsmall'\n",
    "query = {'x': (115.71,  115.7777),\n",
    "         'y': (-31.7363, -31.8787),\n",
    "         'time': ('1987', '2018'),\n",
    "         'dask_chunks': {'x': 1000, 'y': 1000},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "\n",
    "name = 'fremantle'\n",
    "query = {'x': (115.7199,  115.7650),\n",
    "         'y': (-31.9944, -32.0786),\n",
    "         'time': ('1987', '2018'),\n",
    "         'dask_chunks': {'x': 1000, 'y': 1000},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "name = 'fremantle2'\n",
    "query = {'x': (115.7312,  115.7726),\n",
    "         'y': ( -32.0692, -32.1451),\n",
    "         'time': ('1987', '2018'),\n",
    "         'dask_chunks': {'x': 1000, 'y': 1000},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "name = 'portlonsdale'\n",
    "query = {'x': (144.471801,  144.7868),\n",
    "         'y': (-38.0816, -38.35),\n",
    "         'time': ('1987', '2018'),\n",
    "         'dask_chunks': {'x': 1000, 'y': 1000},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "\n",
    "name = 'cornerinlet'\n",
    "query = {'x': (146.4381,  146.93),\n",
    "         'y': (-38.58, -38.8253),\n",
    "         'time': ('1987', '2018'),\n",
    "         'dask_chunks': {'x': 1000, 'y': 1000},\n",
    "         'cloud_cover': [0, 80]}\n",
    "\n",
    "# Preview study area\n",
    "display_map(x=query['x'], y=query['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load virtual product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product = catalog['ls_nbart_indices']\n",
    "# ds = product.load(dc, **query)\n",
    "# ds\n",
    "\n",
    "from dea_datahandling import load_ard\n",
    "from dea_bandindices import calculate_indices\n",
    "\n",
    "ds = load_ard(dc=dc, \n",
    "              measurements=['nbart_blue', 'nbart_green', 'nbart_red', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2'], \n",
    "              min_gooddata=0.0,\n",
    "              products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "              output_crs='epsg:32755',\n",
    "              resampling={'*': 'average', 'fmask': 'nearest', 'oa_fmask': 'nearest'},\n",
    "              resolution=(-30, 30),  \n",
    "              gqa_iterative_mean_xy=[0, 1],\n",
    "              align=(15, 15),\n",
    "              lazy_load=True,\n",
    "              group_by='solar_day',\n",
    "              **query)\n",
    "\n",
    "ds = (calculate_indices(ds, index=['NDWI', 'MNDWI', 'AWEI_ns', 'AWEI_sh'], \n",
    "                        collection='ga_ls_3', \n",
    "                        drop=True)\n",
    "      .rename({'NDWI': 'ndwi', 'MNDWI': 'mndwi', 'AWEI_ns': 'awei_ns', 'AWEI_sh': 'awei_sh'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out subset of modelling points for region around satellite data\n",
    "bounds = shapely.wkt.loads(ds.geobox.geographic_extent.buffer(0.07).wkt)\n",
    "subset_gdf = points_gdf[points_gdf.geometry.intersects(bounds)]\n",
    "\n",
    "# Extract lon, lat from tides, and time from satellite data\n",
    "x_vals = subset_gdf.geometry.centroid.x\n",
    "y_vals = subset_gdf.geometry.centroid.y\n",
    "observed_datetimes = ds.time.data.astype('M8[s]').astype('O').tolist()\n",
    "\n",
    "# Create list of lat/lon/time scenarios to model\n",
    "observed_timepoints = [otps.TimePoint(lon, lat, date) \n",
    "                       for date in observed_datetimes\n",
    "                       for lon, lat in zip(x_vals, y_vals)]\n",
    "\n",
    "# Model tides for each scenario\n",
    "observed_predictedtides = otps.predict_tide(observed_timepoints)\n",
    "\n",
    "# Output results into pandas.DataFrame\n",
    "tidepoints_df = pd.DataFrame([(i.timepoint.timestamp, \n",
    "                               i.timepoint.lon, \n",
    "                               i.timepoint.lat, \n",
    "                               i.tide_m) for i in observed_predictedtides], \n",
    "                             columns=['time', 'lon', 'lat', 'tide_m']) \n",
    "\n",
    "# Convert data to spatial geopandas.GeoDataFrame\n",
    "tidepoints_gdf = gpd.GeoDataFrame(data={'time': tidepoints_df.time, \n",
    "                                        'tide_m': tidepoints_df.tide_m}, \n",
    "                                  geometry=gpd.points_from_xy(tidepoints_df.lon, \n",
    "                                                              tidepoints_df.lat), \n",
    "                                  crs={'init': 'EPSG:4326'})\n",
    "\n",
    "# Reproject to satellite data CRS\n",
    "tidepoints_gdf = tidepoints_gdf.to_crs(epsg=ds.crs.epsg)\n",
    "\n",
    "# Fix time and set to index\n",
    "tidepoints_gdf['time'] = pd.to_datetime(tidepoints_gdf['time'], utc=True)\n",
    "tidepoints_gdf = tidepoints_gdf.set_index('time')\n",
    "\n",
    "# Plot a sample timestep\n",
    "ds.isel(time=0).mndwi.plot()\n",
    "tidepoints_gdf.plot(ax=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate tides into each satellite timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate tides for each timestep into the spatial extent of the data\n",
    "tide_da = ds.groupby('time').apply(interpolate_tide, \n",
    "                                   tidepoints_gdf=tidepoints_gdf,\n",
    "                                   factor=53)\n",
    "\n",
    "# Determine tide cutoff\n",
    "tide_cutoff = tide_da.median(dim='time')\n",
    "\n",
    "# Add interpolated tides as measurement in satellite dataset\n",
    "ds['tide_m'] = tide_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "ds_i = ds['tide_m'].isel(time=0)\n",
    "ds_i.plot.imshow(robust=True, \n",
    "                 cmap='viridis', \n",
    "                 size=12, \n",
    "                 vmin=ds_i.min().item(), \n",
    "                 vmax=ds_i.max().item())\n",
    "tidepoints_gdf.loc[str(ds_i.time.values)[0:10]].plot(ax=plt.gca(), \n",
    "                                                     column='tide_m', \n",
    "                                                     cmap='viridis', \n",
    "                                                     markersize=100,\n",
    "                                                     edgecolor='black',\n",
    "                                                     vmin=ds_i.min().item(), \n",
    "                                                     vmax=ds_i.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate yearly composites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If output folder doesn't exist, create it\n",
    "output_dir = f'output_data/{name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate yearly high tide median water indices\n",
    "yearly_ds = ds.groupby('time.year').apply(tidal_composite, \n",
    "                                          tide_cutoff=tide_cutoff, \n",
    "                                          output_dir=output_dir,\n",
    "                                          export_geotiff=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "yearly_ds['count'].sum(dim='year').plot(size=10, robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_ds.geobox.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datacube.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3df30b689e504a35bbf504664badf913": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     },
     "8ede46452705436ba6aeb83fd72c42df": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
