{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Vegetation Anomalies\n",
    "\n",
    "Notebook is only compatible on the `NCI` as it uses Landsat Collection 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Understanding how the vegetated landscape responds to longer-term environmental drivers such as the El Nino Southern Oscillation (ENSO) or climate change, requires the calculation of seasonal anomalies. Seasonal anomalies subtract the long-term seasonal mean from a time-series, thus removing seasonal variability and highlighting change related to longer-term drivers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook will calculate the seasonal anomaly for any given season and year. The long-term seasonal climatologies for the vegetation indices, `MSAVI`, and `NDVI` (NDVI hasn't been run yet), have been pre-calculated and are stored on disk. Given an AOI, season, and year, the script will calculate the seasonal mean for one of these indices and subtract the seasonal mean from the long-term climatology, resulting in a map of vegetation anomalies for your AOI.  Optionally, the script will output a geotiff of the result. \n",
    "\n",
    "**IMPORTANT NOTES:** \n",
    "\n",
    "* It is a convention to establish climatologies based on a 30-year time range to account for inter-annual and inter-decadal modes of climate variability (often 1980-2010). As the landsat archive only goes back to 1987, the climatologies here have been calculated using the date-range `Dec 1987 - Feb 2011` (inclusive).  While this is not ideal, a 22-year climatology should suffice to capture the bulk of inter-annual and inter-decadal variability, for example, both a major El Nino (1998) and a major La Nina (2010) are captured by this time-range.\n",
    "* Currently, the pre-computed climatologies are only `MSAVI mean`, the `MSAVI Standard Deviation` has not been pre-computed, hence it is not yet possible to calculate `standardised anomalies`. `NDVI anomalies` have not yet been run.\n",
    "* The pre-computed climatologies do not include LS7 after the SLC failure (2003).\n",
    "* `.yaml` files for running datacube stats to calculate vegetation climatologies are located here: `'/g/data/r78/cb3058/dea-notebooks/vegetation_anomlies/dcstats'`. There is a different .yaml file for each season, and a corresponding custom datacube-statistics python file for each season; these are located here: `'/g/data/r78/datacube_stats/custom_stats'` \n",
    "* The pre-computed climatologies are stored here: `/g/data/r78/cb3058/dea-notebooks/vegetation_anomalies/results`.  The script below will use this string location to grab the data, so shifting the climatology mosaics to another location will require editing the `anomalies.py` script in the `src` folder. \n",
    "* So far, climatolgies have been produced for the `Northern Murray-Darling Basin` for the `SON` and `JJA` seasons; the `DJF` season has a mosaic that is 2/3 complete (59 out of 90 albers tiles). The `NW Queensland` region has mosaicked climatologies for `JJA` and `DJF`, but these are also only 2/3 complete (60 out of 90 albers tiles processed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical details\n",
    "\n",
    "* **Products used:** ls5_nbart_albers, ls7_nbart_albers, ls8_nbart_albers\n",
    "* **Analyses used:** NDVI, MSAVI, seasonal anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To run this analysis, go to the `Analysis Parameters` section and enter the relevant details, then run all the cells in the notebook. If running the analysis multiple times, only run the `Set up dask cluster` and `import libraries` cells once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from datacube.helpers import write_geotiff\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('src')\n",
    "from anomalies import calculate_anomalies, load_ard\n",
    "sys.path.append('../Scripts')\n",
    "from dea_plotting import display_map, map_shapefile\n",
    "from dea_dask import create_local_dask_cluster\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up local dask cluster\n",
    "\n",
    "Dask will create a local cluster of cpus for running this analysis in parallel. If you'd like to see what the dask cluster is doing, click on the hyperlink that prints after you run the cell and you can watch the cluster run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:46305</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>30.67 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:46305' processes=1 threads=8, memory=30.67 GB>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#delete old client if one still exists\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "The following cell sets the parameters, which define the area of interest and the season to conduct the analysis over. The parameters are:\n",
    "\n",
    "* `from_shape`: If providing a shapefile to the define the area of interest, set this parameter to `True`, otherwise set to `False`.\n",
    "* `shp_fpath`: If you set `from_shape` to True, provide a filepath to the shapefile.\n",
    "* `lat`, `lon`, `buffer`: If not using a shapefile to define the AOI, then use a latitide and longitude point and buffer to define a query 'box'.\n",
    "* `year`: The year of interest, e.g. `'2018'`\n",
    "* `season`:  The season of interest, e.g 'DJF','JFM', 'FMA' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters\n",
    "from_shape = True\n",
    "shp_fpath = \"data/nmdb_individual_catchments/BORDER_RIVERS_NSW.shp\"\n",
    "lat, lon, buff = -33.999, 150.258, 0.5\n",
    "year = '2018'\n",
    "season = 'JJA'\n",
    "\n",
    "#dask chunk size, shouldn't need to change\n",
    "dask_chunks = {'x':'auto', 'y':'auto'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine your area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using a shapefile, run this cell\n",
    "# map_shapefile(gpd.read_file(shp_fpath), attribute='BNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your specifying a lat, lon and buffer, run this cell\n",
    "# display_map(y=(lat-buffer, lat + buffer), x=(lon-buffer, lon + buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the anomaly for the AOI\n",
    "\n",
    "For large queries (e.g > 4000 x 4000 pixels), the code will take several minutes to run.  Queries much larger than ~15,000 x 15,000 pixels will start to fail due to memory limitations (that said, a 19,000 x 15,000 pixel catchment has been successfully run on the VDI). Check the x,y dimensions in the lazily loaded output to get idea of how big your result will be before you run  the `.compute()` cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data based on shapefile extent\n",
      "Finding datasets\n",
      "    ga_ls8c_ard_3\n",
      "Applying pixel quality/cloud mask\n",
      "Returning 24 time steps as a dask array\n",
      "calculating vegetation indice\n",
      "calculating anomalies\n",
      "<xarray.Dataset>\n",
      "Dimensions:        (x: 10974, y: 6210)\n",
      "Coordinates:\n",
      "  * x              (x) float64 1.598e+06 1.598e+06 ... 1.927e+06 1.927e+06\n",
      "  * y              (y) float64 -3.219e+06 -3.219e+06 ... -3.405e+06 -3.405e+06\n",
      "    band           int64 1\n",
      "Data variables:\n",
      "    std_anomalies  (y, x) float32 dask.array<chunksize=(1696, 4347), meta=np.ndarray>\n",
      "Attributes:\n",
      "    crs:      epsg:3577\n",
      "    units:    1\n"
     ]
    }
   ],
   "source": [
    "#Lazily run calculations, this will check for errors before\n",
    "# we actually compute the results\n",
    "anomalies = calculate_anomalies(from_shape=from_shape,\n",
    "                                shp_fpath=shp_fpath,\n",
    "                                query_box=(lat,lon,buff),\n",
    "                                year=year,\n",
    "                                season=season,\n",
    "                                products=['ga_ls8c_ard_3'],\n",
    "                                dask_chunks=dask_chunks)\n",
    "\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell will actually compute the above calculations\n",
    "anomalies = anomalies.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write geotiff to a location\n",
    "write_geotiff('ndvi_'+year+\"_\"+season+ '_standardised_anomalies.tif', anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the result\n",
    "\n",
    "If your AOI is very large, plotting the result can crash the notebook. In this case, its better to export the geotiff and view it in QGIS or ArcGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.std_anomalies.plot(figsize=(10,10), vmin=-2.0, vmax=2.0, cmap='BrBG')\n",
    "\n",
    "plt.title(season+ \", \" +year)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
