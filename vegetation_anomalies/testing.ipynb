{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datacube \n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = gpd.read_file('dcstats/landsat_tiles.geojson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles.to_file('landsatC3_tiles.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsw = gpd.read_file('data/NSW_STATE_POLYGON.shp').to_crs(epsg=3577)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_in_nsw = gpd.overlay(tiles, nsw, how='intersection')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [12,-58]\n",
      "- [12,-60]\n",
      "- [12,-62]\n",
      "- [14,-52]\n",
      "- [14,-54]\n",
      "- [14,-56]\n",
      "- [14,-58]\n",
      "- [14,-60]\n",
      "- [14,-62]\n",
      "- [16,-52]\n",
      "- [16,-54]\n",
      "- [16,-56]\n",
      "- [16,-58]\n",
      "- [16,-60]\n",
      "- [16,-62]\n",
      "- [16,-64]\n",
      "- [18,-52]\n",
      "- [18,-54]\n",
      "- [18,-56]\n",
      "- [18,-58]\n",
      "- [18,-60]\n",
      "- [18,-62]\n",
      "- [18,-64]\n",
      "- [18,-66]\n",
      "- [20,-52]\n",
      "- [20,-54]\n",
      "- [20,-56]\n",
      "- [20,-58]\n",
      "- [20,-60]\n",
      "- [20,-62]\n",
      "- [20,-64]\n",
      "- [20,-66]\n",
      "- [22,-52]\n",
      "- [22,-54]\n",
      "- [22,-56]\n",
      "- [22,-58]\n",
      "- [22,-60]\n",
      "- [22,-62]\n",
      "- [22,-64]\n",
      "- [22,-66]\n",
      "- [22,-68]\n",
      "- [24,-52]\n",
      "- [24,-54]\n",
      "- [24,-56]\n",
      "- [24,-58]\n",
      "- [24,-60]\n",
      "- [24,-62]\n",
      "- [24,-64]\n",
      "- [24,-66]\n",
      "- [24,-68]\n",
      "- [26,-52]\n",
      "- [26,-54]\n",
      "- [26,-56]\n",
      "- [26,-58]\n",
      "- [26,-60]\n",
      "- [26,-62]\n",
      "- [26,-64]\n",
      "- [26,-66]\n",
      "- [26,-68]\n",
      "- [26,-70]\n",
      "- [28,-52]\n",
      "- [28,-54]\n",
      "- [28,-56]\n",
      "- [28,-58]\n",
      "- [28,-60]\n",
      "- [28,-62]\n",
      "- [28,-64]\n",
      "- [28,-66]\n",
      "- [30,-54]\n",
      "- [30,-56]\n",
      "- [30,-58]\n",
      "- [30,-60]\n",
      "- [30,-62]\n",
      "- [32,-52]\n",
      "- [32,-54]\n",
      "- [32,-56]\n",
      "- [32,-58]\n",
      "- [32,-60]\n",
      "- [34,-52]\n",
      "- [34,-54]\n",
      "- [34,-56]\n"
     ]
    }
   ],
   "source": [
    "for i in tiles_in_nsw.label:\n",
    "    print(\"- [\" +i+\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info for parallel processing with Dask\n",
    "1. If reading netcdf files make sure each worker has one thread\n",
    "2. memory_limit is per worker not per cluster of workers\n",
    "3. When launching multiple workers (needed when reading netcdfs) on the same node you have to supply memory limit, otherwise every worker will assume they have all the memory\n",
    "\n",
    "Notes from Imam\n",
    "- consider the datatype, float32 is best. Perhaps prevent the load landsat function changing nodata values\n",
    "- Need to 'scale' the reflectance values to 0-1, rather than 0-10,000\n",
    "- .to_netcdf() is a bottle neck, need to limit the number of workers hitting it (bigger chunks is better)\n",
    "- look into .to_zarr(), which is an experimental netcdf writer that deals is better for multprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "\n",
    "# client = Client(n_workers=6, threads_per_worker=1, memory_limit='4GB')\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not using a polygon then enter your AOI coords\n",
    "#below:\n",
    "lat, lon = -34.294, 146.037\n",
    "latLon_adjust = 0.025\n",
    "\n",
    "start = '2013-12-01'\n",
    "end = '2019-05-31'\n",
    "\n",
    "shp_fpath = \"/g/data1a/r78/cb3058/dea-notebooks/dcStats/data/spatial/MDB_plus_NSW.shp\"\n",
    "chunk_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = query_from_shp.query_from_shp(shp_fpath, start, end)\n",
    "query = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "         'lat': (lat - latLon_adjust, lat + latLon_adjust),\n",
    "        'time': (start, end)}\n",
    "\n",
    "# query = query_from_shp.query_from_shp(shp_fpath, start, end)\n",
    "dc = datacube.Datacube(app='load_clearlandsat')\n",
    "\n",
    "ds = anomalies.load_landsat(dc=dc, query=query, sensors=['ls5','ls7','ls8'], \n",
    "                           bands_of_interest=['nir', 'red'], lazy_load=True,\n",
    "                           dask_chunks = {'x': chunk_size, 'y': chunk_size})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter= {'JFM': [1,2,3],\n",
    "           'FMA': [2,3,4],\n",
    "           'MAM': [3,4,5],\n",
    "           'AMJ': [4,5,6],\n",
    "           'MJJ': [5,6,7],\n",
    "           'JJA': [6,7,8],\n",
    "           'JAS': [7,8,9],\n",
    "           'ASO': [8,9,10],\n",
    "           'SON': [9,10,11],\n",
    "           'OND': [10,11,12],\n",
    "           'NDJ': [11,12,1],\n",
    "           'DJF': [12,1,2]\n",
    "              }\n",
    "\n",
    "def compute(data, quarter):\n",
    "    \n",
    "    def attrs_reassign(da, dtype=np.float32):\n",
    "        \"\"\"little function to reassigna atributes\n",
    "        to the dataArrays inside a dataset\"\"\"\n",
    "        da_attr = data.attrs\n",
    "        da = da.assign_attrs(**da_attr)\n",
    "        return da\n",
    "\n",
    "    ndvi = (data.nir - data.red) / (data.nir + data.red)\n",
    "\n",
    "    ndvi_var = []\n",
    "    for q in quarter:\n",
    "        ix=ndvi['time.month'].isin(quarter[q])\n",
    "        ndvi_clim_mean=ndvi[ix].mean(dim='time')   \n",
    "        ndvi_clim_mean=ndvi_clim_mean.rename('ndvi_clim_mean_'+q)\n",
    "        ndvi_var.append(ndvi_clim_mean)\n",
    "    \n",
    "    q_clim_mean = xr.merge(ndvi_var)   \n",
    "    q_clim_mean.attrs = data.attrs \n",
    "    #assign back attributes\n",
    "    q_clim_mean = q_clim_mean.apply(attrs_reassign, keep_attrs=True)\n",
    "      \n",
    "\n",
    "    return q_clim_mean   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compute(ds, quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndvi_clim_mean_AMJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xr.merge(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate seasonal anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_climatology(data, stat='mean'):\n",
    "    #Scale reflectance values to 0-1\n",
    "    nir = data.nir / 10000\n",
    "    red = data.red / 10000\n",
    "    #calculate msavi\n",
    "    msavi = (2*nir+1-((2*nir+1)**2 - 8*(nir-red))**0.5)/2\n",
    "    msavi = msavi.astype('float32') #convert to reduce memory\n",
    "    \n",
    "    if stat == 'mean':\n",
    "        #calculate climatologies and compute\n",
    "        msavi = msavi.resample(time='QS-DEC').mean('time')\n",
    "        climatology_mean = msavi.groupby('time.season').mean('time').rename('masvi_mean_climatology')\n",
    "        climatology_mean = climatology_mean.to_dataset()\n",
    "        climatology_mean.to_zarr('results/masvi_climatology_mean.nc')\n",
    "\n",
    "    if stat == 'std':\n",
    "        #calculate climatologies and compute\n",
    "        msavi = msavi.resample(time='QS-DEC').mean('time')\n",
    "        climatology_std = msavi.groupby('time.season').std('time').rename('masvi_std_climatology')\n",
    "        climatology_std = climatology_std.to_dataset()\n",
    "        climatology_std.to_zarr('results/masvi_climatology_std.nc')\n",
    "    \n",
    "#     return msavi#, climatology_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_climatology(ds, stat='mean')\n",
    "compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msavi = xr.open_zarr('results/masvi_climatology_mean.nc')\n",
    "msavi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xr.open_rasterio('msavi_2018_SON_anomalies.tif').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.plot(figsize=(15,15), vmin=-0.25, vmax=0.25, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.plot(x='x',y='y', col='time', col_wrap=2, vmin=-2.0,vmax=2.0,cmap='RdBu', figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.mean(['x', 'y']).mean()#plot(figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_anomalies(data, output_dir):\n",
    "#     msavi = xr.open_zarr('results/masvi.nc', chunks={'x': chunk_size, 'y': chunk_size})\n",
    "#     #resample to quarterly and groupby seasons\n",
    "#     msavi_seasonalMeans = msavi.resample(time='QS-DEC').mean('time')\n",
    "#     msavi_seasonalMeans = msavi_seasonalMeans.groupby('time.season')\n",
    "    \n",
    "#     #import climatology\n",
    "#     climatology_mean = xr.open_dataarray('results/masvi_climatology_mean_test.nc', chunks={'x': chunk_size, 'y': chunk_size})\n",
    "#     climatology_std = xr.open_dataarray('results/masvi_climatology_std_test.nc', chunks={'x': chunk_size, 'y': chunk_size})\n",
    "    \n",
    "#     #calculate standardised anomalies\n",
    "#     msavi_stand_anomalies = xr.apply_ufunc(lambda x, m, s: (x - m) / s,\n",
    "#                                  msavi_seasonalMeans, climatology_mean, climatology_std,\n",
    "#                                  dask='allowed')\n",
    "    \n",
    "#     #write out results (will compute now)\n",
    "#     msavi_stand_anomalies.to_zarr(output_dir)\n",
    "    \n",
    "#     return msavi_stand_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUTS/ code for mosaicing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/g/data1a/r78/cb3058/dea-notebooks/vegetation_anomalies/results/northern_basins_SON/tiffs/')\n",
    "os.system(\"gdalbuildvrt msavi_NMDB_climatology_SON_mosaic.vrt *.tif\")\n",
    "os.system(\"gdal_translate \"\\\n",
    "   \"-co BIGTIFF=YES \"\\\n",
    "   \"-co COMPRESS=DEFLATE \"\\\n",
    "   \"-co ZLEVEL=9 \"\\\n",
    "   \"-co PREDICTOR=1 \"\\\n",
    "   \"-co TILED=YES \"\\\n",
    "   \"-co BLOCKXSIZE=1024 \"\\\n",
    "   \"-co BLOCKYSIZE=1024 \"\\\n",
    "   +\"msavi_NMDB_climatology_SON_mosaic.vrt \"+ \"msavi_NMDB_climatology_SON_mosaic.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/g/data1a/r78/cb3058/dea-notebooks/vegetation_anomalies/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gpd.read_file(\"data/tiles_not_run_DJF_NWQLD.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in x.label:\n",
    "    a.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.setdiff1d(z,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['1988-12-01', '1989-12-01', '1990-12-01']\n",
    "\n",
    "def timesListDCLoad(times, arrs)\n",
    "    arrs = []\n",
    "    query = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "             'lat': (lat - latLon_adjust, lat + latLon_adjust),\n",
    "            'time': times[0]}\n",
    "\n",
    "    dc = datacube.Datacube(app='load_clearlandsat')\n",
    "    ds1 = dc.load(query)\n",
    "    arrs.append(ds1)\n",
    "    \n",
    "    for t in times[1:]:\n",
    "        query = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "             'lat': (lat - latLon_adjust, lat + latLon_adjust),\n",
    "            'time': t}\n",
    "\n",
    "        dc = datacube.Datacube(app='load_clearlandsat')\n",
    "        ds = dc.load(query, like=ds1)\n",
    "        arrs.append(ds)\n",
    "        \n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msavi_func(nir, red):\n",
    "    return (2*nir+1-np.sqrt((2*nir+1)**2 - 8*(nir-red)))/2\n",
    "\n",
    "def msavi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        msavi_func, ds.nir, ds.red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "msavi = msavi_ufunc(ds_mo).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatology = msavi.groupby('time.month').mean('time')\n",
    "\n",
    "anomalies = msavi.groupby('time.month') - climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Functions for weighting months to help with seasonal climatology\n",
    "    dpm = {'noleap': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           '365_day': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'standard': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'gregorian': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'proleptic_gregorian': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'all_leap': [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           '366_day': [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           '360_day': [0, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}\n",
    "\n",
    "    def leap_year(year, calendar='standard'):\n",
    "        \"\"\"Determine if year is a leap year\"\"\"\n",
    "        leap = False\n",
    "        if ((calendar in ['standard', 'gregorian',\n",
    "            'proleptic_gregorian', 'julian']) and\n",
    "            (year % 4 == 0)):\n",
    "            leap = True\n",
    "            if ((calendar == 'proleptic_gregorian') and\n",
    "                (year % 100 == 0) and\n",
    "                (year % 400 != 0)):\n",
    "                leap = False\n",
    "            elif ((calendar in ['standard', 'gregorian']) and\n",
    "                     (year % 100 == 0) and (year % 400 != 0) and\n",
    "                     (year < 1583)):\n",
    "                leap = False\n",
    "        return leap\n",
    "\n",
    "    def get_dpm(time, calendar='standard'):\n",
    "        \"\"\"\n",
    "        return a array of days per month corresponding to the months provided in `months`\n",
    "        \"\"\"\n",
    "        month_length = np.zeros(len(time), dtype=np.int)\n",
    "\n",
    "        cal_days = dpm[calendar]\n",
    "\n",
    "        for i, (month, year) in enumerate(zip(time.month, time.year)):\n",
    "            month_length[i] = cal_days[month]\n",
    "            if leap_year(year, calendar=calendar):\n",
    "                month_length[i] += 1\n",
    "        return month_length\n",
    "\n",
    "    def season_mean(ds, calendar='standard'):\n",
    "        # Make a DataArray of season/year groups\n",
    "        year_season = xr.DataArray(ds.time.to_index().to_period(freq='Q-NOV').to_timestamp(how='E'),\n",
    "                                   coords=[ds.time], name='year_season')\n",
    "\n",
    "        # Make a DataArray with the number of days in each month, size = len(time)\n",
    "        month_length = xr.DataArray(get_dpm(ds.time.to_index(), calendar=calendar),\n",
    "                                    coords=[ds.time], name='month_length')\n",
    "        # Calculate the weights by grouping by 'time.season'\n",
    "        weights = month_length.groupby('time.season') / month_length.groupby('time.season').sum()\n",
    "\n",
    "        # Test that the sum of the weights for each season is 1.0\n",
    "        np.testing.assert_allclose(weights.groupby('time.season').sum().values, np.ones(4))\n",
    "\n",
    "        # Calculate the weighted average\n",
    "        return (ds * weights).groupby('time.season').sum(dim='time')\n",
    "\n",
    "    #calculate the seasonal climatology\n",
    "#     msavi_seasonalClimatology = season_mean(msavi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunked_nbarx(dc, sensor, query, product='nbart', bands_of_interest='', filter_pq=True, dask_chunks={}, like=None):\n",
    "    \n",
    "    product_name = '{}_{}_albers'.format(sensor, product)\n",
    "    mask_product = '{}_{}_albers'.format(sensor, 'pq')\n",
    "    \n",
    "    if like is None:\n",
    "        \n",
    "        # If bands of interest are given, assign measurements in dc.load call\n",
    "        if bands_of_interest:\n",
    "            ds = dc.load(product=product_name, measurements=bands_of_interest,\n",
    "                         group_by='solar_day', **query, dask_chunks=dask_chunks)\n",
    "        # If no bands of interest given, run without specifying measurements\n",
    "        else:\n",
    "            ds = dc.load(product=product_name, group_by='solar_day', **query, dask_chunks=dask_chunks)\n",
    "\n",
    "        # Proceed if the resulting call returns data\n",
    "        if ds.variables:\n",
    "            # If pixel quality filtering is enabled, extract PQ data to use as mask\n",
    "            if filter_pq:\n",
    "                sensor_pq = dc.load(product=mask_product, fuse_func=ga_pq_fuser,\n",
    "                                    group_by='solar_day', **query, dask_chunks=dask_chunks)\n",
    "\n",
    "                # If PQ call returns data, use to mask input data\n",
    "                if sensor_pq.variables:\n",
    "                    good_quality = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                                     cloud_acca='no_cloud',\n",
    "                                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                                     cloud_fmask='no_cloud',\n",
    "                                                     blue_saturated=False,\n",
    "                                                     green_saturated=False,\n",
    "                                                     red_saturated=False,\n",
    "                                                     nir_saturated=False,\n",
    "                                                     swir1_saturated=False,\n",
    "                                                     swir2_saturated=False,\n",
    "                                                     contiguous=True)\n",
    "\n",
    "                    # Apply mask to preserve only good data\n",
    "                    ds = ds.where(good_quality)\n",
    "            # Replace nodata values with nans\n",
    "                ds = masking.mask_invalid_data(ds)\n",
    "            return ds\n",
    "        else:\n",
    "            return None, None, None\n",
    "    \n",
    "    else:\n",
    "        #If bands of interest are given, assign measurements in dc.load call\n",
    "        if bands_of_interest:\n",
    "            ds = dc.load(product=product_name, measurements=bands_of_interest,\n",
    "                         group_by='solar_day',dask_chunks=dask_chunks, like=like)\n",
    "\n",
    "        # If no bands of interest given, run without specifying measurements\n",
    "        else:\n",
    "            ds = dc.load(product=product_name, group_by='solar_day', dask_chunks=dask_chunks, like=like)\n",
    "\n",
    "        # Proceed if the resulting call returns data\n",
    "        if ds.variables:\n",
    "            # If pixel quality filtering is enabled, extract PQ data to use as mask\n",
    "            if filter_pq:\n",
    "                sensor_pq = dc.load(product=mask_product, fuse_func=ga_pq_fuser,\n",
    "                                    group_by='solar_day', dask_chunks=dask_chunks, like=like)\n",
    "\n",
    "                # If PQ call returns data, use to mask input data\n",
    "                if sensor_pq.variables:\n",
    "                    good_quality = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                                     cloud_acca='no_cloud',\n",
    "                                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                                     cloud_fmask='no_cloud',\n",
    "                                                     blue_saturated=False,\n",
    "                                                     green_saturated=False,\n",
    "                                                     red_saturated=False,\n",
    "                                                     nir_saturated=False,\n",
    "                                                     swir1_saturated=False,\n",
    "                                                     swir2_saturated=False,\n",
    "                                                     contiguous=True)\n",
    "\n",
    "                    # Apply mask to preserve only good data\n",
    "                    ds = ds.where(good_quality)\n",
    "            # Replace nodata values with nans\n",
    "                ds = masking.mask_invalid_data(ds)\n",
    "            return ds\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "def load_aligned_nbarx(dc, query, sensors, product='nbart', bands_of_interest='', filter_pq=True, dask_chunks={}):\n",
    "    print('loading ' + sensors[0])\n",
    "    \n",
    "    #list for adding loaded data too\n",
    "    filtered_sensors = []\n",
    "    \n",
    "    #load the first sensor data and append to list\n",
    "    ds1 = load_chunked_nbarx(dc, query, sensor=sensors[0], product=product, bands_of_interest=bands_of_interest,\n",
    "                             filter_pq=filter_pq, dask_chunks=dask_chunks, like=None)\n",
    "    \n",
    "    filtered_sensors.append(ds1)\n",
    "    \n",
    "    #now load the other sensors using the first sensor as the 'like' parameter and append\n",
    "    for sensor in sensors[1:]:\n",
    "        print(\"\\r\", 'loading sensor ' + sensor, end='')\n",
    "        ds = load_chunked_nbarx(dc,sensor=sensors[0], product=product, bands_of_interest=bands_of_interest,\n",
    "                                filter_pq=filter_pq, dask_chunks=dask_chunks, like=ds1)\n",
    "        filtered_sensors.append(ds1)\n",
    "    \n",
    "    # Concatenate all sensors into one big xarray dataset, and then sort by time \n",
    "    print(', concatenating & sorting sensors')\n",
    "    combined_ds = xr.concat(filtered_sensors, dim='time')\n",
    "    combined_ds = combined_ds.sortby('time')                                                               \n",
    "    # Return combined dataset\n",
    "    return combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "         'lat': (lat - latLon_adjust, lat + latLon_adjust),\n",
    "        'time': (start, end)}\n",
    "\n",
    "# query = query_from_shp.query_from_shp(shp_fpath, start, end)\n",
    "dc = datacube.Datacube(app='load_clearlandsat')\n",
    "ds = load_aligned_nbarx(dc=dc, sensors=['ls7', 'ls8'], query=query, bands_of_interest=['nir', 'red'], \n",
    "                        filter_pq=True, dask_chunks = {'x': chunk_size, 'y': chunk_size})\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
