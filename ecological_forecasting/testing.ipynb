{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datacube \n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "import DEADataHandling\n",
    "import query_from_shp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info for parallel processing with Dask\n",
    "1. If reading netcdf files make sure each worker has one thread\n",
    "2. memory_limit is per worker not per cluster of workers\n",
    "3. When launching multiple workers (needed when reading netcdfs) on the same node you have to supply memory limit, otherwise every worker will assume they have all the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='5GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not using a polygon then enter your AOI coords\n",
    "#below:\n",
    "lat, lon = -34.294, 146.037\n",
    "latLon_adjust = 0.10\n",
    "\n",
    "start = '2019-03-01'\n",
    "end = '2019-05-31'\n",
    "\n",
    "shp_fpath = \"/g/data/r78/cb3058/dea-notebooks/dcStats/data/spatial/griffith_MSAVI_test.shp\"\n",
    "chunk_size = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_from_shp.query_from_shp(shp_fpath, start, end)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = {'lon': (lon - latLon_adjust, lon + latLon_adjust),\n",
    "#          'lat': (lat - latLon_adjust, lat + latLon_adjust),\n",
    "#         'time': ('2014-12-01', '2019-05-31'), }\n",
    "\n",
    "query = query_from_shp.query_from_shp(shp_fpath, start, end)\n",
    "dc = datacube.Datacube(app='load_clearlandsat')\n",
    "ds = DEADataHandling.load_clearlandsat(dc=dc, query=query, sensors=['ls5','ls7','ls8'], bands_of_interest=['nir', 'red'],\n",
    "                                       dask_chunks = {'x': chunk_size, 'y': chunk_size}, masked_prop=0.25, mask_pixel_quality=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msavi_func(nir, red):\n",
    "    return (2*nir+1-np.sqrt((2*nir+1)**2 - 8*(nir-red)))/2\n",
    "\n",
    "def msavi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        msavi_func, ds.nir, ds.red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "def compute_seasonal(data):\t\t\n",
    "    msavi = msavi_ufunc(data)\n",
    "    #calculate the MSAVI    \n",
    "    msavi = msavi.resample(time='M').mean('time')\n",
    "    #calculate seasonal climatology\n",
    "    msavi_seasonalClimatology = msavi.groupby('time.season').mean('time')\n",
    "    #resample monthly msavi to seasonal means\n",
    "    msavi_seasonalMeans = msavi.resample(time='QS-DEC').mean('time')\n",
    "    #calculate anomalies\n",
    "    masvi_anomalies = msavi_seasonalMeans.groupby('time.season') - msavi_seasonalClimatology\n",
    "    return masvi_anomalies\n",
    "\n",
    "a = compute_seasonal(ds)\n",
    "a.to_netcdf('results/test.nc')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reopen without dask chunks and it'll plot quickly\n",
    "b = xr.open_dataarray('results/test.nc')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.plot(x='x',y='y', col='time', col_wrap=5, vmin=-0.5,vmax=0.5, cmap='RdBu', figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.mean(['x', 'y']).plot(figsize=(12,5), ylim=(-0.25, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute monthly MSAVI anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly(data):\t\t\n",
    "    #calculate the MSAVI\n",
    "    msavi = xr.DataArray(data = (2*data.nir+1-np.sqrt((2*data.nir+1)**2 - 8*(data.nir-data.red)))/2,\n",
    "                  coords=data.coords,attrs=dict(crs=data.crs))\n",
    "    \n",
    "    msavi = msavi.resample(time='M').mean('time')\n",
    "    \n",
    "    #calculate climatology\n",
    "    climatology = msavi.groupby('time.month').mean('time')\n",
    "    #calculate anomalies\n",
    "    anomalies = msavi.groupby('time.month') - climatology\n",
    "\n",
    "    return anomalies, climatology \n",
    "\n",
    "x, y = compute_monthly(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.plot(x='x', y='y', col='month',col_wrap=4, vmin=0.0,vmax=1.0, figsize=(15,10), cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean(['x', 'y']).plot(figsize=(12,5), ylim=(-0.25, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 anomalies\n",
    "# x.isel(time=range(-17,-5)).plot(x='x',y='y',col='time',col_wrap=4,figsize=(13,10), vmin=-0.5, vmax=0.5, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010 anomalies\n",
    "# x.isel(time=range(265,277)).plot(x='x',y='y',col='time',col_wrap=4,figsize=(13,10), vmin=-0.5, vmax=0.5, cmap='BrBG')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal MSAVi anomalies \n",
    "\n",
    "Not sure this is working as expected. Appears that the mean of the anomalies is not zero as you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_seasonal(data):\t\t\n",
    "    #calculate the MSAVI\n",
    "    msavi = xr.DataArray(data = (2*data.nir+1-np.sqrt((2*data.nir+1)**2 - 8*(data.nir-data.red)))/2,\n",
    "              coords=data.coords,attrs=dict(crs=data.crs))\n",
    "    \n",
    "    msavi = msavi.resample(time='M').mean('time')\n",
    "    #calculate seasonal climatology\n",
    "    msavi_seasonalClimatology = msavi.groupby('time.season').mean('time')\n",
    "    \n",
    "    #resample monthly msavi to seasonal means\n",
    "    msavi_seasonalMeans = msavi.resample(time='QS-DEC').mean('time')\n",
    "    #calculate anomalies\n",
    "    masvi_anomalies = msavi_seasonalMeans.groupby('time.season') - msavi_seasonalClimatology\n",
    "\n",
    "    return masvi_anomalies, msavi_seasonalClimatology\n",
    "\n",
    "a,b=compute_seasonal(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.plot(x='x',y='y', col='season',col_wrap=2, vmin=0,vmax=1.0, figsize=(10,7), cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean(['x', 'y']).plot(figsize=(12,5), ylim=(-0.25, 0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 anomalies\n",
    "# a.isel(time=range(-6,-2)).plot(x='x',y='y',col='time',col_wrap=2,figsize=(13,10), vmin=-0.5, vmax=0.5, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2010 anomalies\n",
    "# a.isel(time=range(-38,-34)).plot(x='x',y='y',col='time',col_wrap=2,figsize=(13,10), vmin=-0.5, vmax=0.5, cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msavi_func(nir, red):\n",
    "    return (2*nir+1-np.sqrt((2*nir+1)**2 - 8*(nir-red)))/2\n",
    "\n",
    "def msavi_ufunc(ds):\n",
    "    return xr.apply_ufunc(\n",
    "        msavi_func, ds.nir, ds.red,\n",
    "        dask='parallelized',\n",
    "        output_dtypes=[float])\n",
    "\n",
    "msavi = msavi_ufunc(ds_mo).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climatology = msavi.groupby('time.month').mean('time')\n",
    "\n",
    "anomalies = msavi.groupby('time.month') - climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Functions for weighting months to help with seasonal climatology\n",
    "    dpm = {'noleap': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           '365_day': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'standard': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'gregorian': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'proleptic_gregorian': [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           'all_leap': [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           '366_day': [0, 31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31],\n",
    "           '360_day': [0, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30]}\n",
    "\n",
    "    def leap_year(year, calendar='standard'):\n",
    "        \"\"\"Determine if year is a leap year\"\"\"\n",
    "        leap = False\n",
    "        if ((calendar in ['standard', 'gregorian',\n",
    "            'proleptic_gregorian', 'julian']) and\n",
    "            (year % 4 == 0)):\n",
    "            leap = True\n",
    "            if ((calendar == 'proleptic_gregorian') and\n",
    "                (year % 100 == 0) and\n",
    "                (year % 400 != 0)):\n",
    "                leap = False\n",
    "            elif ((calendar in ['standard', 'gregorian']) and\n",
    "                     (year % 100 == 0) and (year % 400 != 0) and\n",
    "                     (year < 1583)):\n",
    "                leap = False\n",
    "        return leap\n",
    "\n",
    "    def get_dpm(time, calendar='standard'):\n",
    "        \"\"\"\n",
    "        return a array of days per month corresponding to the months provided in `months`\n",
    "        \"\"\"\n",
    "        month_length = np.zeros(len(time), dtype=np.int)\n",
    "\n",
    "        cal_days = dpm[calendar]\n",
    "\n",
    "        for i, (month, year) in enumerate(zip(time.month, time.year)):\n",
    "            month_length[i] = cal_days[month]\n",
    "            if leap_year(year, calendar=calendar):\n",
    "                month_length[i] += 1\n",
    "        return month_length\n",
    "\n",
    "    def season_mean(ds, calendar='standard'):\n",
    "        # Make a DataArray of season/year groups\n",
    "        year_season = xr.DataArray(ds.time.to_index().to_period(freq='Q-NOV').to_timestamp(how='E'),\n",
    "                                   coords=[ds.time], name='year_season')\n",
    "\n",
    "        # Make a DataArray with the number of days in each month, size = len(time)\n",
    "        month_length = xr.DataArray(get_dpm(ds.time.to_index(), calendar=calendar),\n",
    "                                    coords=[ds.time], name='month_length')\n",
    "        # Calculate the weights by grouping by 'time.season'\n",
    "        weights = month_length.groupby('time.season') / month_length.groupby('time.season').sum()\n",
    "\n",
    "        # Test that the sum of the weights for each season is 1.0\n",
    "        np.testing.assert_allclose(weights.groupby('time.season').sum().values, np.ones(4))\n",
    "\n",
    "        # Calculate the weighted average\n",
    "        return (ds * weights).groupby('time.season').sum(dim='time')\n",
    "\n",
    "    #calculate the seasonal climatology\n",
    "#     msavi_seasonalClimatology = season_mean(msavi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
