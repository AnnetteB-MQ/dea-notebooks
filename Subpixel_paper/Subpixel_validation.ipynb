{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "\n",
    "tidepost_lat, tidepost_lon, date =-15.92129443, 145.36472283, \"2010-11-13T00:40:58.612213Z\"\n",
    "# tidepost_lat, tidepost_lon, date =-18.53226273, 146.35253872, \"2011-05-10T00:58:07.007102Z\"  # 80\n",
    "# tidepost_lat, tidepost_lon, date =-20.86943927, 148.91656844, \"2011-05-27T00:36:48.717232Z\"  # 100\n",
    "\n",
    "# Use the tidal mode to compute tide heights for each observation:\n",
    "# obs_datetimes = ds.time.data.astype('M8[s]').astype('O').tolist()\n",
    "obs_datetimes = [pd.to_datetime(date)]\n",
    "obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "obs_predictedtides = predict_tide(obs_timepoints)\n",
    "\n",
    "# Extract tide height\n",
    "obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "obs_tideheights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(\"2010-11-13T00:40:58.612213Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.storage import masking\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "import datacube\n",
    "# import numpy as np\n",
    "# from datetime import time, datetime\n",
    "# import pandas as pd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "\n",
    "import sys\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools, DEADataHandling\n",
    "\n",
    "def load_cloudmaskedlandsat(dc, query, platform='ls8', bands=['red', 'green', 'blue']):\n",
    "    \n",
    "    '''\n",
    "    This function returns cloud-masked Landsat `*_nbar_scene` data by loading \n",
    "    both Landsat and Landsat pixel quality data and masking out any pixels \n",
    "    affected by cloud, cloud shadow, saturated pixels or any pixels missing data \n",
    "    in any band. For convenience, the resulting data is returned with sensible\n",
    "    band names (e.g. 'red', 'green', 'blue') instead of the unnamed bands in the\n",
    "    original data.\n",
    "    \n",
    "    Last modified: March 2019\n",
    "    Author: Robbi Bishop-Taylor\n",
    "    \n",
    "    Parameters\n",
    "    ----------  \n",
    "    dc : datacube Datacube object\n",
    "        A specific Datacube to import from, i.e. `dc = datacube.Datacube(app='Clear Landsat')`. This allows you to \n",
    "        also use development datacubes if they have been imported into the environment.    \n",
    "    query : dict\n",
    "        A dict containing the query bounds. Can include lat/lon, time etc. If no `time` query is given, the \n",
    "        function defaults to all timesteps available to all sensors (e.g. 1987-2018)\n",
    "    platform : list, optional\n",
    "        An optional Landsat platform name to load data from. Options are 'ls5', 'ls7', 'ls8'.\n",
    "    bands : list, optional\n",
    "        An optional list of strings containing the bands to be read in; options include 'red', 'green', 'blue', \n",
    "        'nir', 'swir1', 'swir2'; defaults to `['red', 'green', 'blue']`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    landsat_ds : xarray Dataset\n",
    "        An xarray dataset containing pixel-quality masked Landsat observations        \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Define dictionary for converting band names between numbered \n",
    "    # (e.g. '2', '3', '4') and named bands (e.g. 'red', 'green', 'blue')\n",
    "    if (platform == 'ls5') or (platform == 'ls7'):\n",
    "    \n",
    "        band_nametonum = {'blue': '1', 'green': '2', 'red': '3', \n",
    "                          'nir': '4', 'swir1': '5', 'swir2': '7'}\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        band_nametonum = {'coastal': '1', 'blue': '2', 'green': '3', \n",
    "                  'red': '4', 'nir': '5', 'swir1': '6', 'swir2': '7'}\n",
    "\n",
    "    \n",
    "    # Test if data is available for query\n",
    "    n_obs = len(dc.find_datasets(product=f'{platform}_nbar_scene', **query))\n",
    "    if n_obs > 0:\n",
    "\n",
    "        print(f'Loading data for {n_obs} {platform} observations')\n",
    "        landsat_ds = dc.load(product=f'{platform}_nbar_scene', \n",
    "                             measurements=[band_nametonum[i] for i in bands],\n",
    "                             group_by='solar_day', \n",
    "                             **query)\n",
    "\n",
    "        print(f'Loading pixel quality data for {n_obs} {platform} observations')\n",
    "        landsat_pq = dc.load(product=f'{platform}_pq_scene', \n",
    "                             group_by='solar_day', \n",
    "                             fuse_func=ga_pq_fuser,\n",
    "                             **query)\n",
    "\n",
    "        print('Masking out poor quality pixels (e.g. cloud)')\n",
    "        good_quality = masking.make_mask(landsat_pq.pqa,                        \n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True)\n",
    "\n",
    "        # Apply pixel quality mask\n",
    "        landsat_ds = landsat_ds.where(good_quality)\n",
    "\n",
    "        # Rename bands to useful names and return data\n",
    "        band_numtoname = {b: a for a, b in band_nametonum.items() if a in bands}\n",
    "        landsat_ds = landsat_ds.rename(band_numtoname)\n",
    "\n",
    "        return landsat_ds\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f'No data was returned for the query {query}. '\n",
    "                        'Please change lat, lon and time extents to an area with data.')\n",
    "        \n",
    "\n",
    "def load_landsatscenes(dc, query, platforms=['ls5', 'ls7', 'ls8'], bands=['red', 'green', 'blue']):\n",
    "    \n",
    "    out = []\n",
    "\n",
    "    for i in platforms:\n",
    "\n",
    "        try:\n",
    "\n",
    "            out.append(load_cloudmaskedlandsat(dc, query, platform=i, bands=bands))\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "\n",
    "    # Concatenate\n",
    "    return xr.concat(out, dim='time').sortby('time')\n",
    "        \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Load in validation data\n",
    "wv2_xr = xr.open_rasterio('/g/data/r78/rt1527/dea-notebooks/Subpixel_paper/raw_data/052743823130_011_geo.tif', chunks={'x': 500, 'y': 500})\n",
    "wv2_xr = wv2_xr.to_dataset(dim='band').rename({1: 'coastal', 2: 'blue', 3: 'green', 4: 'yellow', 5: 'red', 6: 'red_edge', 7: 'nir1', 8: 'nir2'})\n",
    "wv2_xr = wv2_xr.where(wv2_xr > 0)\n",
    "# wv2_xr[['red', 'green', 'blue']].coarsen({'x': 10, 'y': 10}, boundary='trim').mean().to_array().plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2_xr_test = wv2_xr   #.coarsen({'x': 4, 'y': 4}, boundary='trim').mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndwi = ((wv2_xr_test.red - wv2_xr_test.nir1) / (wv2_xr_test.red + wv2_xr_test.nir1)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import affine\n",
    "affine.Affine(*wv2_xr.transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.2\n",
    "\n",
    "contours_gdf = SpatialTools.contour_extract(z_values=[thresh],\n",
    "                                   ds_array=ndwi,\n",
    "                                   ds_crs=wv2_xr.crs[6:],\n",
    "                                   ds_affine=affine.Affine(*wv2_xr.transform),\n",
    "                                   output_shp=f'waterline_wv2_7.shp',\n",
    "                                   min_vertices=2,\n",
    "                                   verbose=False)\n",
    "contours_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv2_xr[['red', 'green', 'blue']].isel(x=slice(100, 2000), y=slice(2000, 4000)).coarsen({'x': 2.5, 'y': 2.5}, boundary='trim').mean().to_array().plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the datacube\n",
    "dc = datacube.Datacube(app='Coastal erosion')\n",
    "\n",
    "# Create 'query' based on the longitudes, latitudes and time provided above\n",
    "query = {'y': (-15.68847186, -16.04168290),\n",
    "         'x': (145.34355739, 145.47173940),\n",
    "         'time': ('2010-10-01', '2010-12-31'),\n",
    "         'output_crs': 'EPSG:28355',\n",
    "         'resolution': (-25, 25)}\n",
    "\n",
    "# Load Landsat 8 data for the time and area in the query. This may take several minutes!\n",
    "ds = load_cloudmaskedlandsat(dc=dc, \n",
    "                             query=query, \n",
    "                             platform='ls5', \n",
    "                             bands=['red', 'green', 'blue', 'nir', 'swir1'])\n",
    "print(ds)\n",
    "\n",
    "# ds['MNDWI'] = (ds.green - ds.swir1) / (ds.green + ds.swir1)\n",
    "ds['MNDWI'] = (ds.green - ds.nir) / (ds.green + ds.nir)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidepost_lat, tidepost_lon, date =-15.92129443, 145.36472283, \"2010-11-13T00:40:58.612213Z\"\n",
    "\n",
    "# Use the tidal mode to compute tide heights for each observation:\n",
    "obs_datetimes = ds.time.data.astype('M8[s]').astype('O').tolist()\n",
    "obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "obs_predictedtides = predict_tide(obs_timepoints)\n",
    "\n",
    "# Extract tide height\n",
    "obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "obs_tideheights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sel = ds.isel(time=2)[['MNDWI']]\n",
    "ds_sel.MNDWI.plot(size=10, aspect=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.0\n",
    "\n",
    "contours_gdf = SpatialTools.contour_extract(z_values=[thresh],\n",
    "                                   ds_array=ds_sel.MNDWI,\n",
    "                                   ds_crs=ds.crs,\n",
    "                                   ds_affine=ds.geobox.transform,\n",
    "                                   output_shp=f'waterline_subpixel.shp',\n",
    "                                   min_vertices=2,  \n",
    "                                   verbose=False)\n",
    "contours_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jagged contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.features import shapes\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, Point\n",
    "import numpy as np\n",
    "\n",
    "ds_bool = (ds_sel.MNDWI > thresh).where(np.isfinite(ds_sel.MNDWI))\n",
    "\n",
    "# Polygonise and filter to keep only values = 1\n",
    "geoms = list(shapes(ds_bool.values.astype(np.int16), transform=ds.geobox.transform))\n",
    "# geoms = [shape(a) for (a, b) in geoms if b == 1.0]\n",
    "geoms_shape = [shape(a) for (a, b) in geoms]\n",
    "data = [b for (a, b) in geoms]\n",
    "\n",
    "\n",
    "polygon = gpd.GeoDataFrame(data={'value': data}, geometry = geoms_shape, crs = str(ds.crs))\n",
    "# polygon_dissolved = polygon.dissolve(by='value')\n",
    "\n",
    "# from shapely.geometry import LineString\n",
    "polygon['geometry'] =  [LineString(i.coords) for i in polygon.geometry.exterior]\n",
    "# polygon.to_file('waterline_blocky.shp')\n",
    "polygon.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "\n",
    "out_strings = []\n",
    "\n",
    "water_poly = polygon[polygon.area == max(polygon.area)].geometry.iloc[0]\n",
    "# polygon_dissolved.iloc[1][0]\n",
    "\n",
    "for i in polygon_dissolved.iloc[0][0][0:500]:\n",
    "    x, y = i.exterior.coords.xy\n",
    "    \n",
    "    good_coords = []\n",
    "    all_coords = []\n",
    "    \n",
    "    for (x, y) in zip(x, y):\n",
    "    \n",
    "        on_border = water_poly.intersects(Point(x, y))\n",
    "        \n",
    "        if on_border:\n",
    "            good_coords.append((x, y))\n",
    "        all_coords.append((x, y))\n",
    "        \n",
    "    out_strings.append(LineString(good_coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_boundaries = gpd.GeoDataFrame(geometry=out_strings)\n",
    "line_boundaries[line_boundaries.length > 0].to_file('waterline_blocky.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[polygon.iloc[[0]].geometry.intersects(Point(a, b))[0] for (a, b) in ii.exterior.coords.xy for ii in polygon.iloc[0][0]]\n",
    "\n",
    "\n",
    "# polygon.iloc[[0]].geometry.intersects(Point(x[5], y[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [(j, k) for j in s1 for k in s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon.iloc[[0]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# contours_gdf.interpolate(np.array([20, 40]))\n",
    "import geopandas as gpd\n",
    "\n",
    "# Import validation dataset \n",
    "validation_studyarea = gpd.read_file('/g/data/r78/rt1527/dea-notebooks/Subpixel_paper/raw_data/woldview_validation_studyarea_sub.shp')\n",
    "validation_shoreline = gpd.read_file('/g/data/r78/rt1527/dea-notebooks/Subpixel_paper/raw_data/woldview_validation_shoreline.shp')\n",
    "validation_shoreline_feat = validation_shoreline.geometry.unary_union\n",
    "# validation_shoreline_buff = validation_shoreline_feat.buffer(500)\n",
    "\n",
    "\n",
    "def points_along_line(line_gdf, dist=5):    \n",
    "\n",
    "    out = [] \n",
    "    \n",
    "    # create points every 30 meters along the line\n",
    "    for i, distance in enumerate(range(0, int(line_gdf.length), dist)):\n",
    "        point = line_gdf.interpolate(distance)\n",
    "        out.append(point[0])\n",
    "        \n",
    "    subpixel_points = gpd.GeoDataFrame(geometry=out, crs=line_gdf.crs)\n",
    "    return subpixel_points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_gdf['geometry'] = contours_gdf.geometry.intersection(validation_studyarea.geometry)\n",
    "subpixel_points = points_along_line(contours_gdf, dist=5)\n",
    "subpixel_points['dist_val'] = subpixel_points.apply(lambda x: x.geometry.distance(validation_shoreline_feat), axis=1)\n",
    "subpixel_points.to_file('subpixel_dists.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocky_gdf = gpd.GeoDataFrame(geometry=[polygon.unary_union], crs=str(ds.crs))\n",
    "blocky_gdf['geometry'] = blocky_gdf.geometry.intersection(validation_studyarea.geometry)\n",
    "blocky_points = points_along_line(blocky_gdf, dist=5)\n",
    "blocky_points['dist_val'] = blocky_points.apply(lambda x: x.geometry.distance(validation_shoreline_feat), axis=1)\n",
    "blocky_points.to_file('blocky_dists.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpixel_points.dist_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocky_points.dist_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpixel_points.dist_val.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocky_points.dist_val.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((subpixel_points.dist_val ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt((blocky_points.dist_val ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpixel_points.dist_val.hist(bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocky_points.dist_val.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Landsat 7 scenes with the Scan Line Correction (SLC) missing data\n",
    "LS7_BROKEN_DATE = datetime(2003, 5, 31)\n",
    "is_pre_slc_failure = lambda dataset: dataset.center_time < LS7_BROKEN_DATE\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "#Create function to ensure wofls in correct format\n",
    "def wofls_fuser(dest, src):\n",
    "    where_nodata = (src & 1) == 0\n",
    "    np.copyto(dest, src, where=where_nodata)\n",
    "    return dest\n",
    "\n",
    "#Create virtual product so that datacube data can be loaded effectively within memory\n",
    "fc_and_water_yaml = \"\"\"\n",
    "        juxtapose:\n",
    "          - collate:\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls5_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls5_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls7_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                        dataset_predicate: __main__.is_pre_slc_failure\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls7_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls8_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls8_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "          - transform: make_mask\n",
    "            input:\n",
    "                product: wofs_albers\n",
    "                group_by: solar_day\n",
    "                fuse_func: __main__.wofls_fuser\n",
    "            flags:\n",
    "                wet: true\n",
    "            mask_measurement_name: water\n",
    "\"\"\"\n",
    "\n",
    "fc_and_water = construct_from_yaml(fc_and_water_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test = fc_and_water.load(dc, **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test.isel(time=6).water.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
