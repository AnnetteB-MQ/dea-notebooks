{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gdal\n",
    "import zipfile\n",
    "import numexpr\n",
    "import datetime\n",
    "import requests\n",
    "import warnings\n",
    "import odc.algo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from collections import Counter\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "import sys\n",
    "import datacube    \n",
    "sys.path.append('../../Scripts')\n",
    "from dea_plotting import animated_timeseries\n",
    "from dea_dask import create_local_dask_cluster\n",
    "\n",
    "dc = datacube.Datacube(app='Animated timeseries')\n",
    "\n",
    "# # Configure local dask cluster\n",
    "# # create_local_dask_cluster()\n",
    "\n",
    "\n",
    "def _dc_query_only(**kw):\n",
    "    \"\"\"\n",
    "    Remove load-only parameters, the rest can be passed to Query\n",
    "    Returns\n",
    "    -------\n",
    "    dict of query parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def _impl(measurements=None,\n",
    "              output_crs=None,\n",
    "              resolution=None,\n",
    "              resampling=None,\n",
    "              skip_broken_datasets=None,\n",
    "              dask_chunks=None,\n",
    "              fuse_func=None,\n",
    "              align=None,\n",
    "              datasets=None,\n",
    "              progress_cbk=None,\n",
    "              group_by=None,\n",
    "              **query):\n",
    "        return query\n",
    "\n",
    "    return _impl(**kw)\n",
    "\n",
    "\n",
    "def _common_bands(dc, products):\n",
    "    \"\"\"\n",
    "    Takes a list of products and returns a list of measurements/bands\n",
    "    that are present in all products\n",
    "    Returns\n",
    "    -------\n",
    "    List of band names\n",
    "    \"\"\"\n",
    "    common = None\n",
    "    bands = None\n",
    "\n",
    "    for p in products:\n",
    "        p = dc.index.products.get_by_name(p)\n",
    "        if common is None:\n",
    "            common = set(p.measurements)\n",
    "            bands = list(p.measurements)\n",
    "        else:\n",
    "            common = common.intersection(set(p.measurements))\n",
    "    return [band for band in bands if band in common]\n",
    "\n",
    "\n",
    "def load_ard(dc,\n",
    "             products=None,\n",
    "             min_gooddata=0.0,\n",
    "             fmask_categories=['valid', 'snow', 'water'],\n",
    "             mask_pixel_quality=True,\n",
    "             mask_contiguity=False,\n",
    "             ls7_slc_off=True,\n",
    "             predicate=None,\n",
    "             dtype='auto',\n",
    "             **kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    Loads and combines Landsat Collection 3 or Sentinel 2 Definitive \n",
    "    and Near Real Time data for multiple sensors (i.e. ls5t, ls7e and \n",
    "    ls8c for Landsat; s2a and s2b for Sentinel 2), optionally applies \n",
    "    pixel quality and contiguity masks, and drops time steps that \n",
    "    contain greater than a minimum proportion of good quality (e.g. non-\n",
    "    cloudy or shadowed) pixels. \n",
    "    The function supports loading the following DEA products:\n",
    "    \n",
    "        ga_ls5t_ard_3\n",
    "        ga_ls7e_ard_3\n",
    "        ga_ls8c_ard_3\n",
    "        s2a_ard_granule\n",
    "        s2b_ard_granule\n",
    "        s2a_nrt_granule\n",
    "        s2b_nrt_granule\n",
    "    Last modified: March 2020\n",
    "    Parameters\n",
    "    ----------\n",
    "    dc : datacube Datacube object\n",
    "        The Datacube to connect to, i.e. `dc = datacube.Datacube()`.\n",
    "        This allows you to also use development datacubes if required.\n",
    "    products : list\n",
    "        A list of product names to load data from. Valid options are\n",
    "        ['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'] for Landsat,\n",
    "        ['s2a_ard_granule', 's2b_ard_granule'] for Sentinel 2 Definitive,\n",
    "        and ['s2a_nrt_granule', 's2b_nrt_granule'] for Sentinel 2 Near\n",
    "        Real Time (on the DEA Sandbox only).\n",
    "    min_gooddata : float, optional\n",
    "        An optional float giving the minimum percentage of good quality\n",
    "        pixels required for a satellite observation to be loaded.\n",
    "        Defaults to 0.0 which will return all observations regardless of\n",
    "        pixel quality (set to e.g. 0.99 to return only observations with\n",
    "        more than 99% good quality pixels).\n",
    "    fmask_categories : list, optional\n",
    "        An optional list of fmask category names to treat as good\n",
    "        quality pixels in the above `min_gooddata` calculation, and for\n",
    "        masking data by pixel quality (if `mask_pixel_quality=True`).\n",
    "        The default is `['valid', 'snow', 'water']` which will return\n",
    "        non-cloudy or shadowed land, snow and water pixels. Choose from:\n",
    "        'nodata', 'valid', 'cloud', 'shadow', 'snow', and 'water'.\n",
    "    mask_pixel_quality : bool, optional\n",
    "        An optional boolean indicating whether to mask out poor quality\n",
    "        pixels using fmask based on the `fmask_categories` provided \n",
    "        above. The default is True, which will set poor quality pixels \n",
    "        to NaN if `dtype='auto'` (which will convert the data to \n",
    "        'float32'), or set poor quality pixels to the data's native \n",
    "        nodata value if `dtype='native' (which can be useful for \n",
    "        reducing memory).         \n",
    "    mask_contiguity : str or bool, optional\n",
    "        An optional string or boolean indicating whether to mask out\n",
    "        pixels missing data in any band (i.e. \"non-contiguous\" values).\n",
    "        This can be important for generating clean composite datasets. \n",
    "        The default is False, which will ignore non-contiguous values \n",
    "        completely. If loading NBART data, set the parameter to:       \n",
    "        `mask_contiguity='nbart_contiguity'`. If loading NBAR data, \n",
    "        specify `mask_contiguity='nbar_contiguity'` instead. \n",
    "        Non-contiguous pixels will be set to NaN if `dtype='auto'`, or \n",
    "        set to the data's native nodata value if `dtype='native'` \n",
    "        (which can be useful for reducing memory).\n",
    "    dtype : string, optional\n",
    "        An optional parameter that controls the data type/dtype that\n",
    "        layers are coerced to after loading. Valid values: 'native', \n",
    "        'auto', 'float{16|32|64}'. When 'auto' is used, the data will be \n",
    "        converted to `float32` if masking is used, otherwise data will \n",
    "        be returned in the native data type of the data. Be aware that\n",
    "        if data is loaded in its native dtype, nodata and masked \n",
    "        pixels will be returned with the data's native nodata value \n",
    "        (typically -999), not NaN. \n",
    "    ls7_slc_off : bool, optional\n",
    "        An optional boolean indicating whether to include data from\n",
    "        after the Landsat 7 SLC failure (i.e. SLC-off). Defaults to\n",
    "        True, which keeps all Landsat 7 observations > May 31 2003.\n",
    "    predicate : function, optional\n",
    "        An optional function that can be passed in to restrict the\n",
    "        datasets that are loaded by the function. A predicate function\n",
    "        should take a `datacube.model.Dataset` object as an input (i.e.\n",
    "        as returned from `dc.find_datasets`), and return a boolean.\n",
    "        For example, a predicate function could be used to return True \n",
    "        for only datasets acquired in January:\n",
    "        `dataset.time.begin.month == 1`\n",
    "    **kwargs :\n",
    "        A set of keyword arguments to `dc.load` that define the\n",
    "        spatiotemporal query and load parameters used to extract data. \n",
    "        Keyword arguments can either be listed directly in the \n",
    "        `load_ard` call like any other parameter (e.g. \n",
    "        `measurements=['nbart_red']`), or by passing in a query kwarg \n",
    "        dictionary (e.g. `**query`). Keywords can include `measurements`, \n",
    "        `x`, `y`, `time`, `resolution`, `resampling`, `group_by`, `crs`;\n",
    "        see the `dc.load` documentation for all possible options:\n",
    "        https://datacube-core.readthedocs.io/en/latest/dev/api/generate/datacube.Datacube.load.html\n",
    "    Returns\n",
    "    -------\n",
    "    combined_ds : xarray Dataset\n",
    "        An xarray dataset containing only satellite observations that\n",
    "        contains greater than `min_gooddata` proportion of good quality\n",
    "        pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    #########\n",
    "    # Setup #\n",
    "    #########\n",
    "\n",
    "    # Use 'nbart_contiguity' by default if mask_contiguity is true\n",
    "    if mask_contiguity is True:\n",
    "        mask_contiguity = 'nbart_contiguity'\n",
    "\n",
    "    # We deal with `dask_chunks` separately\n",
    "    dask_chunks = kwargs.pop('dask_chunks', None)\n",
    "    requested_measurements = kwargs.pop('measurements', None)\n",
    "\n",
    "    # Warn user if they combine lazy load with min_gooddata\n",
    "    if (min_gooddata > 0.0) and dask_chunks is not None:\n",
    "        warnings.warn(\"Setting 'min_gooddata' percentage to > 0.0 \"\n",
    "                      \"will cause dask arrays to compute when \"\n",
    "                      \"loading pixel-quality data to calculate \"\n",
    "                      \"'good pixel' percentage. This can \"\n",
    "                      \"slow the return of your dataset.\")\n",
    "\n",
    "    # Verify that products were provided, and determine if Sentinel-2\n",
    "    # or Landsat data is being loaded\n",
    "    if not products:\n",
    "        raise ValueError(\"Please provide a list of product names \"\n",
    "                         \"to load data from. Valid options are: \\n\"\n",
    "                         \"['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'] \"\n",
    "                         \"for Landsat, ['s2a_ard_granule', \"\n",
    "                         \"'s2b_ard_granule'] \\nfor Sentinel 2 Definitive, or \"\n",
    "                         \"['s2a_nrt_granule', 's2b_nrt_granule'] for \"\n",
    "                         \"Sentinel 2 Near Real Time\")\n",
    "    elif all(['ls' in product for product in products]):\n",
    "        product_type = 'ls'\n",
    "    elif all(['s2' in product for product in products]):\n",
    "        product_type = 's2'\n",
    "\n",
    "    fmask_band = 'fmask'\n",
    "    measurements = requested_measurements.copy() if requested_measurements else None\n",
    "\n",
    "    if measurements is None:\n",
    "        \n",
    "        # Deal with \"load all\" case: pick a set of bands common across \n",
    "        # all products\n",
    "        measurements = _common_bands(dc, products)\n",
    "\n",
    "        # If no `measurements` are specified, Landsat ancillary bands are \n",
    "        # loaded with a 'oa_' prefix, but Sentinel-2 bands are not. As a \n",
    "        # work-around, we need to rename the default contiguity and fmask \n",
    "        # bands if loading Landsat data without specifying `measurements`\n",
    "        if product_type == 'ls':\n",
    "            mask_contiguity = f'oa_{mask_contiguity}' if mask_contiguity else False\n",
    "            fmask_band = f'oa_{fmask_band}'\n",
    "\n",
    "    # If `measurements` are specified but do not include fmask or\n",
    "    # contiguity variables, add these to `measurements`\n",
    "    if fmask_band not in measurements:\n",
    "        measurements.append(fmask_band)\n",
    "    if mask_contiguity and mask_contiguity not in measurements:\n",
    "        measurements.append(mask_contiguity)\n",
    "\n",
    "    # Get list of data and mask bands so that we can later exclude\n",
    "    # mask bands from being masked themselves\n",
    "    data_bands = [band for band in measurements if band not in (fmask_band, mask_contiguity)]\n",
    "    mask_bands = [band for band in measurements if band not in data_bands]\n",
    "\n",
    "    #################\n",
    "    # Find datasets #\n",
    "    #################\n",
    "\n",
    "    # Pull out query params only to pass to dc.find_datasets\n",
    "    query = _dc_query_only(**kwargs)\n",
    "    \n",
    "    # Extract datasets for each product using subset of dcload_kwargs\n",
    "    dataset_list = []\n",
    "\n",
    "    # Get list of datasets for each product\n",
    "    print('Finding datasets')\n",
    "    for product in products:\n",
    "\n",
    "        # Obtain list of datasets for product\n",
    "        print(f'    {product} (ignoring SLC-off observations)' \n",
    "              if not ls7_slc_off and product == 'ga_ls7e_ard_3' \n",
    "              else f'    {product}')\n",
    "        datasets = dc.find_datasets(product=product, **query)\n",
    "\n",
    "        # Remove Landsat 7 SLC-off observations if ls7_slc_off=False\n",
    "        if not ls7_slc_off and product == 'ga_ls7e_ard_3':\n",
    "            datasets = [i for i in datasets if i.time.begin <\n",
    "                        datetime.datetime(2003, 5, 31)]\n",
    "\n",
    "        # Add any returned datasets to list\n",
    "        dataset_list.extend(datasets)\n",
    "\n",
    "    # Raise exception if no datasets are returned\n",
    "    if len(dataset_list) == 0:\n",
    "        raise ValueError(\"No data available for query: ensure that \"\n",
    "                         \"the products specified have data for the \"\n",
    "                         \"time and location requested\")\n",
    "\n",
    "    # If predicate is specified, use this function to filter the list\n",
    "    # of datasets prior to load\n",
    "    if predicate:\n",
    "        print(f'Filtering datasets using predicate function')\n",
    "        dataset_list = [ds for ds in dataset_list if predicate(ds)]\n",
    "\n",
    "    # Raise exception if filtering removes all datasets\n",
    "    if len(dataset_list) == 0:\n",
    "        raise ValueError(\"No data available after filtering with \"\n",
    "                         \"predicate function\")\n",
    "\n",
    "    #############\n",
    "    # Load data #\n",
    "    #############\n",
    "\n",
    "    # Note we always load using dask here so that we can lazy load data \n",
    "    # before filtering by good data\n",
    "    ds = dc.load(datasets=dataset_list,\n",
    "                 measurements=measurements,\n",
    "                 dask_chunks={} if dask_chunks is None else dask_chunks,\n",
    "                 **kwargs)\n",
    "\n",
    "    ####################\n",
    "    # Filter good data #\n",
    "    ####################\n",
    "\n",
    "    # Calculate pixel quality mask\n",
    "    pq_mask = odc.algo.fmask_to_bool(ds[fmask_band],\n",
    "                                     categories=fmask_categories)\n",
    "\n",
    "    # The good data percentage calculation has to load in all `fmask`\n",
    "    # data, which can be slow. If the user has chosen no filtering\n",
    "    # by using the default `min_gooddata = 0`, we can skip this step\n",
    "    # completely to save processing time\n",
    "    if min_gooddata > 0.0:\n",
    "\n",
    "        # Compute good data for each observation as % of total pixels\n",
    "        print('Counting good quality pixels for each time step')\n",
    "        data_perc = (pq_mask.sum(axis=[1, 2], dtype='int32') /\n",
    "                     (pq_mask.shape[1] * pq_mask.shape[2]))\n",
    "        keep = data_perc >= min_gooddata\n",
    "\n",
    "        # Filter by `min_gooddata` to drop low quality observations\n",
    "        total_obs = len(ds.time)\n",
    "        ds = ds.sel(time=keep)\n",
    "        pq_mask = pq_mask.sel(time=keep)\n",
    "\n",
    "        print(f'Filtering to {len(ds.time)} out of {total_obs} '\n",
    "              f'time steps with at least {min_gooddata:.1%} '\n",
    "              f'good quality pixels')\n",
    "        \n",
    "    ###############\n",
    "    # Apply masks #\n",
    "    ###############      \n",
    "    \n",
    "    # Create an overall mask to hold both pixel quality and contiguity\n",
    "    mask = None    \n",
    "    \n",
    "    # Add pixel quality mask to overall mask\n",
    "    if mask_pixel_quality:\n",
    "        print('Applying pixel quality/cloud mask')\n",
    "        mask = pq_mask\n",
    "\n",
    "    # Add contiguity mask to overall mask\n",
    "    if mask_contiguity:\n",
    "        print('Applying contiguity mask')\n",
    "        cont_mask = ds[mask_contiguity] == 1\n",
    "\n",
    "        # If mask already has data if mask_pixel_quality == True,\n",
    "        # multiply with cont_mask to perform a logical 'or' operation\n",
    "        # (keeping only pixels good in both)\n",
    "        mask = cont_mask if mask is None else mask * cont_mask\n",
    "\n",
    "    # Split into data/masks bands, as conversion to float and masking \n",
    "    # should only be applied to data bands\n",
    "    ds_data = ds[data_bands]\n",
    "    ds_masks = ds[mask_bands]\n",
    "\n",
    "    # Mask data if either of the above masks were generated\n",
    "    if mask is not None:\n",
    "        ds_data = odc.algo.keep_good_only(ds_data, where=mask)\n",
    "\n",
    "    # Automatically set dtype to either native or float32 depending\n",
    "    # on whether masking was requested\n",
    "    if dtype == 'auto':\n",
    "        dtype = 'native' if mask is None else 'float32'\n",
    "    \n",
    "    # Set nodata values using odc.algo tools to reduce peak memory\n",
    "    # use when converting data dtype    \n",
    "    if dtype != 'native':\n",
    "        ds_data = odc.algo.to_float(ds_data, dtype=dtype)\n",
    "\n",
    "    # Put data and mask bands back together\n",
    "    attrs = ds.attrs\n",
    "    ds = xr.merge([ds_data, ds_masks])\n",
    "    ds.attrs.update(attrs)\n",
    "    \n",
    "    ###############\n",
    "    # Return data #\n",
    "    ###############\n",
    "\n",
    "    # Drop bands not originally requested by user\n",
    "    if requested_measurements:\n",
    "        ds = ds[requested_measurements]\n",
    "\n",
    "    # If user supplied dask_chunks, return data as a dask array without\n",
    "    # actually loading it in\n",
    "    if dask_chunks is not None:\n",
    "        print(f'Returning {len(ds.time)} time steps as a dask array')\n",
    "        return ds\n",
    "    else:\n",
    "        print(f'Loading {len(ds.time)} time steps')\n",
    "        return ds.compute()\n",
    "    \n",
    "\n",
    "from skimage import exposure\n",
    "from skimage.color import rgb2hsv, hsv2rgb\n",
    "from skimage.filters import unsharp_mask\n",
    "\n",
    "def hsv_image_processing(rgb_array,\n",
    "                         hue_mult=1, sat_mult=1.035, val_mult=1.035,\n",
    "                         unsharp_radius1=20, unsharp_amount1=2.5, \n",
    "                         unsharp_radius2=1, unsharp_amount2=1.0):   \n",
    "    \n",
    "    # Convert to HSV and multiply bands\n",
    "    hsv_array = rgb2hsv(rgb_array)\n",
    "    hsv_array[:, :, 0] = hsv_array[:, :, 0] * hue_mult\n",
    "    hsv_array[:, :, 1] = hsv_array[:, :, 1] * sat_mult\n",
    "    hsv_array[:, :, 2] = hsv_array[:, :, 2] * val_mult\n",
    "    \n",
    "    # Apply unsharp mask and take average\n",
    "    a = unsharp_mask(hsv_array[:, :, 2], radius=unsharp_radius1, amount=unsharp_amount1)\n",
    "    b = unsharp_mask(hsv_array[:, :, 2], radius=unsharp_radius2, amount=unsharp_amount2)\n",
    "    hsv_array[:, :, 2] = np.mean(np.array([a, b]), axis=0)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    return hsv2rgb(hsv_array.clip(0, 1))\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.ui import select_on_a_map\n",
    "    \n",
    "# Plot interactive map to select area\n",
    "geopolygon = select_on_a_map(height='600px', \n",
    "                             center=(-26, 135), \n",
    "                             zoom=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dea_datahandling import mostcommon_crs\n",
    "\n",
    "# # Obtain native CRS \n",
    "# crs = mostcommon_crs(dc=dc, \n",
    "#                      product='ga_ls5t_ard_3', \n",
    "#                      query={'time': '1990', \n",
    "#                             'geopolygon': geopolygon})\n",
    "\n",
    "# query = {\n",
    "#     'geopolygon': geopolygon,\n",
    "#     'time': ('1988', '2018'),\n",
    "#     'measurements': ['nbart_red', 'nbart_green', 'nbart_blue'],\n",
    "#     'resampling': {'fmask': 'nearest', \n",
    "#                    'oa_fmask': 'nearest', \n",
    "#                    '*': 'average'},\n",
    "#     'output_crs': crs,\n",
    "#     'resolution': (-30, 30),\n",
    "#     'group_by': 'solar_day',\n",
    "#     'dask_chunks': {'x': 1000, 'y': 1000}\n",
    "# }\n",
    "\n",
    "# ds = load_ard(dc=dc,\n",
    "#               products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'],\n",
    "#               min_gooddata=0.90,\n",
    "#               ls7_slc_off=False,\n",
    "#               mask_contiguity=False,\n",
    "#               **query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.compute()\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute rolling median\n",
    "# ds_rolling = ds.rolling(time=25, center=True, min_periods=1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "# image_proc_func = partial(hsv_image_processing, val_mult=1.0, sat_mult=1.3, \n",
    "#                           unsharp_radius1=75, unsharp_amount1=0.6,\n",
    "#                           unsharp_radius2=4, unsharp_amount2=0.8)\n",
    "\n",
    "\n",
    "# # Produce time series animation of red, green and blue bands\n",
    "# animated_timeseries(ds=ds_rolling, \n",
    "#                     output_path='animated_timeseries.mp4',  \n",
    "#                     bands=['nbart_red', 'nbart_green', 'nbart_blue'],\n",
    "#                     interval=40, \n",
    "#                     width_pixels=700,\n",
    "# #                     title='Time-series animation',\n",
    "#                     percentile_stretch=[0.025, 0.985],\n",
    "#                     annotation_kwargs={'fontsize': 20},\n",
    "#                     onebandplot_kwargs={'interpolation': 'nearest'},\n",
    "#                     image_proc_func=image_proc_func) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
