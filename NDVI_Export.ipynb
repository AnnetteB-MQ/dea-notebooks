{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General advice (delete this cell before submitting for review)\n",
    "\n",
    "> * When choosing a location for your analysis, **select an area that has data on both the NCI and DEA Sandbox** to allow your code to be run on both environments. \n",
    "For example, you can check this for Landsat using the [DEA Explorer](https://explorer.sandbox.dea.ga.gov.au/ga_ls5t_ard_3/1990) (use the drop-down menu to view all products).\n",
    "As of September 2019, the `DEA Sandbox` has a single year of continental Landsat data for 2015-16, and the full 1987-onward time-series for three locations (Perth WA, Brisbane QLD, and western NSW).\n",
    "> * When adding **Products used**, embed the hyperlink to that specific product on the DEA Explorer using the `[product_name](product url)` syntax.\n",
    "> * When writing in Markdown cells, start each sentence on a **new line**.\n",
    "This makes it easy to see changes through git commits.\n",
    "> * Use Australian English in markdown cells and code comments.\n",
    "> * Check the [known issues](https://github.com/GeoscienceAustralia/dea-docs/wiki/Known-issues) for formatting regarding the conversion of notebooks to DEA docs using Sphinx.\n",
    "Things to be aware of:\n",
    "    * Sphinx is highly sensitive to bulleted lists:\n",
    "        * Ensure that there is an empty line between any preceding text and the list\n",
    "        * Only use the `*` bullet (`-` is not recognised)\n",
    "        * Sublists must be indented by 4 spaces\n",
    "    * Two kinds of formatting cannot be used simultaneously:\n",
    "        * Hyperlinked code: \\[\\`code_format\\`](hyperlink) fails\n",
    "        * Bolded code: \\*\\*\\`code_format\\`\\*\\* fails\n",
    "    * Headers must appear in heirachical order (`#`, `##`, `###`, `####`) and there can only be one title (`#`).\n",
    "> * Use the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) for code. To make sure all code in the notebook is consistent, you can use the `jupyterlab_code_formatter` tool: select each code cell, then click `Edit` and then one of the `Apply X Formatter` options (`YAPF` or `Black` are recommended). This will reformat the code in the cell to a consistent style.\n",
    "> * For additional guidance, refer to the style conventions and layouts in approved `develop` branch notebooks. \n",
    "Examples include\n",
    "    * [Frequently_used_code/Using_load_ard.ipynb](./Frequently_used_code/Using_load_ard.ipynb)\n",
    "    * [Real_world_examples/Coastal_erosion.ipynb](./Real_world_examples/Coastal_erosion.ipynb)\n",
    "    * [Scripts/dea_datahandling.py](./Scripts/dea_datahandling.py)\n",
    "> * The DEA Image placed in the title cell will display as long as the notebook is contained in one of the standard directories.\n",
    "It does not work in the highest level directory (hence why it doesn't display in the original template notebook).\n",
    "> * In the final notebook cell, include a set of relevant tags which are used to build the DEA User Guide's [Tag Index](https://docs.dea.ga.gov.au/genindex.html). \n",
    "Use all lower-case (unless the tag is an acronym), separate words with spaces (unless it is the name of an imported module), and [re-use existing tags](https://github.com/GeoscienceAustralia/dea-notebooks/wiki/List-of-tags).\n",
    "Ensure the tags cell below is in `Raw` format, rather than `Markdown` or `Code`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Landsat Collection 3 vegetation-related indicies <img align=\"right\" src=\"../Supplementary_data/dea_logo.jpg\">\n",
    "\n",
    "* **Compatability:** Notebook currently compatible with the `NCI`|`DEA Sandbox` environment only\n",
    "* **Products used:** \n",
    "[ga_ls5t_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls5t_ard_3),\n",
    "[ga_ls7e_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls7e_ard_3),\n",
    "[ga_ls8c_ard_3](https://explorer.sandbox.dea.ga.gov.au/ga_ls8c_ard_3)\n",
    "* **Special requirements:** An _optional_ description of any special requirements, e.g. If running on the [NCI](https://nci.org.au/), ensure that `module load otps` is run prior to launching this notebook\n",
    "* **Prerequisites:** An _optional_ list of any notebooks that should be run or content that should be understood prior to launching this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "An *optional* overview of the scientific, economic or environmental management issue or challenge being addressed by Digital Earth Australia. \n",
    "For `Beginners_Guide` or `Frequently_Used_Code` notebooks, this may include information about why the particular technique or approach is useful or required. \n",
    "If you need to cite a scientific paper or link to a website, use a persistent DOI link if possible and link in-text (e.g. [Dhu et al. 2017](https://doi.org/10.1080/20964471.2017.1402490))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "A _compulsory_ description of the notebook, including a brief overview of how Digital Earth Australia helps to address the problem set out above.\n",
    "It can be good to include a run-down of the tools/methods that will be demonstrated in the notebook:\n",
    "\n",
    "1. First we do this\n",
    "2. Then we do this\n",
    "3. Finally we do this\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Provide any particular instructions that the user might need, e.g. To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis.\n",
    "\n",
    "Use standard import commands; some are shown below. \n",
    "Begin with any `iPython` magic commands, followed by standard Python packages, then any additional functionality you need from the `Scripts` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import dask\n",
    "from dask.utils import parse_bytes\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import datacube\n",
    "from datacube.storage import masking\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils.dask import start_local_dask\n",
    "\n",
    "from psutil import virtual_memory, cpu_count\n",
    "\n",
    "# Load custom DEA notebook functions\n",
    "sys.path.append('../dea-notebooks/Scripts')\n",
    "import dea_datahandling\n",
    "import dea_plotting\n",
    "import DEADataHandling\n",
    "from dea_bandindices import calculate_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube\n",
    "\n",
    "Connect to the datacube so we can access DEA data.\n",
    "The `app` parameter is a unique name for the analysis which is based on the notebook file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dc_landsat3 = datacube.Datacube(app='NDVI_Export', env='c3-samples')\n",
    "except:\n",
    "    dc_landsat3 = datacube.Datacube(app='NDVI_Export')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n",
    "\n",
    "* `dry_months`: Specific months of the year that correspond with dry season/low rainfall conditions. Values range from 0-11. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_months = [5,6,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define spatial and temporal query\n",
    "\n",
    "If running this notebook locally, use the smaller spatial extent and subset of the time series. \n",
    "\n",
    "If running on gadi, the the temporal and spatial extent can be increased.  \n",
    "\n",
    "> **Note:** Landsat imagery is available from 1987 onwards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial loop to divide up the data processing and reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series of coordinates (with a fixed increment; 0.10 deg)\n",
    "coords_lon = np.arange(132.07, 135.46, 0.05)\n",
    "coords_lat = np.arange(-20.31, -22.11, -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ga_ls5t_ard_3 data\n",
      "    Filtering to 354 out of 566 observations\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Loading ga_ls7e_ard_3 data\n",
      "    Filtering to 337 out of 682 observations\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Loading ga_ls8c_ard_3 data\n",
      "    Filtering to 105 out of 236 observations\n",
      "    Applying pixel quality/cloud mask\n",
      "    Applying invalid data mask\n",
      "    Applying contiguity mask\n",
      "Combining and sorting data\n",
      "    Returning 796 observations \n"
     ]
    },
    {
     "ename": "RasterBlockError",
     "evalue": "The height and width of dataset blocks must be multiples of 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRasterBlockError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-58d011a30dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Writing data to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mwrite_geotiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Creating an associated metadata file. w - writes, r - reads, a- appends\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20191127/lib/python3.6/site-packages/datacube/helpers.py\u001b[0m in \u001b[0;36mwrite_geotiff\u001b[0;34m(filename, dataset, profile_override, time_index)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0m_calculate_blocksize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_vars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbandnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20191127/lib/python3.6/site-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m                                               \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                                               \u001b[0msharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                                               **kwargs)\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             raise ValueError(\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetWriterBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterBlockError\u001b[0m: The height and width of dataset blocks must be multiples of 16"
     ]
    }
   ],
   "source": [
    "for i in range(coords_lon.size-1):\n",
    "    i = 11\n",
    "    for j in range(coords_lat.size-1):\n",
    "        j = 13\n",
    "        query_3 = {'lon': coords_lon[i:(i+1)],\n",
    "        'lat': coords_lat[j:(j+1)],             # full study area\n",
    "        #'time':('2015-01', '2018-12'),       # subset of time-series\n",
    "        'time':('1987-01', '2018-12'),       # full time-series\n",
    "        'output_crs': 'EPSG:28352',\n",
    "        'resolution': (30, 30),\n",
    "        'group_by': 'solar_day'\n",
    "        }   \n",
    " \n",
    "        # Load Landsat data from Collection 3 using .load_ard.\n",
    "        # mask_dtype = np.float16 helps to keep the memory down, however, the data will need to be converted back to float34 later.\n",
    "        ds = dea_datahandling.load_ard(dc=dc_landsat3,\n",
    "        mask_dtype = np.float16,\n",
    "        products=['ga_ls5t_ard_3', 'ga_ls7e_ard_3', 'ga_ls8c_ard_3'], \n",
    "        measurements=['nbart_red','nbart_nir','nbart_green',\n",
    "                                              'nbart_blue','nbart_swir_1','nbart_swir_2'],\n",
    "        mask_contiguity='nbart_contiguity', min_gooddata=0.90,\n",
    "        **query_3)\n",
    "    \n",
    "        #set variable for path to save files\n",
    "        savefilepath = '/g/data/zk34/ljg547/Outputs/'\n",
    "\n",
    "        # Set project naming convention. Start and end dates are reformated to remove '-'.\n",
    "        Proj = 'SSC_WD_'\n",
    "\n",
    "        ds_startDate = str(ds.isel(time=0).time.values)[0:10]\n",
    "        ds_startDate = str(ds_startDate[0:4] + f'{int(ds_startDate[6:7]):02d}' + \n",
    "                              f'{int(ds_startDate[9:10]):02d}')\n",
    "\n",
    "        ds_endDate = str(ds.isel(time=-1).time.values)[0:10]\n",
    "        ds_endDate = str(ds_endDate[0:4] + f'{int(ds_endDate[6:7]):02d}' + \n",
    "                              f'{int(ds_endDate[9:10]):02d}')\n",
    "        \n",
    "        # Calculate NDVI. NDVI is added to ds as a new band as shown in the below display\n",
    "        calculate_indices(ds,index = 'NDVI', collection = 'ga_ls_3',\n",
    "                normalise = True, deep_copy = False)\n",
    "        \n",
    "        # Create a new NDVI DataArray \n",
    "        ndvi = ds.NDVI\n",
    "        \n",
    "        # Calculate median NDVI through time for each pixel\n",
    "        median_ndvi = ndvi.median(dim='time')\n",
    "\n",
    "        # Calculate NDVI standard deviation through time for each pixel\n",
    "        std_ndvi = ndvi.std(dim='time')\n",
    "\n",
    "        # Group available NDVI time-steps into dry season months for later monthly averaging\n",
    "        ndvi_dryMonths = ndvi[ndvi['time.month'].isin(dry_months)]\n",
    "        \n",
    "        # Calculate median NDVI for each month in the dry period (through time and for each pixel)\n",
    "        median_ndvi_dry = ndvi_dryMonths.groupby('time.month').median(dim = 'time')\n",
    "\n",
    "        # Calculate median NDVI for the dry period based on the median monthly values for each month in the dry period\n",
    "        median_ndvi_dry = median_ndvi_dry.median(dim = 'month')\n",
    "        \n",
    "        # Calculate standard deviation in NDVI for each month in the dry period (through time and for each pixel)\n",
    "        std_ndvi_dry = ndvi_dryMonths.groupby('time.month').std(dim = 'time')\n",
    "\n",
    "        # Calculate mean standard deviation for the dry period based on the mean monthly values for each month in the dry period\n",
    "        std_ndvi_dry = std_ndvi_dry.mean(dim = 'month')\n",
    "        \n",
    "        # Calculate the difference in NDVI standard deviation for two specific months. 0 = January; 7 = July\n",
    "        std_ndvi_diff1 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 0)\n",
    "        std_ndvi_diff2 = ndvi.groupby('time.month').std(dim = 'time').isel(month = 7)\n",
    "        std_ndvi_diff = std_ndvi_diff1 - std_ndvi_diff2\n",
    "    \n",
    "        # Exporting median ndvi \n",
    "        # Convert from float16 to float32\n",
    "        arr = median_ndvi.astype(dtype='float32')\n",
    "\n",
    "        # Convert from DataArray to Dataset\n",
    "        arr = arr.to_dataset(name='median_NDVI')\n",
    "\n",
    "        # Assign CRS from original DataArray\n",
    "        arr.attrs = ds.attrs\n",
    "        \n",
    "        # Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "        fname = str(savefilepath + Proj + 'medianNDVI_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.tif')\n",
    "        \n",
    "        # Writing data to file\n",
    "        write_geotiff(dataset=arr, filename=fname)\n",
    "        \n",
    "        # Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "        f = open(savefilepath + Proj + 'medianNDVI_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.txt','w')  \n",
    "\n",
    "\n",
    "        f.write(\"Median NDVI for all months\" + \" from \" + ds_startDate + \n",
    "               \"-\" + ds_endDate + \".\" + \"\\n\" \n",
    "               \"Coordinates are longitude: \" +  str(round(coords_lon[i],2)) + ' to ' + \n",
    "               str(round(coords_lon[i+2],2)) + \"; latitude: \" + str(round(coords_lat[j],2)) + \".\" +\n",
    "               str(round(coords_lat[j+2],2)) + \"\\n\" + \"Data with >10% cloud was discarded.\" + \"\\n\" \n",
    "               \"This product was derived from NDVI_Export.ipynb\"\n",
    "            )\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "        # Exporting standard deviation of ndvi \n",
    "        # Convert from float16 to float32\n",
    "        arr = std_ndvi.astype(dtype='float32')\n",
    "\n",
    "        # Convert from DataArray to Dataset\n",
    "        arr = arr.to_dataset(name='std_NDVI')\n",
    "\n",
    "        # Assign CRS from original DataArray\n",
    "        arr.attrs = ds.attrs\n",
    "        \n",
    "        # Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "        fname = str(savefilepath + Proj + 'stdNDVI_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.tif')\n",
    "        \n",
    "        # Writing data to file\n",
    "        write_geotiff(dataset=arr, filename=fname)\n",
    "        \n",
    "        # Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "        f = open(savefilepath + Proj + 'stdNDVI_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.txt','w')  \n",
    "\n",
    "\n",
    "        f.write(\"Standard deviation of NDVI for all months\" + \" from \" + ds_startDate + \n",
    "               \"-\" + ds_endDate + \".\" + \"\\n\" + \"Coordinates are longitude: \" +  \n",
    "               str(round(coords_lon[i],2)) + ' to ' + str(round(coords_lon[i+2],2)) + \n",
    "               \"; latitude: \" + str(round(coords_lat[j],2)) + \".\" +\n",
    "               str(round(coords_lat[j+2],2)) + \"\\n\" \"This product was derived from NDVI_Export.ipynb\"\n",
    "            )\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "       \n",
    "        # Exporting median ndvi dry\n",
    "        # Convert from float16 to float32\n",
    "        arr = median_ndvi_dry.astype(dtype='float32')\n",
    "\n",
    "        # Convert from DataArray to Dataset\n",
    "        arr = arr.to_dataset(name='median_NDVI_dry')\n",
    "\n",
    "        # Assign CRS from original DataArray\n",
    "        arr.attrs = ds.attrs\n",
    "        \n",
    "        # Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "        fname = str(savefilepath + Proj + 'medianNDVI_DrySeason_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.tif')\n",
    "        \n",
    "        # Writing data to file\n",
    "        write_geotiff(dataset=arr, filename=fname)\n",
    "        \n",
    "        # Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "        f = open(savefilepath + Proj + 'medianNDVI_DrySeason_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.txt','w')  \n",
    "\n",
    "\n",
    "        f.write(\"Median NDVI of dry period (\" + str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "              \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "              \"NDVI_dry_median is the median value of NDVI over the dry months.\" + \"\\n\"\n",
    "              \"Coordinates are longitude: \" +  str(round(coords_lon[i],2)) + ' to ' + \n",
    "               str(round(coords_lon[i+1],2)) + \"; latitude: \" + str(round(coords_lat[j],2)) + \".\" +\n",
    "               str(round(coords_lat[j+1],2)) + \"\\n\" \"Data with >10% cloud was discarded.\" + \"\\n\" \n",
    "               \"\\n\" \"This product was derived from NDVI_Export.ipynb\"\n",
    "            )\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        \n",
    "        # Exporting standard deviation of ndvi for dry\n",
    "        # Convert from float16 to float32\n",
    "        arr = std_ndvi_dry.astype(dtype='float32')\n",
    "\n",
    "        # Convert from DataArray to Dataset\n",
    "        arr = arr.to_dataset(name='std_NDVI_dry')\n",
    "\n",
    "        # Assign CRS from original DataArray\n",
    "        arr.attrs = ds.attrs\n",
    "        \n",
    "        # Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "        fname = str(savefilepath + Proj + 'stdNDVI_DrySeason_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.tif')\n",
    "        \n",
    "        # Writing data to file\n",
    "        write_geotiff(dataset=arr, filename=fname)\n",
    "        \n",
    "        # Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "        f = open(savefilepath + Proj + 'stdNDVI_DrySeason' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.txt','w')  \n",
    "\n",
    "\n",
    "        f.write(\"Standard deviation of NDVI during the dry period (\" + \n",
    "              str(dry_months[0]+1) + \"-\" + str(dry_months[-1]+1) + \" month)\" +  \n",
    "              \" from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\" +\n",
    "              \"NDVI_dry_std is the mean standard deviation value of NDVI over the dry months.\" + \"\\n\"\n",
    "              \"Coordinates are longitude: \" +  str(round(coords_lon[i],2)) + ' to ' + \n",
    "               str(round(coords_lon[i+2],2)) + \"; latitude: \" + str(round(coords_lat[j],2)) + \".\" +\n",
    "               str(round(coords_lat[j+2],2)) + \"\\n\" \"Data with >10% cloud was discarded.\" + \"\\n\" \n",
    "              \"This product was derived from NDVI_Export.ipynb\"\n",
    "            )\n",
    "        \n",
    "        f.close()\n",
    "         \n",
    "        \n",
    "        # Exporting standard deviation difference for two specific months\n",
    "        # Convert from float16 to float32\n",
    "        arr = std_ndvi_diff.astype(dtype='float32')\n",
    "\n",
    "        # Convert from DataArray to Dataset\n",
    "        arr = arr.to_dataset(name='std_NDVI_diff')\n",
    "\n",
    "        # Assign CRS from original DataArray\n",
    "        arr.attrs = ds.attrs\n",
    "        \n",
    "        # Generating naming convention for dry season files based on Project area (Proj), specified dry season and time series start and end dates. \n",
    "        fname = str(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.tif')\n",
    "        \n",
    "        # Writing data to file\n",
    "        write_geotiff(dataset=arr, filename=fname)\n",
    "        \n",
    "        # Creating an associated metadata file. w - writes, r - reads, a- appends\n",
    "        f = open(savefilepath + Proj + 'stdNDVI_DiffJanAug_' +\n",
    "                      ds_startDate + '_' + ds_endDate + '_' + \n",
    "                      \"Lon\" + str(i) + \"Lat\" + str(j) + '.txt','w')  \n",
    "\n",
    "        f.write(\"Comparison between NDVI standard deviation during the wet season (January) and at the end of the dry season (August).\" + \n",
    "              \"\\n\" \"Time series includes imagery from \" + ds_startDate + \"-\" + ds_endDate + \".\" + \"\\n\"  \n",
    "              \"Where vegetation is accessing more reliable water sources (e.g. groundwater), residual standard deviation is low.\" + \"\\n\"\n",
    "              \"Coordinates are longitude: \" +  str(round(coords_lon[i],2)) + ' to ' + \n",
    "               str(round(coords_lon[i+2],2)) + \"; latitude: \" + str(round(coords_lat[j],2)) + \".\" +\n",
    "               str(round(coords_lat[j+2],2)) + \"\\n\" \"Data with >10% cloud was discarded.\" + \"\\n\" \n",
    "                \"This product was derived from NDVI_Export.ipynb\"\n",
    "            )\n",
    "        \n",
    "        f.close()    \n",
    "        \n",
    "        print(i,j)\n",
    "        \n",
    "    print(i)\n",
    "    \n",
    "print(\"--end--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please post a question on the [Open Data Cube Slack channel](http://slack.opendatacube.org/) or on the [GIS Stack Exchange](https://gis.stackexchange.com/questions/ask?tags=open-data-cube) using the `open-data-cube` tag (you can view previously asked questions [here](https://gis.stackexchange.com/questions/tagged/open-data-cube)).\n",
    "If you would like to report an issue with this notebook, you can file one on [Github](https://github.com/GeoscienceAustralia/dea-notebooks).\n",
    "\n",
    "**Last modified:** October 2019\n",
    "\n",
    "**Compatible datacube version:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7+142.g7f8581cf\n"
     ]
    }
   ],
   "source": [
    "print(datacube.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "Browse all available tags on the DEA User Guide's [Tags Index](https://docs.dea.ga.gov.au/genindex.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`NCI compatible`, :index:`sandbox compatible`, :index:`sentinel 2`, :index:`landsat 8`, :index:`dea_plotting`, :index:`rgb`, :index:`NDVI`, :index:`time series`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
