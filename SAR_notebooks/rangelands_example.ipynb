{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rangelands dynamics with Sentinel-1\n",
    "\n",
    "This notebook gives an example of using Sentinel-1 analysis ready data (ARD) radar backscatter (Gamma nought) and dual-polarimetric decomposition (Alpha, Entropy and Anisotropy) for exploring annual rangeland dynamics.\n",
    "\n",
    "Sentinel-1 (in this case it is the dual-pol Radar Vegetation Index (RVI) and Entropy bands) and Sentinel-2 NDVI are compared for multi-temporal trends - due to their relationships with vegetation biomass. A grasslands mask is then applied, before the monthly means are generated for the RVI and Entropy bands. These monthly means as well as the annual range in RVI and Entropy are output as GeoTIFF files.\n",
    "\n",
    "The following steps are used:\n",
    "1.  Load Sentinel-1 radar backscatter data through the datacube API\n",
    "2.  Apply speckle filtering\n",
    "3.  Calculate dual-pol Radar Vegetation Index (RVI)\n",
    "4.  Load Sentinel-1 dual-pol decomposition data\n",
    "5.  Load Sentinel-2 data and calculate NDVI\n",
    "6.  Interactively compare RVI, Entropy and NDVI time series\n",
    "7.  Show annual statistics\n",
    "8.  Apply grasslands mask to area\n",
    "9.  Generate monthly means for RVI and Entropy\n",
    "10.  Save results to GeoTIFF files\n",
    "\n",
    "This notebook was written using the Virtual Desktop Infrastructure (VDI) on the National Computational Infrastructure (NCI), allowing access to the current Sentinel-1 and Digitial Earth Australia (DEA) ARD datasets. For this notebook to work you must first load the relevant modules:\n",
    "-  module use /g/data/v10/public/modules/modulefiles\n",
    "-  module load dea\n",
    "\n",
    "The Sentinel-1 data are indexed as product 'S1_gamma0_scene_v2' for radar backscatter and 's1_haalpha_scene' for dual-pol decomposition.\n",
    "\n",
    "# 1. Load Sentinel-1 radar backscatter data through the datacube API\n",
    "\n",
    "Area of interest is in the Fitzroy Catchment of Western Australia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "import datacube\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(config='radar.conf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently need to use 2017 for a complete year of data\n",
    "\n",
    "Year = '2017'\n",
    "\n",
    "query = {\n",
    "# Fitzroy floodplain\n",
    "'y': (-18.6, -18.4),\n",
    "'x': (125.0, 125.2),\n",
    "    \n",
    "'time': (Year + '-01-01', Year + '-12-31'),\n",
    "'crs': 'EPSG:4326',\n",
    "'output_crs': 'EPSG: 3577',\n",
    "'resolution': (25, -25)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=dc.load(product='s1_gamma0_scene_v2', group_by='solar_day', **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove scenes that are mostly nulls\n",
    "# Uses code from https://github.com/fangfy/dea-projects/blob/master/water_interoperability/sentinel1_load_and_classify_nci.ipynb\n",
    "\n",
    "total_px=len(bs.x)*len(bs.y)\n",
    "valid=bs.where(bs.vv!=0).where(bs.vh!=0).count(dim=('x','y'))\n",
    "good=(valid.vh/total_px)>0.5\n",
    "bs_good = bs.sel(time=good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apply speckle filtering\n",
    "Uses code from https://github.com/fangfy/dea-projects/blob/master/water_interoperability/sentinel1_load_and_classify_nci.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://stackoverflow.com/questions/39785970/speckle-lee-filter-in-python\n",
    "\n",
    "from scipy.ndimage.filters import uniform_filter\n",
    "from scipy.ndimage.measurements import variance\n",
    "\n",
    "def lee_filter(da, size):\n",
    "    img = da.values\n",
    "    img_mean = uniform_filter(img, (size, size))\n",
    "    img_sqr_mean = uniform_filter(img**2, (size, size))\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply speckle filter\n",
    "\n",
    "# replace nan with 0\n",
    "bs_good_zerofilled = bs_good.where(~bs_good.isnull(), 0)\n",
    "\n",
    "smoothed_vv=bs_good_zerofilled.vv.groupby('time').apply(lee_filter, size=7)\n",
    "smoothed_vh=bs_good_zerofilled.vh.groupby('time').apply(lee_filter, size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smoothed dataset with Nans and assign attributes\n",
    "\n",
    "# save the nodata mask\n",
    "nodata_mask = bs_good.isnull().to_array().any(axis=0)\n",
    "\n",
    "smoothed=smoothed_vv.to_dataset(name='vv')\n",
    "smoothed['vh']=smoothed_vh\n",
    "smoothed=smoothed.where(~nodata_mask)\n",
    "\n",
    "# Remove unused data\n",
    "bs_attrs = bs.attrs\n",
    "smoothed = smoothed.assign_attrs(bs_attrs)\n",
    "\n",
    "del bs, bs_good\n",
    "smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View smoothed VV backscatter data\n",
    "pylab.rcParams['font.size']=6\n",
    "\n",
    "ntimes=len(smoothed.time.values)\n",
    "smoothed.vv.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=0, vmax=0.2, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate dual-pol Radar Vegetation Index (RVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed['RVI'] = 4*smoothed.vh/(smoothed.vv + smoothed.vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View RVI data\n",
    "\n",
    "ntimes=len(smoothed.time.values)\n",
    "smoothed.RVI.isel(time=slice(0,ntimes,2)).plot(col='time',col_wrap=4, vmin=0.2, vmax=1.5, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Sentinel-1 dual-pol decomposition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp=dc.load(product='s1_haalpha_scene', group_by='solar_day', **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any '0' values\n",
    "dp = dp.where(dp.entropy!=0)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View entropy data  \n",
    "ntimes=len(dp.time.values)\n",
    "dp.entropy.isel(time=slice(0,ntimes,4)).plot(col='time',col_wrap=4, vmin=0, vmax=1.0, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load Sentinel-2 data and calculate NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_S2 = datacube.Datacube(app='dc-S2-extract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read S2 green, nir and cloud mask bands for S2a and S2b\n",
    "bands_of_interest = ['nbar_green', 'nbar_nir_2', 'fmask']\n",
    "sensors = ['s2a','s2b']\n",
    "\n",
    "s2 = {}\n",
    "for sensor in sensors:\n",
    "    s2[sensor] = dc_S2.load(product = sensor+'_ard_granule', group_by='solar_day', measurements = bands_of_interest, **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the S2a and S2b dates\n",
    "import xarray as xr\n",
    "s2_clean = xr.concat(s2.values(), dim='time')\n",
    "\n",
    "# Remove clouds and nulls from S2 data\n",
    "nbar_green = s2_clean.nbar_green\n",
    "s2_good=nbar_green.to_dataset(name='green')\n",
    "s2_good['nir']=s2_clean.nbar_nir_2\n",
    "s2_good['fmask']=s2_clean.fmask\n",
    "\n",
    "s2_good_temp = s2_good.where(s2_good.fmask<2) # to remove cloud/shadow/nulls based on fmask\n",
    "s2_good_clean = s2_good_temp.where(s2_good_temp.fmask!=0)\n",
    "\n",
    "s2_good_clean = s2_good_clean.assign_attrs(bs_attrs)\n",
    "\n",
    "del s2, s2_good\n",
    "s2_good_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NDVI band\n",
    "s2_good_clean['ndvi']=(s2_good_clean.nir.astype(float) - s2_good_clean.green.astype(float))/(s2_good_clean.nir.astype(float) + s2_good_clean.green.astype(float))\n",
    "\n",
    "# Remove dates containing mostly nulls\n",
    "total_px=len(s2_good_clean.ndvi.x)*len(s2_good_clean.ndvi.y)\n",
    "valid=s2_good_clean.where(~s2_good_clean.ndvi.isnull()).count(dim=('x','y'))\n",
    "s2_mostly_good=(valid.ndvi/total_px)>0.5\n",
    "s2_good_clean = s2_good_clean.sel(time=s2_mostly_good)\n",
    "\n",
    "# Remove any remaining erroneous values (where NDVI <-1.0 and NDVI > 1.0)\n",
    "s2_good_clean = s2_good_clean.where(s2_good_clean.ndvi > -1.0).where(s2_good_clean.ndvi < 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View S2 NDVI time series\n",
    "ntimes=len(s2_good_clean.time.values)\n",
    "s2_good_clean.isel(time=slice(0,ntimes,3)).ndvi.plot(col='time', col_wrap=4, vmin=0.0, vmax=0.6, figsize=(10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Interactively compare RVI, Entropy and NDVI time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onclick(event):\n",
    "    global pixelx, pixely\n",
    "    x, y = int(event.xdata), int(event.ydata)\n",
    "    image_coords = smoothed.affine * (x, y)\n",
    "    pixelx = x \n",
    "    pixely = y \n",
    "    w.value = 'pixelx : {}, pixely : {}'.format(pixelx, pixely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "print('\\033[1m' + 'Click on the pixel to view its time series below' + '\\033[0m')\n",
    "\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "plt.imshow(np.flip(smoothed.vv.mean(dim='time')), interpolation = 'nearest', clim=(0,0.1), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()]) \n",
    "\n",
    "w = widgets.HTML(\"Click on the pixel to view its time series below\")\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot RVI, Entropy and NDVI through time based on the x,y point selected\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "# expand selected x,y pixel to make a square area of interest\n",
    "xp, yp=slice(pixelx+100,pixelx-100), slice(pixely-100,pixely+100)\n",
    "\n",
    "(smoothed.sel(x=xp,y=yp).RVI.groupby('time').mean()).plot(color='r', figsize=(6,3))\n",
    "(dp.sel(x=xp,y=yp).entropy.groupby('time').mean()).plot(color='b', figsize=(6,3))\n",
    "(s2_good_clean.sel(x=xp,y=yp).ndvi.groupby('time').mean()).plot(color='g', figsize=(6,3))\n",
    "'for point :', xp, yp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Show Annual Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate annual range (max minus min) for radar backscatter RVI, dul-pol decomposition Entropy, and NDVI\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "smoothed['RVI_range'] = smoothed.RVI.max(dim='time') - smoothed.RVI.min(dim='time')\n",
    "dp['entropy_range'] = dp.entropy.max(dim='time') - dp.entropy.min(dim='time')\n",
    "s2_good_clean['ndvi_range'] = s2_good_clean.ndvi.max(dim='time') - s2_good_clean.ndvi.min(dim='time')\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(10,8))\n",
    "smoothed.RVI_range.plot(vmin=0.2,vmax=1.2, ax = ax[0,0])\n",
    "dp.entropy_range.plot(vmin=0.1,vmax=0.6, ax = ax[0,1])\n",
    "s2_good_clean.ndvi_range.plot(vmin=0.0,vmax=0.4, ax = ax[1,0])\n",
    "s2_good_clean.ndvi.mean(dim='time').plot(vmin=0.1,vmax=0.6, ax = ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View same area/year of a true colour Landsat image (annual median value)\n",
    "LS = dc_S2.load(product = 'ls8_nbart_geomedian_annual', **query)\n",
    "LS[['red','green','blue']].isel(time=0).to_array().plot.imshow(robust=True, figsize=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show RVI, Entropy and NDVI mean, min, max and stand deviation for current year\n",
    "\n",
    "RVI_mean = np.flip(smoothed.RVI.mean(dim='time'))\n",
    "RVI_min = np.flip(smoothed.RVI.min(dim='time'))\n",
    "RVI_max = np.flip(smoothed.RVI.max(dim='time'))\n",
    "RVI_std = np.flip(smoothed.RVI.std(dim='time'))\n",
    "\n",
    "entropy_mean = np.flip(dp.entropy.mean(dim='time'))\n",
    "entropy_min = np.flip(dp.entropy.min(dim='time'))\n",
    "entropy_max = np.flip(dp.entropy.max(dim='time'))\n",
    "entropy_std = np.flip(dp.entropy.std(dim='time'))\n",
    "\n",
    "NDVI_mean = np.flip(s2_good_clean.ndvi.mean(dim='time'))\n",
    "NDVI_min = np.flip(s2_good_clean.ndvi.min(dim='time'))\n",
    "NDVI_max = np.flip(s2_good_clean.ndvi.max(dim='time'))\n",
    "NDVI_std = np.flip(s2_good_clean.ndvi.std(dim='time'))\n",
    "\n",
    "fix, ax = plt.subplots(3,4, figsize=(8,7))\n",
    "ax[0,0].imshow(RVI_mean, clim=(0,1.0))\n",
    "ax[0,0].set_title('Mean')\n",
    "ax[0,0].set_ylabel('RVI')\n",
    "ax[0,0].set_xticklabels([])\n",
    "ax[0,0].set_yticklabels([])\n",
    "ax[0,1].imshow(RVI_min, clim=(0,1.0))\n",
    "ax[0,1].set_title('Min')\n",
    "ax[0,1].set_xticklabels([])\n",
    "ax[0,1].set_yticklabels([])\n",
    "ax[0,2].imshow(RVI_max, clim=(0,2.0))\n",
    "ax[0,2].set_title('Max')\n",
    "ax[0,2].set_xticklabels([])\n",
    "ax[0,2].set_yticklabels([])\n",
    "ax[0,3].imshow(RVI_std, clim=(0,0.5))\n",
    "ax[0,3].set_title('StDev')\n",
    "ax[0,3].set_xticklabels([])\n",
    "ax[0,3].set_yticklabels([])\n",
    "ax[1,0].imshow(entropy_mean, clim=(0.2,0.8))\n",
    "ax[1,0].set_ylabel('entropy')\n",
    "ax[1,0].set_xticklabels([])\n",
    "ax[1,0].set_yticklabels([])\n",
    "ax[1,1].imshow(entropy_min, clim=(0,0.8))\n",
    "ax[1,1].set_xticklabels([])\n",
    "ax[1,1].set_yticklabels([])\n",
    "ax[1,2].imshow(entropy_max, clim=(0.4,1.0))\n",
    "ax[1,2].set_xticklabels([])\n",
    "ax[1,2].set_yticklabels([])\n",
    "ax[1,3].imshow(entropy_std, clim=(0.05,0.2))\n",
    "ax[1,3].set_xticklabels([])\n",
    "ax[1,3].set_yticklabels([])\n",
    "ax[2,0].imshow(NDVI_mean, clim=(0.1,0.5))\n",
    "ax[2,0].set_ylabel('NDVI')\n",
    "ax[2,0].set_xticklabels([])\n",
    "ax[2,0].set_yticklabels([])\n",
    "ax[2,1].imshow(NDVI_min, clim=(0,0.5))\n",
    "ax[2,1].set_xticklabels([])\n",
    "ax[2,1].set_yticklabels([])\n",
    "ax[2,2].imshow(NDVI_max, clim=(0,1.0))\n",
    "ax[2,2].set_xticklabels([])\n",
    "ax[2,2].set_yticklabels([])\n",
    "ax[2,3].imshow(NDVI_std, clim=(0,0.2))\n",
    "ax[2,3].set_xticklabels([])\n",
    "ax[2,3].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Apply grasslands mask to area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ogr, gdal, osr\n",
    "\n",
    "# Define pixel size (keep same as original) and NoData value of new raster\n",
    "xres = 25 \n",
    "yres = 25 \n",
    "NoData_value = 0\n",
    "\n",
    "# Filename of grassland mask shape file for current area \n",
    "vector_fn = 'fromglc10v01_-20_124_GrasslandMask_FitzSub_West.shp'\n",
    "\n",
    "# set the geotransform properties\n",
    "xcoords = smoothed.isel(time=1).vv.indexes['x'] \n",
    "ycoords = smoothed.isel(time=1).vv.indexes['y'] \n",
    "MaxValX = xcoords.shape\n",
    "MaxValY = ycoords.shape\n",
    "geotransform = (xcoords[MaxValX[0]-1]-(xres*0.5), xres, 0, ycoords[MaxValY[0]-1]+(yres*0.5), 0, -yres)\n",
    "\n",
    "# Open the data source and read in the extent\n",
    "source_ds = ogr.Open(vector_fn)\n",
    "source_layer = source_ds.GetLayer()\n",
    "source_srs = source_layer.GetSpatialRef()\n",
    "vx_min, vx_max, vy_min, vy_max = source_layer.GetExtent()\n",
    "\n",
    "# Create the destination extent\n",
    "arr_aoi = smoothed.vv.isel(time =1)\n",
    "arr_aoi.shape\n",
    "yt,xt = arr_aoi.shape # to be the same size as SAR images\n",
    "\n",
    "# set up mask image including projection\n",
    "target_ds = gdal.GetDriverByName('MEM').Create('', xt, yt, gdal.GDT_Byte)\n",
    "target_ds.SetGeoTransform(geotransform) # this is the same as the NDVI diff LL geoTIFF\n",
    "latlon = osr.SpatialReference() # establish encoding\n",
    "latlon.ImportFromEPSG(3577) # to projection (4326 or 3577)\n",
    "target_ds.SetProjection(latlon.ExportToWkt()) # export coords to file\n",
    "band = target_ds.GetRasterBand(1)\n",
    "band.SetNoDataValue(NoData_value)\n",
    "\n",
    "# rasterise\n",
    "gdal.RasterizeLayer(target_ds, [1], source_layer, burn_values=[1])\n",
    "\n",
    "# Read as mask array and view\n",
    "Mask_array = band.ReadAsArray()\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.imshow(Mask_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grassland mask to the RVI, Entropy\n",
    "\n",
    "smoothed_grassland = smoothed.where(np.flip(Mask_array==1))\n",
    "dp_grassland = dp.where(np.flip(Mask_array==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the RVI, Entropy annual range\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "ax[0].imshow(np.flip(smoothed_grassland.RVI_range), clim=(0,1.2), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[1].imshow(np.flip(dp_grassland.entropy_range), clim=(0.2,0.6), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "\n",
    "ax[0].set_title('RVI_range')\n",
    "ax[1].set_title('Entropy_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Generate monthly means for RVI and Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate RVI monthly mean for current year\n",
    "\n",
    "import pandas as pd\n",
    "time = pd.to_datetime([Year+'-01-01',Year+'-02-01',Year+'-03-01',Year+'-04-01',Year+'-05-01',Year+'-06-01',\n",
    "                             Year+'-07-01',Year+'-08-01',Year+'-09-01',Year+'-10-01',Year+'-11-01',Year+'-12-01'])\n",
    "\n",
    "monthly_mean_RVI=xr.Dataset({'Jan':(smoothed_grassland.RVI.sel(time=slice(Year+'-01-01', Year+'-01-31')).mean(dim='time')), 'time': time})\n",
    "monthly_mean_RVI['Feb']=smoothed_grassland.RVI.sel(time=slice(Year+'-02-01', Year+'-02-28')).mean(dim='time')\n",
    "monthly_mean_RVI['Mar']=smoothed_grassland.RVI.sel(time=slice(Year+'-03-01', Year+'-03-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Apr']=smoothed_grassland.RVI.sel(time=slice(Year+'-04-01', Year+'-04-30')).mean(dim='time')\n",
    "monthly_mean_RVI['May']=smoothed_grassland.RVI.sel(time=slice(Year+'-05-01', Year+'-05-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Jun']=smoothed_grassland.RVI.sel(time=slice(Year+'-06-01', Year+'-06-30')).mean(dim='time')\n",
    "monthly_mean_RVI['Jul']=smoothed_grassland.RVI.sel(time=slice(Year+'-07-01', Year+'-07-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Aug']=smoothed_grassland.RVI.sel(time=slice(Year+'-08-01', Year+'-08-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Sep']=smoothed_grassland.RVI.sel(time=slice(Year+'-09-01', Year+'-09-30')).mean(dim='time')\n",
    "monthly_mean_RVI['Oct']=smoothed_grassland.RVI.sel(time=slice(Year+'-10-01', Year+'-10-31')).mean(dim='time')\n",
    "monthly_mean_RVI['Nov']=smoothed_grassland.RVI.sel(time=slice(Year+'-11-01', Year+'-11-30')).mean(dim='time')\n",
    "monthly_mean_RVI['Dec']=smoothed_grassland.RVI.sel(time=slice(Year+'-12-01', Year+'-12-31')).mean(dim='time')\n",
    "\n",
    "monthly_mean_RVI = monthly_mean_RVI.assign_attrs(bs_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the RVI monthly means\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(10,8))\n",
    "ax[0,0].imshow(np.flip(monthly_mean_RVI.Jan), clim=(0.0,1.0), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[0,1].imshow(np.flip(monthly_mean_RVI.Apr), clim=(0.0,1.0), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[1,0].imshow(np.flip(monthly_mean_RVI.Jul), clim=(0.0,1.0), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[1,1].imshow(np.flip(monthly_mean_RVI.Oct), clim=(0.0,1.0), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "\n",
    "ax[0,0].set_title('January')\n",
    "ax[0,1].set_title('April')\n",
    "ax[1,0].set_title('July')\n",
    "ax[1,1].set_title('October')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate Entropy monthly mean for current year\n",
    "\n",
    "import pandas as pd\n",
    "time = pd.to_datetime([Year+'-01-01',Year+'-02-01',Year+'-03-01',Year+'-04-01',Year+'-05-01',Year+'-06-01',\n",
    "                             Year+'-07-01',Year+'-08-01',Year+'-09-01',Year+'-10-01',Year+'-11-01',Year+'-12-01'])\n",
    "\n",
    "monthly_mean_entropy=xr.Dataset({'Jan':(dp_grassland.entropy.sel(time=slice(Year+'-01-01', Year+'-01-31')).mean(dim='time')), 'time': time})\n",
    "monthly_mean_entropy['Feb']=dp_grassland.entropy.sel(time=slice(Year+'-02-01', Year+'-02-28')).mean(dim='time')\n",
    "monthly_mean_entropy['Mar']=dp_grassland.entropy.sel(time=slice(Year+'-03-01', Year+'-03-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Apr']=dp_grassland.entropy.sel(time=slice(Year+'-04-01', Year+'-04-30')).mean(dim='time')\n",
    "monthly_mean_entropy['May']=dp_grassland.entropy.sel(time=slice(Year+'-05-01', Year+'-05-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Jun']=dp_grassland.entropy.sel(time=slice(Year+'-06-01', Year+'-06-30')).mean(dim='time')\n",
    "monthly_mean_entropy['Jul']=dp_grassland.entropy.sel(time=slice(Year+'-07-01', Year+'-07-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Aug']=dp_grassland.entropy.sel(time=slice(Year+'-08-01', Year+'-08-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Sep']=dp_grassland.entropy.sel(time=slice(Year+'-09-01', Year+'-09-30')).mean(dim='time')\n",
    "monthly_mean_entropy['Oct']=dp_grassland.entropy.sel(time=slice(Year+'-10-01', Year+'-10-31')).mean(dim='time')\n",
    "monthly_mean_entropy['Nov']=dp_grassland.entropy.sel(time=slice(Year+'-11-01', Year+'-11-30')).mean(dim='time')\n",
    "monthly_mean_entropy['Dec']=dp_grassland.entropy.sel(time=slice(Year+'-12-01', Year+'-12-31')).mean(dim='time')\n",
    "\n",
    "monthly_mean_entropy = monthly_mean_entropy.assign_attrs(bs_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some of the Entropy monthly means\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(10,8))\n",
    "ax[0,0].imshow(np.flip(monthly_mean_entropy.Jan), clim=(0.0,0.9), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[0,1].imshow(np.flip(monthly_mean_entropy.Apr), clim=(0.0,0.9), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[1,0].imshow(np.flip(monthly_mean_entropy.Jul), clim=(0.0,0.9), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "ax[1,1].imshow(np.flip(monthly_mean_entropy.Oct), clim=(0.0,0.9), extent=[smoothed.coords['x'].min(), smoothed.coords['x'].max(), smoothed.coords['y'].min(), smoothed.coords['y'].max()])\n",
    "\n",
    "ax[0,0].set_title('January')\n",
    "ax[0,1].set_title('April')\n",
    "ax[1,0].set_title('July')\n",
    "ax[1,1].set_title('October')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save results to GeoTIFFs\n",
    "RVI and Entropy monthly means and annual range are saved to 'out_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write output images (in Albers) to GeoTIFF\n",
    "\n",
    "import ogr, gdal, osr\n",
    "\n",
    "# define coordinates for Albers equal area (3577)\n",
    "xcoords = smoothed_grassland.isel(time=1).vv.indexes['x']\n",
    "ycoords = smoothed_grassland.isel(time=1).vv.indexes['y']\n",
    "yt,xt = smoothed_grassland.isel(time=1).vv.shape\n",
    "MaxValX = xcoords.shape\n",
    "MaxValY = ycoords.shape\n",
    "\n",
    "# set geotransform and output projection\n",
    "xres = 25 \n",
    "yres = 25 \n",
    "geotransform = (xcoords[MaxValX[0]-1]-(xres*0.5), xres, 0, ycoords[MaxValY[0]-1]+(yres*0.5), 0, -yres) # offset by half the pixel size since it needs to be top-left pixel coord\n",
    "srs = osr.SpatialReference() \n",
    "srs.ImportFromEPSG(3577)\n",
    "\n",
    "# loop through monthly images\n",
    "months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "\n",
    "# Edit output filenames (filename_RVI and filename_entropy) and directory as required\n",
    "for month in months:\n",
    "    \n",
    "    filename_RVI = 'S1_RVI_Grasslands_3577_'+ month +'_'+ Year +'.tif' \n",
    "    S1_ds = gdal.GetDriverByName('GTiff').Create(filename_RVI, xt, yt, 1, gdal.GDT_Float32)\n",
    "    S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "    S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "    S1_ds.GetRasterBand(1).WriteArray(np.flip(monthly_mean_RVI[month]).data) # write band to raster\n",
    "    S1_ds.FlushCache()  # write to file\n",
    "    S1_ds = None # save and close\n",
    "    \n",
    "    filename_entropy = 'S1_Entropy_Grasslands_3577_'+ month + '_'+ Year + '.tif' \n",
    "    S1_ds = gdal.GetDriverByName('GTiff').Create(filename_entropy, xt, yt, 1, gdal.GDT_Float32)\n",
    "    S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "    S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "    S1_ds.GetRasterBand(1).WriteArray(np.flip(monthly_mean_entropy[month]).data) # write band to raster\n",
    "    S1_ds.FlushCache()  # write to file\n",
    "    S1_ds = None # save and close\n",
    "    \n",
    "# Output annual RVI and Entropy range images\n",
    "filename_RVI = 'S1_RVI_Grasslands_3577_Annual_Range_' + Year +'.tif' \n",
    "S1_ds = gdal.GetDriverByName('GTiff').Create(filename_RVI, xt, yt, 1, gdal.GDT_Float32)\n",
    "S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "S1_ds.GetRasterBand(1).WriteArray(np.flip(smoothed_grassland.RVI_range).data) # write band to raster\n",
    "S1_ds.FlushCache()  # write to file\n",
    "S1_ds = None # save and close\n",
    "\n",
    "# Output annual RVI and Entropy range images\n",
    "filename_entropy = 'S1_Entropy_Grasslands_3577_Annual_Range_' + Year + '.tif' \n",
    "S1_ds = gdal.GetDriverByName('GTiff').Create(filename_entropy, xt, yt, 1, gdal.GDT_Float32)\n",
    "S1_ds.SetGeoTransform(geotransform) # specify coordinates\n",
    "S1_ds.SetProjection(srs.ExportToWkt()) # export coords to file\n",
    "S1_ds.GetRasterBand(1).WriteArray(np.flip(dp_grassland.entropy_range).data) # write band to raster\n",
    "S1_ds.FlushCache()  # write to file\n",
    "S1_ds = None # save and close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
