{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClassificationTools.py\n",
    "\"\"\"\n",
    "This file contains a set of python functions used for classifying remote sensing data.\n",
    "Currently includes functions for random forest classification, but additional classifiers to be\n",
    "added in near future. Available functions:\n",
    "    randomforest_train\n",
    "    randomforest_classify\n",
    "    randomforest_eval\n",
    "Last modified: March 2018\n",
    "Author: Robbi Bishop-Taylor\n",
    "\"\"\"\n",
    "\n",
    "# Load modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from osgeo import gdal\n",
    "from os.path import splitext\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import DEA Notebooks scripts\n",
    "sys.path.append(os.path.abspath('/g/data/r78/rt1527/dea-notebooks/algorithms'))\n",
    "from ShapefileTools import layer_extent\n",
    "from ShapefileTools import rasterize_vector\n",
    "from DEADataHandling import array_to_geotiff\n",
    "\n",
    "\n",
    "def randomforest_train(train_shps, train_field, data_func, data_func_params={},\n",
    "                       classifier_params={}, train_reclass=None):\n",
    "    '''\n",
    "    Extracts training data from xarray dataset for multiple training shapefiles.\n",
    "    Loops through each each training shapefile, using shapefile extent for spatial\n",
    "    query. Outputs a trained classifier object and training label and data arrays.\n",
    "    :attr train_shps: list of training shapefile paths to import. Each file\n",
    "    should cover a small enough spatial area so as to not slow dc.load function\n",
    "    excessively (e.g. 100 x 100km max)\n",
    "    :attr train_field: shapefile field containing classification class\n",
    "    :attr data_func: function to import xarray data for each shapefile. Should return\n",
    "    an xarray dataset with 'crs' and 'affine' attributes\n",
    "    :attr data_func_params: optional dict of dc.load query inputs. Useful for defining\n",
    "    time query for temporal datasets (spatial queries are set automatically from shapefiles)\n",
    "    :attr classifier_params: optional dict of parameters for training random forest\n",
    "    :attr train_reclass: optional dict of from:to pairs to re-map shapefile field classes.\n",
    "    Useful for simplifying multiple classes into a simpler set of classes\n",
    "    :returns: trained classifier\n",
    "    :returns: array of training labels\n",
    "    :returns: array of training data\n",
    "    '''\n",
    "\n",
    "    # Output training label and pixel arrays\n",
    "    training_labels_list = list()\n",
    "    training_samples_list = list()\n",
    "\n",
    "    # For each shapefile, extract datacube data using extent of points\n",
    "    # and add resulting spectral data and labels to list of arrays\n",
    "    for train_shp in train_shps:\n",
    "\n",
    "        print(\"Importing training data from {}:\".format(train_shp))\n",
    "\n",
    "        try:\n",
    "\n",
    "            # Open vector of training points with gdal\n",
    "            data_source = gdal.OpenEx(train_shp, gdal.OF_VECTOR)\n",
    "            layer = data_source.GetLayer(0)\n",
    "\n",
    "            # Compute extents and generate spatial query\n",
    "            xmin, xmax, ymin, ymax = layer_extent(layer)\n",
    "            query_train = {'x': (xmin + 2000, xmax - 2000),\n",
    "                           'y': (ymin + 2000, ymax - 2000),\n",
    "                           'crs': 'EPSG:3577',\n",
    "                           **data_func_params}\n",
    "            print(query_train)\n",
    "\n",
    "            # Import data  as xarray and extract projection/transform data\n",
    "            training_xarray = data_func(query_train)\n",
    "            geo_transform_train = training_xarray.affine.to_gdal()\n",
    "            proj_train = training_xarray.crs.wkt\n",
    "\n",
    "            # Covert to array and rearrange dimension order\n",
    "            bands_array_train = training_xarray.to_array().values\n",
    "            bands_array_train = np.einsum('bxy->xyb', bands_array_train)\n",
    "            rows_train, cols_train, bands_n_train = bands_array_train.shape\n",
    "\n",
    "            # Import training data shapefiles and convert to matching raster pixels\n",
    "            training_pixels = rasterize_vector(layer, cols_train, rows_train,\n",
    "                                               geo_transform_train, proj_train,\n",
    "                                               field=train_field)\n",
    "\n",
    "            # Extract matching image sample data for each labelled pixel location\n",
    "            is_train = np.nonzero(training_pixels)\n",
    "            training_labels = training_pixels[is_train]\n",
    "            training_samples = bands_array_train[is_train]\n",
    "\n",
    "            # Remove nans from training samples\n",
    "            training_labels = training_labels[~np.isnan(training_samples).any(axis=1)]\n",
    "            training_samples = training_samples[~np.isnan(training_samples).any(axis=1)]\n",
    "\n",
    "            # Append outputs\n",
    "            training_labels_list.append(training_labels)\n",
    "            training_samples_list.append(training_samples)\n",
    "\n",
    "        except AttributeError:\n",
    "\n",
    "            print(\"  Skipping training data from {}; check file path\".format(train_shp))\n",
    "\n",
    "    # Combine polygon training data\n",
    "    training_labels = np.concatenate(training_labels_list, axis=0)\n",
    "    training_samples = np.concatenate(training_samples_list, axis=0)\n",
    "\n",
    "    # Optionally re-map classes prior to classification training\n",
    "    if train_reclass:\n",
    "        # For each class in training labels, re-map to new values using train_reclass\n",
    "        training_labels[:] = [train_reclass[label] for label in training_labels]\n",
    "\n",
    "    # Set up classifier and train on training sample data and labels\n",
    "    # Options for tuning: https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/\n",
    "    print(\"\\nTraining random forest classifier...\")\n",
    "    classifier = RandomForestClassifier(**classifier_params)\n",
    "    classifier.fit(training_samples, training_labels)\n",
    "    print(\"Model trained on {0} bands and \"\n",
    "          \"{1} training samples\".format(training_samples.shape[1],\n",
    "                                        str(len(training_samples))))\n",
    "\n",
    "    return classifier, training_labels, training_samples\n",
    "\n",
    "\n",
    "def randomforest_classify(classifier, analysis_data, classification_output, class_prob=False):\n",
    "    '''\n",
    "    Performs classification of xarray dataset using pre-trained random forest classifier,\n",
    "    and export classified output to a geotiff. Optionally, also export a predicted class\n",
    "    probability raster (i.e. indicating fraction of samples of the predicted class in a leaf)\n",
    "    :attr classifier: random forest classifier generated using randomforest_train\n",
    "    :attr analysis_data: xarray dataset with 'crs' and 'affine' attributes\n",
    "    and the same number of bands as data used to train classifier\n",
    "    :attr classification_output: file path to output geotiff classification\n",
    "    :attr class_prob: if True, compute predicted class probability and export to\n",
    "    geotiff suffixed with \"_prob.tif\"\n",
    "    :returns: classified array and (optional) classification probability array\n",
    "    '''\n",
    "\n",
    "    geo_transform = analysis_data.affine.to_gdal()\n",
    "    proj = analysis_data.crs.wkt\n",
    "\n",
    "    # Covert to array and rearrange dimension order\n",
    "    analysis_array = analysis_data.to_array().values\n",
    "    analysis_array = np.einsum('bxy->xyb', analysis_array)\n",
    "    rows, cols, bands_n = analysis_array.shape\n",
    "    print(\"Data to classify:\\n  Rows: {0}\\n  Columns: {1}\\n  Bands: {2}\".format(rows, cols, bands_n))\n",
    "\n",
    "    # Remove nodata and return flattened 'pixel x bands' array\n",
    "    input_nodata = np.isnan(analysis_array).any(axis=2)\n",
    "    flat_pixels = analysis_array[~input_nodata]\n",
    "\n",
    "    # Run classification\n",
    "    print(\"\\nClassification processing...\")\n",
    "    result = classifier.predict(flat_pixels)\n",
    "\n",
    "    # Restore 2D array by assigning flattened output to empty array\n",
    "    classification = np.zeros((rows, cols))\n",
    "    classification[~input_nodata] = result\n",
    "\n",
    "    # Nodata removed\n",
    "    print(\"  {} nodata cells removed\".format(str(np.sum(classification == 0))))\n",
    "\n",
    "    # Export to file\n",
    "    array_to_geotiff(classification_output,\n",
    "                     data=classification,\n",
    "                     geo_transform=geo_transform,\n",
    "                     projection=proj,\n",
    "                     nodata_val=0)\n",
    "    print(\"  Classification exported\")\n",
    "\n",
    "    # If requested, export classification probability:\n",
    "    if class_prob:\n",
    "\n",
    "        # Compute predicted class probability (fraction of samples of same class in a leaf)\n",
    "        # Use max to return only highest probability (the one that determined output class)\n",
    "        print(\"\\nClass probability processing...\")\n",
    "        result_prob = classifier.predict_proba((flat_pixels))\n",
    "        result_prob = np.max(result_prob, axis=1) * 100.0\n",
    "\n",
    "        # Restore 2D array by assigning flattened output to empty array\n",
    "        classification_prob = np.zeros((rows, cols))\n",
    "        classification_prob[~input_nodata] = result_prob\n",
    "\n",
    "        # Export to file\n",
    "        array_to_geotiff(splitext(classification_output)[0] + \"_prob.tif\",\n",
    "                         data=classification_prob,\n",
    "                         geo_transform=geo_transform,\n",
    "                         projection=proj,\n",
    "                         nodata_val=-999)\n",
    "        print(\"  Class probability exported\")\n",
    "\n",
    "        return classification, classification_prob\n",
    "\n",
    "    else:\n",
    "\n",
    "        return classification, None\n",
    "\n",
    "\n",
    "def randomforest_eval(training_labels, training_samples, classifier_scenario,\n",
    "                      output_path, max_estimators=100):\n",
    "    \"\"\"\n",
    "    Takes a set of training labels and training samples, and plots OOB error against\n",
    "    a range of classifier parameters to explore how parameters affect classification.\n",
    "    :attr training_labels: an (X, ) array of training labels\n",
    "    :attr training_samples: an (X, B) array of training sample data\n",
    "    :attr classifier_scenario: dict of classifier scenarios to plot\n",
    "    :attr output_path: output path for plot of OOB error by scenario\n",
    "    :attr max_estimators: max number of estimators to plot on x-axis (default = 100)\n",
    "    \"\"\"\n",
    "\n",
    "    # Map classifier name to list of n_estimators, error rate pairs.\n",
    "    error_rate = OrderedDict((label, []) for label, _ in classifier_scenario)\n",
    "\n",
    "    # Set min estimators to evaluate\n",
    "    min_estimators = 1\n",
    "\n",
    "    # For each classifier in pre-defined scenario\n",
    "    for label, clf in classifier_scenario:\n",
    "\n",
    "        for i in range(min_estimators, max_estimators + 1):\n",
    "            clf.set_params(n_estimators=i)\n",
    "            clf.fit(training_samples, training_labels)\n",
    "\n",
    "            # Record OOB error rate\n",
    "            oob_error = 1 - clf.oob_score_\n",
    "            error_rate[label].append((i, oob_error))\n",
    "\n",
    "    # Generate \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "    for label, clf_err in error_rate.items():\n",
    "        xs, ys = zip(*clf_err)\n",
    "        plt.plot(xs, ys, label=label)\n",
    "\n",
    "    # Plot and save output as figure\n",
    "    plt.xlim(min_estimators, max_estimators)\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(\"OOB error rate\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.yscale('log')\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
