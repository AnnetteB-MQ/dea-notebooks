{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Water Body Polygons\n",
    "\n",
    "**What does this notebook do?** \n",
    "This code follows the following workflow:\n",
    "* Generate a list of netCDF files within a specified folder location\n",
    "* Opens each netCDF file and:\n",
    "    * Keep only pixels observed at least x times\n",
    "    * Keep only pixels identified as wet at least x% of the time\n",
    "        * Here the code can take in two wetness thresholds, to produce two initial temporary polygon files. \n",
    "    * Convert the raster data into polygons\n",
    "* Append the polygon set to a temporary shapefile\n",
    "* Remove artificial polygon borders created at tile boundaries by merging polygons that intersect across Albers Tile boundaries\n",
    "* Filter the combined polygon dataset (note that this step happens after the merging of Albers tile boundary polygons to ensure that artifacts are not created by part of a polygon being filtered out, while the remainder of the polygon that sits on a separate tile is treated differently).\n",
    "    * Filter the polygons based on size\n",
    "    * Remove polygons that intersect with Australia's coastline\n",
    "    * Remove erroneous 'water' polygons within high-rise CBD areas\n",
    "    * Combine the two generated wetness thresholds (optional)\n",
    "    * Optional filtering for proximity to major rivers (as identified by the Geofabric dataset)\n",
    "* Save out the final polygon set to a shapefile\n",
    "\n",
    "**Required inputs:**\n",
    "* NetCDF files with WOfS outputs that will be used to define the persistent water body polygons\n",
    "    * Variable name: `TileFolder`\n",
    "    * This folder can be either a custom extraction of datacube-stats (as was done here), or you can choose to use the WOfS summary tiles for all of Australia (see [here for further information](#Tiles)).\n",
    "* A coastline polygon to filter out polygons generated from ocean pixels.\n",
    "    * Variable name: `LandSeaMaskFile`\n",
    "    * Here we have generated a hightide coastline using the [Intertidal Extents Model (ITEM) v2](http://pid.geoscience.gov.au/dataset/ga/113842) dataset. See [here for more details](#coastline)\n",
    "* Urban high rise polygon dataset\n",
    "    * Variable name: `UrbanMaskFile`\n",
    "    * WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This means that our algorithm is defining 'water bodies' around these misclassified shadows in capital cities. [See here](GenerateWaterBodyPolygons.ipynb#Urban) for a discussion of how an urban mask is produced. \n",
    "    \n",
    "**Optional inputs:**\n",
    "* River line dataset for filtering out polygons comprised of river segments.\n",
    "    * Variable name: `MajorRiversDataset`\n",
    "    * The option to filter out major rivers is provided, and so this dataset is optional if `FilterOutRivers = False`.\n",
    "    * Here we use the [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/), filtered to only keep features tagged as `major rivers`. \n",
    "    * There are some identified issues with this data layer that make the filtering using this data inconsistent (see [the discussion here](#rivers))\n",
    "    * We therefore turn this off during the production of the water bodies shapefile. \n",
    "\n",
    "**Running this workflow:** \n",
    "\n",
    "*NOTE: The Australia-wide analysis is too large to complete within a Jupyter Notebook. It was instead run on raijin. See this individual cell blocks below for more details*\n",
    "\n",
    "**Code history**\n",
    "This notebook is an updated version of `FindLotsOfDamsUsingWOFLsInaLoop.ipynb`. This copy was created to maintain the prototype workflow, while updating the operational workflow for water body polygon production.\n",
    "\n",
    "**Date:** August 2019\n",
    "\n",
    "**Author:** Claire Krause\n",
    "\n",
    "**Quality Check Performed, March 2019:** Imam Alam, Alex Vincent, Bex Dunn, Damien Ayers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Tags: :index:`fiona`, :index:`geopandas`, :index:`Geotiff`, :index:`masking`, :index:`rasterio`, :index:`shapefile`, :index:`WOfS`, :index:`WOFL`, :index:`write_geotiff`, :index:`shapely`, :index:`descartes`, :index:`water_classifier_and_wofs`, :index:`raster to polygons`, :index:`polygons`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab notebook\n",
    "\n",
    "import rasterio.features\n",
    "from shapely.geometry import Polygon, shape, mapping\n",
    "from shapely.ops import unary_union\n",
    "import geopandas as gp\n",
    "import fiona\n",
    "from fiona.crs import from_epsg\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "import math\n",
    "import geohash as gh\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the functions for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "def Generate_list_of_albers_tiles(TileFolder = 'TileFolder', CustomData = True):\n",
    "    '''\n",
    "    Generate a list of Albers tiles to loop through for the water body analysis. This \n",
    "    function assumes that the list of tiles will be generated from a custom \n",
    "    datacube-stats run, and the file names will have the format\n",
    "    \n",
    "    */wofs_summary_8_-37_{date}.nc\n",
    "    \n",
    "    The tile number is expected in the 2nd and 3rd last positions when the string has been\n",
    "    broken using `_`. If this is not the case, then this code will not work, and will throw an error. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    TileFolder : str\n",
    "        This is the path to the folder of netCDF files for analysis. If this is not provided, or an\n",
    "        incorrect path name is provided, the code will exit with an error.\n",
    "    CustomData : boolean\n",
    "        This is passed in from elsewhere in the notebook. If this is not entered, the default parameter is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    CustomRegionAlbersTiles: list\n",
    "        List of Albers tiles across the analysis region. \n",
    "        E.g. ['8_-32', '9_-32', '10_-32', '8_-33', '9_-33']\n",
    "    \n",
    "    '''  \n",
    "    if os.path.exists(TileFolder) == False:\n",
    "        print('** ERROR ** \\n'\n",
    "            'You need to specify a folder of files for running a custom region')\n",
    "        return\n",
    "    \n",
    "    # Grab a list of all of the netCDF files in the tile folder\n",
    "    TileFiles = glob.glob(f'{TileFolder}*.nc')\n",
    "    \n",
    "    CustomRegionAlbersTiles = set()\n",
    "    for filePath in TileFiles:\n",
    "        AlbersTiles = re.split('[_\\.]', filePath)\n",
    "        if CustomData:\n",
    "            # Test that the albers tile numbers are actually where we expect them to be in the file name\n",
    "            try: \n",
    "                int(AlbersTiles[-4])\n",
    "                int(AlbersTiles[-3])\n",
    "            except ValueError:\n",
    "                print('** ERROR ** \\n'\n",
    "                    'The netCDF files are expected to have the file format \"*/wofs_summary_8_-37_{date}.nc\",\\n'\n",
    "                    'with the Albers tile numbers in the 2nd and 3rd last positions when separated on `_`. \\n'\n",
    "                    'Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.')\n",
    "                return\n",
    "\n",
    "            # Now that we're happy that the file is reading the correct Albers tiles\n",
    "            ThisTile = f'{AlbersTiles[-3]}_{AlbersTiles[-2]}'\n",
    "        else:\n",
    "            # Test that the albers tile numbers are actually where we expect them to be in the file name\n",
    "            try: \n",
    "                int(AlbersTiles[-3])\n",
    "                int(AlbersTiles[-2])\n",
    "            except ValueError:\n",
    "                print('** ERROR ** \\n'\n",
    "                    'The netCDF files are expected to have the file format \"*/wofs_filtered_summary_8_-37.nc\",\\n'\n",
    "                    'with the Albers tile numbers in the 2nd and 3rd last positions when separated on `_` and `.`. \\n'\n",
    "                    'Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.')\n",
    "                return\n",
    "\n",
    "            # Now that we're happy that the file is reading the correct Albers tiles\n",
    "            ThisTile = f'{AlbersTiles[-3]}_{AlbersTiles[-2]}'\n",
    "        CustomRegionAlbersTiles.add(ThisTile)\n",
    "    CustomRegionAlbersTiles = list(CustomRegionAlbersTiles)\n",
    "    return CustomRegionAlbersTiles\n",
    "\n",
    "def Generate_list_of_tile_datasets(ListofAlbersTiles, Year, TileFolder = 'TileFolder', CustomData = True):\n",
    "    '''\n",
    "    Generate a list of Albers tiles datasets to loop through for the water body analysis. Here, the \n",
    "    ListofAlbersTiles is used to generate a list of NetCDF files where the Albers coordinates have \n",
    "    been substituted into the naming file format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    CustomRegionAlbersTiles: list\n",
    "        List of albers tiles to loop through\n",
    "        E.g. ['8_-32', '9_-32', '10_-32', '8_-33', '9_-33']\n",
    "    Year: int\n",
    "        Year for the analysis. This will correspond to the netCDF files for analysis.\n",
    "    TileFolder : str\n",
    "        This is the path to the folder of netCDF files for analysis. If this is not provided, or an\n",
    "        incorrect path name is provided, the code will exit with an error.\n",
    "    CustomData : boolean\n",
    "        This is passed from elsewhere in the notebook. If this parameter is not entered, the default value\n",
    "        is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Alltilespaths: list\n",
    "        List of file paths to files to be analysed.\n",
    "    \n",
    "    '''  \n",
    "    \n",
    "    if os.path.exists(TileFolder) == False:\n",
    "        print('** ERROR ** \\n'\n",
    "            'You need to specify a folder of files for running a custom region')\n",
    "        raise\n",
    "\n",
    "    Alltilespaths = []\n",
    "    \n",
    "    if CustomData:\n",
    "        for tile in ListofAlbersTiles:\n",
    "            Tiles = glob.glob(f'{TileFolder}*_{tile}_{Year}0101.nc')\n",
    "            Alltilespaths.append(Tiles[0]) # Assumes only one file will be returned\n",
    "    else:\n",
    "        for tile in ListofAlbersTiles:\n",
    "            # Use glob to check that the file actually exists in the format we expect\n",
    "            Tiles = glob.glob(f'{TileFolder}wofs_filtered_summary_{tile}.nc')\n",
    "            # Check that assumption by seeing if the returned list is empty\n",
    "            if not Tiles:\n",
    "                Tiles = glob.glob(f'{TileFolder}WOFS_3577_{tile}_summary.nc')\n",
    "            # Check that we actually have something now\n",
    "            if not Tiles:\n",
    "                print('** ERROR ** \\n'\n",
    "                    'An assumption in the file naming conventions has gone wrong somewhere.\\n'\n",
    "                    'We assume two file naming formats here: {TileFolder}wofs_filtered_summary_{tile}.nc, \\n'\n",
    "                    'and {TileFolder}WOFS_3577_{tile}_summary.nc. The files you have directed to don\\'t meet \\n'\n",
    "                    'either assumption. Please fix the file names, or alter the `Generate_list_of_albers_tiles` function.')\n",
    "            Alltilespaths.append(Tiles[0]) # Assumes only one file will be returned\n",
    " \n",
    "    return Alltilespaths\n",
    "\n",
    "def Filter_shapefile_by_intersection(gpdData, gpdFilter, filtertype = 'intersects', invertMask = True, \n",
    "                                     returnInverse = False):\n",
    "    '''\n",
    "    Filter out polygons that intersect with another polygon shapefile. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gpdData: geopandas dataframe\n",
    "        Polygon data that you wish to filter\n",
    "    gpdFilter: geopandas dataframe\n",
    "        Dataset you are using as a filter\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    filtertype: default = 'intersects'\n",
    "        Options = ['intersects', 'contains', 'within']\n",
    "    invertMask: boolean\n",
    "        Default = 'True'. This determines whether you want areas that DO ( = 'False') or DON'T ( = 'True')\n",
    "        intersect with the filter shapefile.\n",
    "    returnInnverse: boolean\n",
    "        Default = 'False'. If true, then return both parts of the intersection - those that intersect AND \n",
    "        those that don't as two dataframes.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gpdDataFiltered: geopandas dataframe\n",
    "        Filtered polygon set, with polygons that intersect with gpdFilter removed.\n",
    "    IntersectIndex: list of indices of gpdData that intersect with gpdFilter\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    if 'returnInverse = True'\n",
    "    gpdDataFiltered, gpdDataInverse: two geopandas dataframes\n",
    "        Filtered polygon set, with polygons that DON'T intersect with gpdFilter removed.\n",
    "    '''    \n",
    "    \n",
    "    # Check that the coordinate reference systems of both dataframes are the same\n",
    "    \n",
    "    #assert gpdData.crs == gpdFilter.crs, 'Make sure the the coordinate reference systems of the two provided dataframes are the same'\n",
    "    \n",
    "    Intersections = gp.sjoin(gpdFilter, gpdData, how=\"inner\", op=filtertype)\n",
    "    \n",
    "    # Find the index of all the polygons that intersect with the filter\n",
    "    IntersectIndex = sorted(set(Intersections['index_right']))\n",
    "    \n",
    "    # Grab only the polygons NOT in the IntersectIndex\n",
    "    # i.e. that don't intersect with a river\n",
    "    if invertMask:\n",
    "        gpdDataFiltered = gpdData.loc[~gpdData.index.isin(IntersectIndex)]\n",
    "    else:\n",
    "        gpdDataFiltered = gpdData.loc[gpdData.index.isin(IntersectIndex)]\n",
    "    \n",
    "    if returnInverse:\n",
    "        # We need to use the indices from IntersectIndex to find the inverse dataset, so we\n",
    "        # will just swap the '~'.\n",
    "        \n",
    "        if invertMask:\n",
    "            gpdDataInverse = gpdData.loc[gpdData.index.isin(IntersectIndex)]\n",
    "        else:\n",
    "            gpdDataInverse = gpdData.loc[~gpdData.index.isin(IntersectIndex)]\n",
    "            \n",
    "        return gpdDataFiltered, IntersectIndex, gpdDataInverse\n",
    "    else:    \n",
    "        \n",
    "        return gpdDataFiltered, IntersectIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up all the parameters for the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='valid'></a>\n",
    "### Filter results based on number of valid observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of valid WOfS observations for each pixel varies depending on the frequency of clouds and cloud shadow, and the proximity to high slope and terrain shadow, and the seasonal change in solar angle. \n",
    "\n",
    "The `count_clear` parameter provides a count of the number of valid observations each pixel recorded over the analysis period. We can use this parameter to mask out pixels that were infrequently observed. If this mask is not applied, pixels that were observed only once could be included, if that observation was wet (i.e. a single wet observation means the calculation of the frequency statistic would be (1 wet observation) / (1 total observation) = 100% frequency of wet observations).\n",
    "\n",
    "Here we set the minimum number of observations to be 4 per year. For our 32 year analysis, this means the minimum number of valid observations is 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinimumValidObs = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wetnessThreshold'></a>\n",
    "### How wet does a pixel need to be to be included?\n",
    "The value/s set here will be the minimum amount of time (as a decimal between 0 and 1) that you want water to be detected before it is included in the analysis. \n",
    "\n",
    "E.g. If this was set to 0.10, any pixels that are wet *at least* 10% of the time will be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "Following the exploration of an appropriate wetness threshold for our purposes ([see above](#wetness)), we choose to set two thresholds here. The code is set up to loop through both wetness thresholds, and to write out two temporary shapefiles. These two shapefiles with two separate thresholds are then used together to combine polygons from both thresholds later on in the workflow.\n",
    "\n",
    "Polygons identified by the secondary threshold that intersect with the polygons generated by the primary threshold will be extracted, and included in the final polygon dataset. This means that the **location** of polygons is set by the primary threshold, but the **shape** of these polygons is set by the secondary threshold.\n",
    "\n",
    "Threshold values need to be provided as a list of either one or two floating point numbers. If one number is provided, then this will be used to generate the initial polygon dataset. If two thresholds are entered, the **first number becomes the secondary threshold, and the second number becomes the primary threshold**. If more than two numbers are entered, the code will generate an error below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtLeastThisWet = [0.05, 0.10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='size'></a>\n",
    "### How big/small should the polygons be?\n",
    "This filtering step can remove very small and/or very large polygons. The size listed here is in m2. A single pixel in Landsat data is 25 m X 25 m = 625 m2. \n",
    "\n",
    "**MinSize**\n",
    "\n",
    "E.g. A minimum size of 6250 means that polygons need to be at least 10 pixels to be included. If you don't want to use this filter, set this value to 0.\n",
    "\n",
    "**MaxSize**\n",
    "\n",
    "E.g. A maximum size of 1 000 000 means that you only want to consider polygons less than 1 km2. If you don't want to use this filter, set this number to `math.inf`. *NOTE: if you are doing this analysis for all of Australia, very large polygons will be generated offshore, in the steps prior to filtering by the Australian coastline. For this reason, we have used a `MaxSize` = Area of Lake Frome * 2. This will remove the huge ocean polygons, but keep large inland waterbodies that we want to map.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinSize = 3120 # 5 pixels\n",
    "MaxSize = 5000000000 # approx area of Lake Eyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rivers'></a>\n",
    "### Do you want to filter out polygons that intersect with major rivers?\n",
    "\n",
    "**Note that for the Water Body Polygon dataset, we set this filter to False (`FilterOutRivers = False`). The code has been kept here in case it's of use for further filtering of the results later on.**\n",
    "\n",
    "We use the [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/) to filter out polygons that intersect with major rivers. This is done to remove river segments from the polygon dataset. We use the `SH_Network AHGFNetworkStream any` layer within the `SH_Network_GDB_V2_1_1.zip` geodatabase, and filter the dataset to only keep rivers tagged as `major`. It is this filtered dataset we use here.\n",
    "\n",
    "Note that we reproject this dataset to `epsg 3577`, Australian Albers coordinate reference system. If this is not correct for your analysis, you can change this in the cell below.\n",
    "\n",
    "If you don't want to filter out polygons that intersect with rivers, set this parameter to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note when using the Geofabric to filter out rivers\n",
    "\n",
    "The option to filter out rivers was switched off for the production of our water bodies dataset. During testing, the Geofabric dataset was shown to lead to inconsistencies in what was removed, and what remained within the dataset. \n",
    "\n",
    "* The Geofabric continues the streamline through on-river dams, which means these polygons are filtered out. This may not be the desired result. \n",
    "\n",
    "![Stream and Dam intersection](OnRiverDam.JPG \"An in-river dam that would be removed by the river filter, but may not be the desired result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterOutRivers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    "# Where is this file located?\n",
    "MajorRiversDataset = '/g/data/r78/cek156/ShapeFiles/Geofabric_v2_1_1/SH_Network_GDB_V2_1_1_Major_Filtered.shp'\n",
    "\n",
    "# Read in the major rivers dataset (if you are using it)\n",
    "if FilterOutRivers:\n",
    "    MajorRivers = gp.GeoDataFrame.from_file(MajorRiversDataset) \n",
    "    MajorRivers = MajorRivers.to_crs({'init':'epsg:3577'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Tiles'></a>\n",
    "### Set up the input datasets for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllOfAustraliaAllTime = True\n",
    "\n",
    "CustomData = False\n",
    "AutoGenerateTileList = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All of Australia analysis**\n",
    "\n",
    "If you would like to perform the analysis for all of Australia, using the published WOfS all time summaries, set `AllofAustraliaAllTime = True`. \n",
    "\n",
    "The WOfS all time summaries NetCDF files used are located in `/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/`. These files contain the following three variables: `count_wet`, `count_clear` and `frequency`.\n",
    "\n",
    "**Custom Data option**\n",
    "\n",
    "If `CustomData = True`, you will need to specify the location of the data you would like to use for this analysis, setting `TileFolder` below, under the `if CustomData` code section below.  \n",
    "\n",
    "If `CustomData = False`, the code will automatically look at the published WOfS all time summaries.\n",
    "\n",
    "**Autogeneration of tile list**\n",
    "\n",
    "`AutoGenerateTileList` will only be used if `AllOfAustraliaAllTime = False`. We only want to generate a list of tiles to iterate through if it will be a subset of a ll of the available data.\n",
    "\n",
    "If you would like to automatically generate a list of tiles using the outputs of an analysis (e.g. we have previously run a custom `datacube-stats` analysis using this region, and so we can generate a list of tiles that we know covers this area using the outputs of this analysis), set `AutoGenerateTileList = True` and update the location of the output file directory.\n",
    "\n",
    "If you would like to manually feed in a list of albers tiles, set `AutoGenerateTileList = False`, and feed in a list of tiles in the format:\n",
    "\n",
    "```\n",
    "ListofAlbersTiles = ['7_-34', '10_-40', '16_-34']\n",
    "```\n",
    "<br/>    \n",
    "**NOTE** For testing and debugging, set `CustomData = True` and `AutoGenerateTileList = False`, then enter a list of tiles to run using the `ListofAlbersTiles` described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CustomData:\n",
    "    TileFolder = '/g/data/r78/cek156/datacube_stats/WOFSDamsAllTimeNSWMDB/'\n",
    "else:\n",
    "    TileFolder = '/g/data/fk4/datacube/002/WOfS/WOfS_Stats_25_2_1/netcdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to generate the tile list if we are not doing all of Australia.\n",
    "if not AllOfAustraliaAllTime:\n",
    "    if AutoGenerateTileList:\n",
    "        ListofAlbersTiles = Generate_list_of_albers_tiles(TileFolder, CustomData)\n",
    "    else:\n",
    "        ListofAlbersTiles = ['8_-32','19_-40', '10_-30', '14_-31', '13_-31', '18_-38', '19_-33', '9_-31', '20_-34', \n",
    "                         '6_-39', '17_-31', '12_-40', '13_-39', '10_-38', '6_-41', '16_-32', '9_-40', '10_-31', \n",
    "                         '16_-37', '10_-39', '16_-35', '7_-38', '10_-37', '17_-39', '9_-34', '19_-35', '15_-33', \n",
    "                         '15_-30', '11_-32', '20_-39', '17_-30', '13_-40', '7_-41', '17_-29', '12_-33', '15_-28',\n",
    "                         '16_-40', '6_-35', '17_-40', '13_-38', '17_-42', '14_-39', '13_-29', '17_-37', '16_-38',\n",
    "                         '9_-32', '16_-34', '9_-41', '11_-31', '7_-36', '16_-39', '18_-30', '9_-37', '20_-37',\n",
    "                         '13_-32', '7_-40', '18_-41', '20_-32', '8_-41', '14_-28', '18_-39', '14_-43', '12_-39',\n",
    "                         '20_-36', '8_-34', '17_-41', '12_-41', '18_-31', '11_-38', '18_-34', '14_-35', '12_-42',\n",
    "                         '19_-39', '12_-34', '10_-42', '11_-35', '17_-35', '15_-41', '18_-33', '6_-37', '13_-41',\n",
    "                         '10_-40', '14_-33', '13_-37', '8_-36', '6_-36', '16_-43', '18_-36', '14_-40', '15_-43',\n",
    "                         '12_-30', '5_-39', '8_-39', '18_-35', '15_-39', '15_-29', '7_-34', '11_-34', '14_-41',\n",
    "                         '15_-42', '16_-29', '16_-28', '14_-37', '8_-33', '6_-38', '19_-38', '13_-33', '16_-36',\n",
    "                         '15_-37', '12_-38', '7_-35', '18_-40', '12_-31', '16_-41', '14_-38', '19_-37', '10_-34',\n",
    "                         '14_-32', '12_-32', '14_-42', '15_-35', '16_-31', '19_-36', '7_-37', '11_-41', '14_-36',\n",
    "                         '13_-35', '16_-42', '13_-36', '6_-40', '17_-36', '10_-41', '18_-37', '14_-29', '14_-30',\n",
    "                         '20_-38', '17_-38', '12_-36', '10_-35', '9_-42', '21_-33', '12_-37', '17_-32', '15_-31',\n",
    "                         '10_-36', '15_-36', '19_-34', '17_-34', '12_-35', '20_-40', '20_-33', '19_-31', '20_-35',\n",
    "                         '16_-30', '18_-32', '12_-29', '11_-30', '15_-32', '16_-33', '8_-37', '10_-33', '13_-34', \n",
    "                         '11_-33', '13_-30', '11_-36', '8_-40', '11_-40', '10_-32', '9_-38', '11_-42', '11_-39',\n",
    "                         '15_-34', '21_-34', '13_-42', '9_-35', '8_-42', '17_-33', '8_-38', '13_-28', '15_-38',\n",
    "                         '9_-36', '8_-35', '11_-37', '14_-34', '15_-40', '9_-33', '19_-32', '9_-39', '7_-39']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='coastline'></a>\n",
    "### Read in a land/sea mask\n",
    "\n",
    "A high tide coastline for Australia was used to mask out polygons that are continuously connected to the ocean, or an estuary. This coastline was generated using the [Intertidal Extents Model (ITEM) v2](http://pid.geoscience.gov.au/dataset/ga/113842) dataset.\n",
    "\n",
    "For a detailed description of how this coastline was created, see [this notebook](./CreateAustralianCoastlineUsingITEM.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LandSeaMaskFile = '/g/data/r78/cek156/ShapeFiles/ITEMv2Coastline/ITEM_Ocean_Polygon.shp'\n",
    "\n",
    "Coastline = gp.read_file(LandSeaMaskFile)\n",
    "Coastline = Coastline.to_crs({'init': 'epsg:3577'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Urban'></a>\n",
    "\n",
    "### Read in a mask for high-rise CBDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOfS has a known limitation, where deep shadows thrown by tall CBD buildings are misclassified as water. This means that our algorithm is defining 'water bodies' around these misclassified shadows in capital cities. \n",
    "\n",
    "To address this problem, we use the [Australian Bureau of Statistics Statistical Area 3 shapefile (2016)](http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/1270.0.55.001July%202016?OpenDocument#Data) to define a spatial footprint for Australia's CBD areas. \n",
    "\n",
    "We use the following polygons as our CBD filter:\n",
    "\n",
    "|SA3_CODE1|SA3_NAME16|\n",
    "|---------|----------|\n",
    "|11703    |Sydney Inner City|\n",
    "|20604    |Melbourne City|\n",
    "|30501    |Brisbane Inner|\n",
    "|30901    |Broadbeach - Burleigh|\n",
    "|30910    |Surfers Paradise|\n",
    "|40101    |Adelaide City|\n",
    "|50302    |Perth City|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UrbanMaskFile = '/g/data/r78/cek156/ShapeFiles/ABS_1270055001_sa3_2016_aust_shape/HighRiseCBD_ABS_sa3.shp'\n",
    "\n",
    "CBDs = gp.read_file(UrbanMaskFile)\n",
    "CBDs = CBDs.to_crs({'init': 'epsg:3577'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the first temporary polygon dataset\n",
    "\n",
    "This code section:\n",
    "\n",
    "1. Checks that the `AtLeastThisWet` threshold has been correctly entered above\n",
    "2. Sets up a `for` loop that allows the user to input multiple temporal datasets (see below)\n",
    "3. Generates a list of netCDF files to loop through\n",
    "4. Sets up a `for` loop for that list of files. Here we have separate data for each Landsat tile, so this loop loops through the list of tile files\n",
    "5. Opens the netCDF `frequency` data and removes the `time` dimension (which in this case is only of size 1)\n",
    "6. Opens the netCDF `count_clear` data and removes the `time` dimension (which in this case is only of size 1)\n",
    "7. Removes any pixels not observed at least [`MinimumValidObs` times](#valid)\n",
    "8. Sets up a `for` loop for the entered [`AtLeastThisWet` thresholds](#wetnessThreshold)\n",
    "9. Masks out any data that does not meet the wetness threshold\n",
    "10. Converts the data to a Boolean array, with included pixels == 1\n",
    "11. Converts the raster array to a polygon dataset\n",
    "12. Cleans up the polygon dataset\n",
    "13. Resets the `geometry` to a shapely geometry\n",
    "14. Merges any overlapping polygons\n",
    "15. Convert the output of the merging back into a geopandas dataframe\n",
    "16. Calculates the area of each polygon\n",
    "17. Saves the results to a shapefile\n",
    "\n",
    "Within this section you need to set up:\n",
    "- **WaterBodiesShp:** The name and filepath of the intermediate output polygon set\n",
    "- **WOFSshpMerged:** The filepath for the location of temp files during the code run\n",
    "- **WOFSshpFiltered:** The name and filepath of the outputs following the [filtering steps](#filtering)\n",
    "- **FinalName:** The name and file path of the final, completed waterbodies shapefile\n",
    "- **years to analyse:** `for year in range(x,y)` - note that the last year is NOT included in the analysis. This for loop is set up to allow you to loop through multiple datasets to create multiple polygon outputs. If you only have one input dataset, set this to `range(<year of the analysis>, <year of the analysis + 1>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up some file names for the inputs and outputs\n",
    "# The name and filepath of the intermediate output polygon set\n",
    "WaterBodiesShp = f'/g/data/r78/cek156/dea-notebooks/WaterbodyAreaMappingandMonitoring/AusAllTime01-005HybridWaterbodies/Temp'\n",
    "\n",
    "# The name and filepath of the temp, filtered output polygon set\n",
    "WOFSshpMerged = f'/g/data/r78/cek156/dea-notebooks/WaterbodyAreaMappingandMonitoring/AusAllTime01-005HybridWaterbodies/'\n",
    "WOFSshpFiltered = '/g/data/r78/cek156/dea-notebooks/WaterbodyAreaMappingandMonitoring/AusAllTime01-005HybridWaterbodies/AusWaterBodiesFiltered.shp'\n",
    "\n",
    "# Final shapefile name\n",
    "FinalName = '/g/data/r78/cek156/dea-notebooks/WaterbodyAreaMappingandMonitoring/AusAllTime01-005HybridWaterbodies/AusWaterBodies.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, test whether the wetness threshold has been correctly set\n",
    "if len(AtLeastThisWet) == 2:\n",
    "    print(f'We will be running a hybrid wetness threshold. Please ensure that the major threshold is \\n'\n",
    "          f'listed second, with the supplementary threshold entered first.' \n",
    "          f'**You have set {AtLeastThisWet[-1]} as the primary threshold,** \\n'\n",
    "          f'**with {AtLeastThisWet[0]} set as the supplementary threshold.**')\n",
    "elif len(AtLeastThisWet) == 1:\n",
    "    print(f'You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "         f'set this option by including two wetness thresholds in the `AtLeastThisWet` variable above. \\n'\n",
    "         f'The wetness threshold we will use is {AtLeastThisWet}.')\n",
    "else:\n",
    "    raise ValueError (f'There is something wrong with your entered wetness threshold. Please enter a list \\n'\n",
    "                      f'of either one or two numbers. You have entered {AtLeastThisWet}. \\n'\n",
    "                      f'See above for more information')\n",
    "\n",
    "# Now perform the analysis to generate the first iteration of polygons\n",
    "for year in range(1980,1981):\n",
    "    \n",
    "    ### Get the list of netcdf file names to loop through\n",
    "    if AllOfAustraliaAllTime:\n",
    "        # Grab everything from the published WOfS all time summaries\n",
    "        Alltiles = glob.glob(f'{TileFolder}*.nc')\n",
    "    else:\n",
    "        Alltiles = Generate_list_of_tile_datasets(ListofAlbersTiles, year, TileFolder, CustomData)            \n",
    "        \n",
    "    for WOFSfile in Alltiles: \n",
    "        try:\n",
    "            # Read in the data\n",
    "            # Note that the netCDF files we are using here contain a variable called 'frequency',\n",
    "            # which is what we are using to define our water polygons.\n",
    "            # If you use a different netCDF input source, you may need to change this variable name here\n",
    "            WOFSnetCDFData = xr.open_rasterio(f'NETCDF:{WOFSfile}:frequency')\n",
    "            # Remove the superfluous time dimension\n",
    "            WOFSnetCDFData = WOFSnetCDFData.squeeze()\n",
    "            \n",
    "            # Open the clear count variable to generate the minimum observation mask\n",
    "            WOFSvalidcount = xr.open_rasterio(f'NETCDF:{WOFSfile}:count_clear')\n",
    "            WOFSvalidcount = WOFSvalidcount.squeeze()\n",
    "\n",
    "            # Filter our WOfS classified data layer to remove noise\n",
    "            # Remove any pixels not abserved at least MinimumValidObs times\n",
    "            WOFSValidFiltered = WOFSvalidcount >= MinimumValidObs\n",
    "            \n",
    "            for Thresholds in AtLeastThisWet:\n",
    "                # Remove any pixels that are wet < AtLeastThisWet% of the time\n",
    "                WOFSfiltered = WOFSnetCDFData > Thresholds\n",
    "\n",
    "                # Now find pixels that meet both the MinimumValidObs and AtLeastThisWet criteria\n",
    "                # Change all zeros to NaN to create a nan/1 mask layer\n",
    "                # Pixels == 1 now represent our water bodies\n",
    "                WOFSfiltered = WOFSfiltered.where((WOFSfiltered !=0) & (WOFSValidFiltered !=0))\n",
    "\n",
    "                # Convert the raster to polygons\n",
    "                # We use a mask of '1' to only generate polygons around values of '1' (not NaNs)\n",
    "                WOFSpolygons = rasterio.features.shapes(WOFSfiltered.data.astype('float32'), \n",
    "                                                        mask = WOFSfiltered.data.astype('float32') == 1,\n",
    "                                                        transform = WOFSnetCDFData.transform)\n",
    "                # The rasterio.features.shapes returns a tuple. We only want to keep the geometry portion,\n",
    "                # not the value of each polygon (which here is just 1 for everything)\n",
    "                WOFSbreaktuple = (a for a, b in WOFSpolygons)\n",
    "\n",
    "                # Put our polygons into a geopandas geodataframe\n",
    "                PolygonGP = gp.GeoDataFrame(list(WOFSbreaktuple))\n",
    "\n",
    "                # Grab the geometries and convert into a shapely geometry\n",
    "                # so we can quickly calcuate the area of each polygon\n",
    "                PolygonGP['geometry'] = None\n",
    "                for ix, poly in PolygonGP.iterrows():\n",
    "                    poly['geometry'] = shape(poly)\n",
    "\n",
    "                # Set the geometry of the dataframe to be the shapely geometry we just created    \n",
    "                PolygonGP = PolygonGP.set_geometry('geometry')\n",
    "                # We need to add the crs back onto the dataframe\n",
    "                PolygonGP.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "                # Combine any overlapping polygons\n",
    "                MergedPolygonsGeoms = unary_union(PolygonGP['geometry'])\n",
    "\n",
    "                # Turn the combined multipolygon back into a geodataframe\n",
    "                MergedPolygonsGPD = gp.GeoDataFrame([poly for poly in MergedPolygonsGeoms])\n",
    "                # Rename the geometry column\n",
    "                MergedPolygonsGPD.columns = ['geometry']\n",
    "                # We need to add the crs back onto the dataframe\n",
    "                MergedPolygonsGPD.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "                # Calculate the area of each polygon again now that overlapping polygons\n",
    "                # have been merged\n",
    "                MergedPolygonsGPD['area'] = MergedPolygonsGPD['geometry'].area\n",
    "\n",
    "                # Save the polygons to a shapefile\n",
    "                schema = {'geometry': 'Polygon','properties': {'area': 'float'}}\n",
    "                \n",
    "                # Generate our dynamic filename\n",
    "                FileName = f'{WaterBodiesShp}_{Thresholds}.shp'\n",
    "                # Append the file name to the list so we can call it later on\n",
    "\n",
    "                if os.path.isfile(FileName):\n",
    "                    with fiona.open(FileName, \"a\", crs = from_epsg(3577), \n",
    "                                    driver = 'ESRI Shapefile', schema = schema) as output:\n",
    "                        for ix, poly in MergedPolygonsGPD.iterrows():\n",
    "                                        output.write(({'properties': {'area': poly['area']},\n",
    "                                                       'geometry': mapping(shape(poly['geometry']))})) \n",
    "                else:\n",
    "                    with fiona.open(FileName, \"w\", crs = from_epsg(3577), \n",
    "                                    driver = 'ESRI Shapefile', schema = schema) as output:\n",
    "                        for ix, poly in MergedPolygonsGPD.iterrows():\n",
    "                            output.write(({'properties': {'area': poly['area']},\n",
    "                                           'geometry': mapping(shape(poly['geometry']))}))\n",
    "\n",
    "        except:\n",
    "            print(f'{WOFSfile} did not run. \\n'\n",
    "            f'This is probably because there are no waterbodies present in this tile.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MergeTiles'></a>\n",
    "\n",
    "## Merge polygons that have an edge at a tile boundary\n",
    "\n",
    "Now that we have all of the polygons across our whole region of interest, we need to check for artifacts in the data caused by tile boundaries. \n",
    "\n",
    "We have created a shapefile that consists of the albers tile boundaries, plus a 1 pixel (25 m) buffer. This shapefile will help us to find any polygons that have a boundary at the edge of an albers tile. We can then find where polygons touch across this boundary, and join them up.\n",
    "\n",
    "Within this section you need to set up:\n",
    "- **AlbersBuffer:** The file location of a shapefile that is a 1 pixel buffer around the Albers tile boundaries\n",
    "\n",
    "*NOTE: for the Australia-wide analysis, the number and size of polygons means that this cell cannot be run in this notebook. Instead, we ran this cell on raijin*\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#PBS -P r78\n",
    "#PBS -q hugemem\n",
    "#PBS -l walltime=96:00:00\n",
    "#PBS -l mem=500GB\n",
    "#PBS -l jobfs=200GB\n",
    "#PBS -l ncpus=7\n",
    "#PBS -l wd\n",
    "#PBS -lother=gdata1a\n",
    " \n",
    "module use /g/data/v10/public/modules/modulefiles/\n",
    "module load dea\n",
    "\n",
    "PYTHONPATH=$PYTHONPATH:/g/data/r78/cek156/dea-notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "AlbersBuffer = gp.read_file(\n",
    "    '/g/data/r78/cek156/ShapeFiles/AlbersBuffer25m.shp')\n",
    "\n",
    "for Threshold in AtLeastThisWet:\n",
    "    print(f'Working on {Threshold} shapefile')\n",
    "    # We are using the more severe wetness threshold as the main polygon dataset.\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'AtLeastThisWet'\n",
    "    # variable, with the higher threshold listed second.\n",
    "    WaterPolygons = gp.read_file(f'{WaterBodiesShp}_{Threshold}.shp')\n",
    "\n",
    "    # Find where the albers polygon overlaps with our dam polygons\n",
    "    BoundaryMergedDams, IntersectIndexes, NotBoundaryDams= Filter_shapefile_by_intersection(WaterPolygons, AlbersBuffer, \n",
    "                                                                          invertMask=False, \n",
    "                                                                          returnInverse=True)\n",
    "\n",
    "    # Now combine overlapping polygons in `BoundaryDams`\n",
    "    UnionBoundaryDams = BoundaryMergedDams.unary_union\n",
    "\n",
    "    # `Explode` the multipolygon back out into individual polygons\n",
    "    UnionGDF = gp.GeoDataFrame(crs=WaterPolygons.crs, geometry=[UnionBoundaryDams])\n",
    "    MergedDams = UnionGDF.explode()\n",
    "\n",
    "    # Then combine our new merged polygons with the `NotBoundaryDams`\n",
    "    # Combine New merged polygons with the remaining polygons that are not near the tile boundary\n",
    "    AllTogether = gp.GeoDataFrame(pd.concat([NotBoundaryDams, MergedDams],\n",
    "                                            ignore_index=True, sort=True)).set_geometry('geometry')\n",
    "\n",
    "    # Calculate the area of each polygon\n",
    "    AllTogether['area'] = AllTogether.area\n",
    "    \n",
    "    schema = {'geometry': 'Polygon','properties': {'area': 'float'}}\n",
    "\n",
    "    print(f'Writing out {Threshold} shapefile')\n",
    "    \n",
    "    with fiona.open(f'{WOFSshpMerged}Union_{Threshold}.shp', \"w\", crs = from_epsg(3577), \n",
    "                    driver = 'ESRI Shapefile', schema = schema) as output:\n",
    "        for ix, poly in AllTogether.iterrows():\n",
    "            output.write(({'properties': {'area': poly['area']},\n",
    "                           'geometry': mapping(shape(poly['geometry']))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Filtering'></a>\n",
    "\n",
    "## Filter the merged polygons by:\n",
    "- **Area:**\n",
    "Based on the `MinSize` and `MaxSize` parameters set [here](#size).\n",
    "- **Coastline:**\n",
    "Using the `Coastline` dataset loaded [here](#coastline).\n",
    "- **CBD location:**\n",
    "Using the `CBDs` dataset loaded [here](#Urban).\n",
    "- **Wetness thresholds:**\n",
    "Here we apply the hybrid threshold described [here](#wetness)\n",
    "- **Intersection with rivers (optional):**\n",
    "Using the `MajorRivers` dataset loaded [here](#rivers)\n",
    "\n",
    "*NOTE: for the Australia-wide analysis, the number and size of polygons means that this cell cannot be run in this notebook. Instead, we ran this cell on raijin*\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "#PBS -P r78\n",
    "#PBS -q hugemem\n",
    "#PBS -l walltime=96:00:00\n",
    "#PBS -l mem=500GB\n",
    "#PBS -l jobfs=200GB\n",
    "#PBS -l ncpus=7\n",
    "#PBS -l wd\n",
    "#PBS -lother=gdata1a\n",
    " \n",
    "module use /g/data/v10/public/modules/modulefiles/\n",
    "module load dea\n",
    "\n",
    "PYTHONPATH=$PYTHONPATH:/g/data/r78/cek156/dea-notebooks\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllTogether = gp.read_file(f'{WOFSshpMerged}Temp_{AtLeastThisWet[1]}.shp')\n",
    "AllTogether['area'] = pd.to_numeric(AllTogether.area)\n",
    "\n",
    "# Filter out any polygons smaller than MinSize, and greater than MaxSize\n",
    "WaterBodiesBig = AllTogether.loc[((AllTogether['area'] > MinSize) & (AllTogether['area'] <= MaxSize))]\n",
    "\n",
    "# Filter out any ocean in the pixel\n",
    "WaterBodiesLand, IntersectIndexes = Filter_shapefile_by_intersection(WaterBodiesBig, Coastline, \n",
    "                                                   invertMask = True)\n",
    "\n",
    "# WOfS has a known bug where deep shadows from high-rise CBD buildings are misclassified\n",
    "# as water. We will use the ABS sa3 dataset to filter out Brisbane, Gold Coast, Sydney, \n",
    "# Melbourne, Adelaide and Perth CBDs. \n",
    "NotCities, IntersectIndexes = Filter_shapefile_by_intersection(WaterBodiesLand, CBDs)\n",
    "\n",
    "# Check for hybrid wetness thresholds\n",
    "if len(AtLeastThisWet) == 2:\n",
    "    # Note that this assumes that the thresholds have been correctly entered into the 'AtLeastThisWet'\n",
    "    # variable, with the supplementary threshold listed first.\n",
    "    LowerThreshold = gp.read_file(f'{WOFSshpMerged}Union_{AtLeastThisWet[0]}.shp')\n",
    "    LowerThreshold['area'] = pd.to_numeric(LowerThreshold.area)\n",
    "    # Filter out those pesky huge polygons\n",
    "    LowerThreshold = LowerThreshold.loc[(LowerThreshold['area'] <= MaxSize)]\n",
    "    # Find where the albers polygon overlaps with our dam polygons\n",
    "    BoundaryMergedDams, IntersectIndexes = Filter_shapefile_by_intersection(LowerThreshold,\n",
    "                                                                            NotCities)\n",
    "    # Pull out the polygons from the supplementary shapefile that intersect with the primary shapefile\n",
    "    LowerThresholdToUse = LowerThreshold.loc[LowerThreshold.index.isin(IntersectIndexes)]\n",
    "    # Concat the two polygon sets together\n",
    "    CombinedPolygons = gp.GeoDataFrame(pd.concat([LowerThresholdToUse, NotCities], ignore_index=True))\n",
    "    # Merge overlapping polygons\n",
    "    CombinedPolygonsUnion = CombinedPolygons.unary_union\n",
    "    # `Explode` the multipolygon back out into individual polygons\n",
    "    UnionGDF = gp.GeoDataFrame(crs=LowerThreshold.crs, geometry=[CombinedPolygonsUnion])\n",
    "    HybridDams = UnionGDF.explode()\n",
    "else:\n",
    "    print('You have not set up the hybrid threshold option. If you meant to use this option, please \\n'\n",
    "          'set this option by including two wetness thresholds in the `AtLeastThisWet` variable above')\n",
    "    HybridDams = NotCities\n",
    "\n",
    "# Here is where we do the river filtering (if FilterOutRivers == True)\n",
    "if FilterOutRivers:\n",
    "    WaterBodiesBigRiverFiltered, IntersectIndexes = Filter_shapefile_by_intersection(HybridDams, MajorRivers)\n",
    "else:\n",
    "    # If river filtering is turned off, then we just keep all the same polygons\n",
    "    WaterBodiesBigRiverFiltered = HybridDams\n",
    "    \n",
    "# We need to add the crs back onto the dataframe\n",
    "WaterBodiesBigRiverFiltered.crs = {'init': 'epsg:3577'}\n",
    "\n",
    "# Calculate the area and perimeter of each polygon again now that overlapping polygons\n",
    "# have been merged\n",
    "WaterBodiesBigRiverFiltered['area'] = WaterBodiesBigRiverFiltered['geometry'].area\n",
    "WaterBodiesBigRiverFiltered['perimeter'] = WaterBodiesBigRiverFiltered['geometry'].length\n",
    "\n",
    "# Calculate the Polsby-Popper value (see below), and write out too\n",
    "WaterBodiesBigRiverFiltered['PPtest'] = ((WaterBodiesBigRiverFiltered['area'] * 4 * math.pi) / \n",
    "                                         (WaterBodiesBigRiverFiltered['perimeter'] ** 2))\n",
    "\n",
    "# Save the polygons to a shapefile\n",
    "schema = {'geometry': 'Polygon','properties': {'area': 'float', 'perimeter':'float', 'PPtest':'float'}}\n",
    "\n",
    "with fiona.open(WOFSshpFiltered, \"w\", crs = from_epsg(3577), \n",
    "                driver = 'ESRI Shapefile', schema = schema) as output:\n",
    "    for ix, poly in WaterBodiesBigRiverFiltered.iterrows():\n",
    "        output.write(({'properties': {'area': poly['area'], \n",
    "                                      'perimeter': poly['perimeter'], \n",
    "                                      'PPtest': poly['PPtest']},\n",
    "                       'geometry': mapping(shape(poly['geometry']))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dividing up very large polygons\n",
    "\n",
    "The size of polygons is determined by the contiguity of waterbody pixels through the landscape. This can result in very large polygons, e.g. where rivers are wide and unobscured by trees, or where waterbodies are connected to rivers or neighbouring waterbodies. The image below shows this for the Menindee Lakes, NSW. The relatively flat terrain in this part of Australia means that the 0.05 wetness threshold results in the connection of a large stretch of river and the individual lakes into a single large polygon that spans 154 km. This polygon is too large to provide useful insights into the changing water surface area of the Menindee Lakes, and needs to be broken into smaller, more useful polygons.\n",
    "\n",
    "![Menindee Lakes original polygon](menindeeLakes.JPG)\n",
    "\n",
    "We do this by applying the [Polsby-Popper test (1991)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2936284). The Polsby-Popper test is an assessment of the 'compactness' of a polygon. This method was originally developed to test the shape of congressional and state legislative districts, to prevent gerrymandering. \n",
    "\n",
    "The Polsby-Popper test examines the ratio between the area of a polygon, and the area of a circle equal to the perimeter of that polygon. The result falls between 0 and 1, with values closer to 1 being assessed as more compact.\n",
    "\n",
    "\\begin{align*}\n",
    "PPtest = \\frac{polygon\\ area * 4\\pi}{polygon\\ perimeter^2}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The Menindee Lakes polygon above has a PPtest value $\\approx$ 0.00. \n",
    "\n",
    "We selected all polygons with a `PPtest` value <=0.005. This resulted in a subset of 186 polygons. \n",
    "\n",
    "![Polygons with a Polsby-Popper test score of less than 0.005](PPtestlessthan005.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 186 polygons were buffered with a -50 meter (2 pixel) buffer to separate the polygons where they are connected bu two pixels or less. This allows us to split up these very large polygons by using natural thinning points. The resulting negatively buffered polygons was run through the `multipart to singlepart` tool in QGIS, to give the now separated polygons unique IDs. \n",
    "\n",
    "These polygons were then buffered with a +50 meter buffer to return the polygons to approximately their original size. These final polygons were used to separate the 186 original polygons identified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for dividing up the identified very large polygons varied depending on the polygon in question. Where large waterbodies (like the Menindee Lakes) were connected, the buffered polygons were used to determine the cut points in the original polygons. Where additional breaks were required, the [Bureau of Meteorology's Geofabric v 3.0.5 Beta (Suface Hydrology Network)](ftp://ftp.bom.gov.au/anon/home/geofabric/) `waterbodies` dataset was used as an additional source of information for breaking up connected segments.\n",
    "\n",
    "The buffering method didn't work on large segments of river, which became a series of disconnected pieces when negatively and positively buffered. Instead, we used a combination of tributaries and man-made features such as bridges and weirs to segment these river sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final checks and recalculation of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaterBodiesBigRiverFiltered = gp.read_file(WOFSshpFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the area and perimeter of each polygon again following the manual checking\n",
    "# step performed above\n",
    "WaterBodiesBigRiverFiltered['area'] = WaterBodiesBigRiverFiltered['geometry'].area\n",
    "WaterBodiesBigRiverFiltered['perimeter'] = WaterBodiesBigRiverFiltered['geometry'].length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove the PPtest column, since we don't really want this as an attribute of the final shapefile\n",
    "WaterBodiesBigRiverFiltered.drop(labels = 'PPtest', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapply the size filtering, just to check that all of the split and filtered waterbodies are\n",
    "# still in the size range we want\n",
    "DoubleCheckArea = WaterBodiesBigRiverFiltered.loc[((WaterBodiesBigRiverFiltered['area'] > MinSize) & \n",
    "                                                   (WaterBodiesBigRiverFiltered['area'] <= MaxSize))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a unique ID for each polygon\n",
    "\n",
    "A unique identifier is required for every polygon to allow it to be referenced. The naming convention for generating unique IDs here is the [geohash](geohash.org).\n",
    "\n",
    "A Geohash is a geocoding system used to generate short unique identifiers based on latitude/longitude coordinates. It is a short combination of letters and numbers, with the length of the string a function of the precision of the location. The methods for generating a geohash are outlined [here - yes, the official documentation is a wikipedia article](https://en.wikipedia.org/wiki/Geohash).\n",
    "\n",
    "Here we use the python package `python-geohash` to generate a geohash unique identifier for each polygon. We use `precision = 9` geohash characters, which represents an on the ground accuracy of <20 metres. This ensures that the precision is high enough to differentiate between waterbodies located next to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert from Albers coordinates to lat/lon, in order to generate the geohash\n",
    "GetUniqueID = DoubleCheckArea.to_crs(epsg=4326)\n",
    "\n",
    "# Generate a geohash for the centroid of each polygon\n",
    "GetUniqueID['UID'] = GetUniqueID.apply(lambda x: gh.encode(x.geometry.centroid.y, x.geometry.centroid.x, \n",
    "                                                           precision=9), axis=1)\n",
    "\n",
    "# Check that our unique ID is in fact unique\n",
    "assert GetUniqueID['UID'].is_unique\n",
    "\n",
    "# Make an arbitrary numerical ID for each polygon. We will first sort the dataframe by geohash\n",
    "# so that polygons close to each other are numbered similarly\n",
    "SortedData = GetUniqueID.sort_values(by = ['UID']).reset_index() \n",
    "SortedData['FID'] = SortedData.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The step above creates an 'index' column, which we don't actually want, so drop it.\n",
    "SortedData.drop(labels = 'index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the final results to a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackToAlbers = SortedData.to_crs(epsg=3577)\n",
    "BackToAlbers.to_file(FinalName, driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
